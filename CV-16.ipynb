{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cb401c",
   "metadata": {},
   "source": [
    "# 행동 스티커 만들기\n",
    "\n",
    "목차\n",
    "\n",
    "- 라이브러리 임포트\n",
    "\n",
    "- 데이터 셋 로드\n",
    "\n",
    "- 파싱 함수 구현\n",
    "\n",
    "- TFRecord 파일 생성\n",
    "\n",
    "- ray 패키지로 병렬 처리\n",
    "\n",
    "- TFRecord 생성\n",
    "\n",
    "- 모델에 필요한 데이터로 변환\n",
    "\n",
    "- keypoint 를 heatmap 으로 변경\n",
    "\n",
    "- 데이터 전처리\n",
    "\n",
    "- 모델 - Hourglass\n",
    "\n",
    "- intermediate output을 위한 linear layer\n",
    "\n",
    "- stacked hourglass\n",
    "\n",
    "- 모델 - Simplebaseline\n",
    "\n",
    "- 학습 엔진\n",
    "\n",
    "- 데이터 셋 제작\n",
    "\n",
    "- StackedHourglassNetwork 학습\n",
    "\n",
    "- 예측 엔진\n",
    "\n",
    "- StackedHourglass 모델 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9311d",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b845c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있음\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17ed08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faebdf1",
   "metadata": {},
   "source": [
    "## 데이터 셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe80e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f52085",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d88f14",
   "metadata": {},
   "source": [
    "## 파싱 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1c2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bcf0401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772615d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842af4b",
   "metadata": {},
   "source": [
    "## TFRecord 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63115eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc95eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154fa35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f363f65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b0add",
   "metadata": {},
   "source": [
    "## ray 패키지로 병렬 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2705d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4834cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b72b0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45be12b",
   "metadata": {},
   "source": [
    "## TFRecord 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e5303e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 14:16:59,487\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.96gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=141)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=139)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135006f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc832f19",
   "metadata": {},
   "source": [
    "## 모델에 필요한 데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e632f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=140)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e85e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c2f9d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57d4d",
   "metadata": {},
   "source": [
    "## keypoint 를 heatmap 으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55f0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4464ef8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc56539",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d8af12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec7ef0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538233c6",
   "metadata": {},
   "source": [
    "## 모델 - Hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7417a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed43ab3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003f425",
   "metadata": {},
   "source": [
    "## intermediate output을 위한 linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c340d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9981e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a54c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53256153",
   "metadata": {},
   "source": [
    "## stacked hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e7e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a3a41",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed3e57",
   "metadata": {},
   "source": [
    "## 모델 - Simplebaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb5d0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n",
      "94781440/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5086d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(2,2), strides=2))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddfc9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19be3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fe29248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487a99d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2944af0",
   "metadata": {},
   "source": [
    "## 학습 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "106ad023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                model_file_name,\n",
    "                model,\n",
    "                epochs,\n",
    "                global_batch_size,\n",
    "                strategy,\n",
    "                initial_learning_rate):\n",
    "\n",
    "        self.model_file_name = model_file_name\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/{}-epoch-{}-loss-{:.4f}.h5'.format(self.model_file_name, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485ed4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cc724",
   "metadata": {},
   "source": [
    "## 데이터 셋 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11ebc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69ae7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, model_file_name='model', is_stackedhourglassnetwork=True):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "        \n",
    "        if is_stackedhourglassnetwork:\n",
    "            model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        else:\n",
    "            model = Simplebaseline(IMAGE_SHAPE)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_file_name,\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c1488",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3625e",
   "metadata": {},
   "source": [
    "## StackedHourglassNetwork 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4fa26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63b37b3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.26788616 epoch total loss 2.26788616\n",
      "Trained batch 2 batch loss 2.16127491 epoch total loss 2.21458054\n",
      "Trained batch 3 batch loss 2.31869864 epoch total loss 2.24928665\n",
      "Trained batch 4 batch loss 2.08849907 epoch total loss 2.20908976\n",
      "Trained batch 5 batch loss 2.04567075 epoch total loss 2.17640591\n",
      "Trained batch 6 batch loss 2.02959561 epoch total loss 2.15193748\n",
      "Trained batch 7 batch loss 2.0114181 epoch total loss 2.13186336\n",
      "Trained batch 8 batch loss 2.07697 epoch total loss 2.12500167\n",
      "Trained batch 9 batch loss 2.22638083 epoch total loss 2.13626599\n",
      "Trained batch 10 batch loss 2.20918655 epoch total loss 2.14355803\n",
      "Trained batch 11 batch loss 2.20327187 epoch total loss 2.14898658\n",
      "Trained batch 12 batch loss 2.16363192 epoch total loss 2.15020704\n",
      "Trained batch 13 batch loss 2.17391062 epoch total loss 2.15203047\n",
      "Trained batch 14 batch loss 2.0163908 epoch total loss 2.14234185\n",
      "Trained batch 15 batch loss 1.77077973 epoch total loss 2.11757112\n",
      "Trained batch 16 batch loss 1.83916593 epoch total loss 2.10017085\n",
      "Trained batch 17 batch loss 1.92515481 epoch total loss 2.08987594\n",
      "Trained batch 18 batch loss 1.92995715 epoch total loss 2.08099151\n",
      "Trained batch 19 batch loss 2.02418089 epoch total loss 2.0780015\n",
      "Trained batch 20 batch loss 1.98946583 epoch total loss 2.07357478\n",
      "Trained batch 21 batch loss 1.99163687 epoch total loss 2.06967306\n",
      "Trained batch 22 batch loss 1.98152709 epoch total loss 2.06566644\n",
      "Trained batch 23 batch loss 1.96798778 epoch total loss 2.06141949\n",
      "Trained batch 24 batch loss 1.93581188 epoch total loss 2.05618572\n",
      "Trained batch 25 batch loss 1.9775219 epoch total loss 2.05303931\n",
      "Trained batch 26 batch loss 1.90910888 epoch total loss 2.04750347\n",
      "Trained batch 27 batch loss 1.8375 epoch total loss 2.03972554\n",
      "Trained batch 28 batch loss 1.79482043 epoch total loss 2.03097892\n",
      "Trained batch 29 batch loss 1.85419691 epoch total loss 2.02488303\n",
      "Trained batch 30 batch loss 1.80689836 epoch total loss 2.01761699\n",
      "Trained batch 31 batch loss 1.82883394 epoch total loss 2.01152706\n",
      "Trained batch 32 batch loss 1.82423949 epoch total loss 2.00567436\n",
      "Trained batch 33 batch loss 1.78134322 epoch total loss 1.99887645\n",
      "Trained batch 34 batch loss 1.72417712 epoch total loss 1.99079692\n",
      "Trained batch 35 batch loss 1.72563946 epoch total loss 1.98322105\n",
      "Trained batch 36 batch loss 1.80893672 epoch total loss 1.97837973\n",
      "Trained batch 37 batch loss 1.7852819 epoch total loss 1.97316086\n",
      "Trained batch 38 batch loss 1.7363534 epoch total loss 1.96692896\n",
      "Trained batch 39 batch loss 1.76743507 epoch total loss 1.96181369\n",
      "Trained batch 40 batch loss 1.75878668 epoch total loss 1.95673811\n",
      "Trained batch 41 batch loss 1.63921988 epoch total loss 1.9489938\n",
      "Trained batch 42 batch loss 1.64257383 epoch total loss 1.94169796\n",
      "Trained batch 43 batch loss 1.73550022 epoch total loss 1.93690264\n",
      "Trained batch 44 batch loss 1.80432987 epoch total loss 1.93388951\n",
      "Trained batch 45 batch loss 1.79626334 epoch total loss 1.93083119\n",
      "Trained batch 46 batch loss 1.72332883 epoch total loss 1.92632031\n",
      "Trained batch 47 batch loss 1.7547642 epoch total loss 1.92267013\n",
      "Trained batch 48 batch loss 1.76481521 epoch total loss 1.9193815\n",
      "Trained batch 49 batch loss 1.58440757 epoch total loss 1.9125452\n",
      "Trained batch 50 batch loss 1.66176021 epoch total loss 1.90752947\n",
      "Trained batch 51 batch loss 1.73258162 epoch total loss 1.90409911\n",
      "Trained batch 52 batch loss 1.7352798 epoch total loss 1.90085268\n",
      "Trained batch 53 batch loss 1.78911746 epoch total loss 1.89874434\n",
      "Trained batch 54 batch loss 1.78565383 epoch total loss 1.89665008\n",
      "Trained batch 55 batch loss 1.80260062 epoch total loss 1.89494\n",
      "Trained batch 56 batch loss 1.69219255 epoch total loss 1.89131951\n",
      "Trained batch 57 batch loss 1.79430127 epoch total loss 1.88961756\n",
      "Trained batch 58 batch loss 1.67282128 epoch total loss 1.88587964\n",
      "Trained batch 59 batch loss 1.56059456 epoch total loss 1.88036633\n",
      "Trained batch 60 batch loss 1.65780687 epoch total loss 1.87665701\n",
      "Trained batch 61 batch loss 1.77791023 epoch total loss 1.87503815\n",
      "Trained batch 62 batch loss 1.79479992 epoch total loss 1.87374401\n",
      "Trained batch 63 batch loss 1.70034993 epoch total loss 1.87099171\n",
      "Trained batch 64 batch loss 1.82409179 epoch total loss 1.87025881\n",
      "Trained batch 65 batch loss 1.71411967 epoch total loss 1.86785662\n",
      "Trained batch 66 batch loss 1.73901117 epoch total loss 1.86590445\n",
      "Trained batch 67 batch loss 1.76817393 epoch total loss 1.86444581\n",
      "Trained batch 68 batch loss 1.75878024 epoch total loss 1.86289191\n",
      "Trained batch 69 batch loss 1.75490689 epoch total loss 1.86132705\n",
      "Trained batch 70 batch loss 1.78679037 epoch total loss 1.86026216\n",
      "Trained batch 71 batch loss 1.81375 epoch total loss 1.8596071\n",
      "Trained batch 72 batch loss 1.73025644 epoch total loss 1.8578105\n",
      "Trained batch 73 batch loss 1.71555877 epoch total loss 1.8558619\n",
      "Trained batch 74 batch loss 1.70894527 epoch total loss 1.85387647\n",
      "Trained batch 75 batch loss 1.80713844 epoch total loss 1.85325336\n",
      "Trained batch 76 batch loss 1.77722788 epoch total loss 1.85225296\n",
      "Trained batch 77 batch loss 1.70783 epoch total loss 1.85037732\n",
      "Trained batch 78 batch loss 1.73974371 epoch total loss 1.84895897\n",
      "Trained batch 79 batch loss 1.77349091 epoch total loss 1.84800351\n",
      "Trained batch 80 batch loss 1.78609991 epoch total loss 1.84722972\n",
      "Trained batch 81 batch loss 1.6608274 epoch total loss 1.8449285\n",
      "Trained batch 82 batch loss 1.70682061 epoch total loss 1.84324419\n",
      "Trained batch 83 batch loss 1.75836849 epoch total loss 1.8422215\n",
      "Trained batch 84 batch loss 1.73447216 epoch total loss 1.84093881\n",
      "Trained batch 85 batch loss 1.63668466 epoch total loss 1.83853579\n",
      "Trained batch 86 batch loss 1.56029618 epoch total loss 1.83530056\n",
      "Trained batch 87 batch loss 1.57234359 epoch total loss 1.83227801\n",
      "Trained batch 88 batch loss 1.53688788 epoch total loss 1.8289212\n",
      "Trained batch 89 batch loss 1.6685307 epoch total loss 1.82711911\n",
      "Trained batch 90 batch loss 1.70031881 epoch total loss 1.82571018\n",
      "Trained batch 91 batch loss 1.70114338 epoch total loss 1.8243413\n",
      "Trained batch 92 batch loss 1.77190554 epoch total loss 1.82377148\n",
      "Trained batch 93 batch loss 1.75001311 epoch total loss 1.82297838\n",
      "Trained batch 94 batch loss 1.74380803 epoch total loss 1.82213604\n",
      "Trained batch 95 batch loss 1.51452792 epoch total loss 1.81889808\n",
      "Trained batch 96 batch loss 1.60466814 epoch total loss 1.8166666\n",
      "Trained batch 97 batch loss 1.71914661 epoch total loss 1.81566119\n",
      "Trained batch 98 batch loss 1.68418324 epoch total loss 1.81431973\n",
      "Trained batch 99 batch loss 1.55667806 epoch total loss 1.81171715\n",
      "Trained batch 100 batch loss 1.64072263 epoch total loss 1.81000721\n",
      "Trained batch 101 batch loss 1.53029537 epoch total loss 1.80723763\n",
      "Trained batch 102 batch loss 1.69098163 epoch total loss 1.80609787\n",
      "Trained batch 103 batch loss 1.66062546 epoch total loss 1.80468559\n",
      "Trained batch 104 batch loss 1.75972915 epoch total loss 1.80425334\n",
      "Trained batch 105 batch loss 1.73198938 epoch total loss 1.80356514\n",
      "Trained batch 106 batch loss 1.7317673 epoch total loss 1.8028878\n",
      "Trained batch 107 batch loss 1.63250089 epoch total loss 1.80129552\n",
      "Trained batch 108 batch loss 1.5272553 epoch total loss 1.79875803\n",
      "Trained batch 109 batch loss 1.65282071 epoch total loss 1.79741919\n",
      "Trained batch 110 batch loss 1.65998745 epoch total loss 1.79616976\n",
      "Trained batch 111 batch loss 1.63872337 epoch total loss 1.79475129\n",
      "Trained batch 112 batch loss 1.61341441 epoch total loss 1.79313219\n",
      "Trained batch 113 batch loss 1.61974382 epoch total loss 1.79159784\n",
      "Trained batch 114 batch loss 1.68339658 epoch total loss 1.7906487\n",
      "Trained batch 115 batch loss 1.64798748 epoch total loss 1.78940833\n",
      "Trained batch 116 batch loss 1.64227581 epoch total loss 1.78813982\n",
      "Trained batch 117 batch loss 1.64679718 epoch total loss 1.78693175\n",
      "Trained batch 118 batch loss 1.61986637 epoch total loss 1.78551602\n",
      "Trained batch 119 batch loss 1.65303242 epoch total loss 1.78440273\n",
      "Trained batch 120 batch loss 1.696419 epoch total loss 1.78366959\n",
      "Trained batch 121 batch loss 1.61889732 epoch total loss 1.78230774\n",
      "Trained batch 122 batch loss 1.72538877 epoch total loss 1.78184116\n",
      "Trained batch 123 batch loss 1.73163807 epoch total loss 1.78143311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 124 batch loss 1.73343384 epoch total loss 1.78104603\n",
      "Trained batch 125 batch loss 1.65691233 epoch total loss 1.7800529\n",
      "Trained batch 126 batch loss 1.64016747 epoch total loss 1.7789427\n",
      "Trained batch 127 batch loss 1.67110658 epoch total loss 1.77809358\n",
      "Trained batch 128 batch loss 1.66235161 epoch total loss 1.77718937\n",
      "Trained batch 129 batch loss 1.55202 epoch total loss 1.77544391\n",
      "Trained batch 130 batch loss 1.5783993 epoch total loss 1.77392817\n",
      "Trained batch 131 batch loss 1.66414857 epoch total loss 1.77309012\n",
      "Trained batch 132 batch loss 1.70923734 epoch total loss 1.77260649\n",
      "Trained batch 133 batch loss 1.68635654 epoch total loss 1.77195799\n",
      "Trained batch 134 batch loss 1.68762898 epoch total loss 1.77132857\n",
      "Trained batch 135 batch loss 1.46417665 epoch total loss 1.76905334\n",
      "Trained batch 136 batch loss 1.42035151 epoch total loss 1.76648939\n",
      "Trained batch 137 batch loss 1.61675072 epoch total loss 1.76539636\n",
      "Trained batch 138 batch loss 1.70360076 epoch total loss 1.76494849\n",
      "Trained batch 139 batch loss 1.69061542 epoch total loss 1.76441371\n",
      "Trained batch 140 batch loss 1.76076889 epoch total loss 1.76438773\n",
      "Trained batch 141 batch loss 1.78890157 epoch total loss 1.76456153\n",
      "Trained batch 142 batch loss 1.74893034 epoch total loss 1.7644515\n",
      "Trained batch 143 batch loss 1.71385312 epoch total loss 1.76409757\n",
      "Trained batch 144 batch loss 1.7234025 epoch total loss 1.76381505\n",
      "Trained batch 145 batch loss 1.64727736 epoch total loss 1.76301134\n",
      "Trained batch 146 batch loss 1.6461755 epoch total loss 1.76221097\n",
      "Trained batch 147 batch loss 1.68417394 epoch total loss 1.76168013\n",
      "Trained batch 148 batch loss 1.70740271 epoch total loss 1.76131332\n",
      "Trained batch 149 batch loss 1.70242763 epoch total loss 1.76091814\n",
      "Trained batch 150 batch loss 1.69122589 epoch total loss 1.76045346\n",
      "Trained batch 151 batch loss 1.65180707 epoch total loss 1.75973392\n",
      "Trained batch 152 batch loss 1.57848978 epoch total loss 1.75854146\n",
      "Trained batch 153 batch loss 1.65780437 epoch total loss 1.75788307\n",
      "Trained batch 154 batch loss 1.56237268 epoch total loss 1.75661361\n",
      "Trained batch 155 batch loss 1.6558671 epoch total loss 1.75596356\n",
      "Trained batch 156 batch loss 1.68883145 epoch total loss 1.75553322\n",
      "Trained batch 157 batch loss 1.71810842 epoch total loss 1.75529492\n",
      "Trained batch 158 batch loss 1.67140746 epoch total loss 1.75476408\n",
      "Trained batch 159 batch loss 1.65798056 epoch total loss 1.7541554\n",
      "Trained batch 160 batch loss 1.69707227 epoch total loss 1.75379872\n",
      "Trained batch 161 batch loss 1.65576744 epoch total loss 1.7531898\n",
      "Trained batch 162 batch loss 1.58665919 epoch total loss 1.75216186\n",
      "Trained batch 163 batch loss 1.60575426 epoch total loss 1.75126362\n",
      "Trained batch 164 batch loss 1.58599 epoch total loss 1.75025582\n",
      "Trained batch 165 batch loss 1.4409436 epoch total loss 1.74838126\n",
      "Trained batch 166 batch loss 1.55564153 epoch total loss 1.74722016\n",
      "Trained batch 167 batch loss 1.58621573 epoch total loss 1.74625599\n",
      "Trained batch 168 batch loss 1.57262039 epoch total loss 1.74522257\n",
      "Trained batch 169 batch loss 1.59509778 epoch total loss 1.74433422\n",
      "Trained batch 170 batch loss 1.6674974 epoch total loss 1.7438823\n",
      "Trained batch 171 batch loss 1.51906419 epoch total loss 1.74256766\n",
      "Trained batch 172 batch loss 1.5714221 epoch total loss 1.7415725\n",
      "Trained batch 173 batch loss 1.6358813 epoch total loss 1.74096167\n",
      "Trained batch 174 batch loss 1.62580597 epoch total loss 1.74029982\n",
      "Trained batch 175 batch loss 1.54545474 epoch total loss 1.73918629\n",
      "Trained batch 176 batch loss 1.56570756 epoch total loss 1.73820066\n",
      "Trained batch 177 batch loss 1.73769784 epoch total loss 1.7381978\n",
      "Trained batch 178 batch loss 1.52921939 epoch total loss 1.73702371\n",
      "Trained batch 179 batch loss 1.44701719 epoch total loss 1.73540354\n",
      "Trained batch 180 batch loss 1.43871665 epoch total loss 1.73375535\n",
      "Trained batch 181 batch loss 1.46119583 epoch total loss 1.73224938\n",
      "Trained batch 182 batch loss 1.54648232 epoch total loss 1.73122871\n",
      "Trained batch 183 batch loss 1.63597846 epoch total loss 1.73070824\n",
      "Trained batch 184 batch loss 1.66458213 epoch total loss 1.73034883\n",
      "Trained batch 185 batch loss 1.63508856 epoch total loss 1.72983396\n",
      "Trained batch 186 batch loss 1.51603723 epoch total loss 1.72868466\n",
      "Trained batch 187 batch loss 1.53232121 epoch total loss 1.72763455\n",
      "Trained batch 188 batch loss 1.62417674 epoch total loss 1.72708428\n",
      "Trained batch 189 batch loss 1.63116348 epoch total loss 1.72657669\n",
      "Trained batch 190 batch loss 1.60864294 epoch total loss 1.72595596\n",
      "Trained batch 191 batch loss 1.6057682 epoch total loss 1.72532678\n",
      "Trained batch 192 batch loss 1.62710619 epoch total loss 1.72481525\n",
      "Trained batch 193 batch loss 1.59396672 epoch total loss 1.72413719\n",
      "Trained batch 194 batch loss 1.64048922 epoch total loss 1.72370613\n",
      "Trained batch 195 batch loss 1.63481975 epoch total loss 1.72325027\n",
      "Trained batch 196 batch loss 1.64696705 epoch total loss 1.72286117\n",
      "Trained batch 197 batch loss 1.69387507 epoch total loss 1.72271407\n",
      "Trained batch 198 batch loss 1.69702625 epoch total loss 1.72258425\n",
      "Trained batch 199 batch loss 1.69641495 epoch total loss 1.72245276\n",
      "Trained batch 200 batch loss 1.66344666 epoch total loss 1.72215772\n",
      "Trained batch 201 batch loss 1.68888509 epoch total loss 1.72199214\n",
      "Trained batch 202 batch loss 1.60960364 epoch total loss 1.72143567\n",
      "Trained batch 203 batch loss 1.54386353 epoch total loss 1.72056091\n",
      "Trained batch 204 batch loss 1.6454078 epoch total loss 1.72019255\n",
      "Trained batch 205 batch loss 1.58457088 epoch total loss 1.71953094\n",
      "Trained batch 206 batch loss 1.5982362 epoch total loss 1.71894217\n",
      "Trained batch 207 batch loss 1.59730232 epoch total loss 1.71835446\n",
      "Trained batch 208 batch loss 1.54370105 epoch total loss 1.71751475\n",
      "Trained batch 209 batch loss 1.64420545 epoch total loss 1.71716392\n",
      "Trained batch 210 batch loss 1.51444316 epoch total loss 1.71619856\n",
      "Trained batch 211 batch loss 1.59254324 epoch total loss 1.71561253\n",
      "Trained batch 212 batch loss 1.56016719 epoch total loss 1.71487927\n",
      "Trained batch 213 batch loss 1.56609797 epoch total loss 1.71418083\n",
      "Trained batch 214 batch loss 1.61033225 epoch total loss 1.71369553\n",
      "Trained batch 215 batch loss 1.58180833 epoch total loss 1.71308208\n",
      "Trained batch 216 batch loss 1.59389186 epoch total loss 1.71253037\n",
      "Trained batch 217 batch loss 1.56877112 epoch total loss 1.71186781\n",
      "Trained batch 218 batch loss 1.44307661 epoch total loss 1.71063483\n",
      "Trained batch 219 batch loss 1.50397527 epoch total loss 1.70969117\n",
      "Trained batch 220 batch loss 1.58043218 epoch total loss 1.7091037\n",
      "Trained batch 221 batch loss 1.60859752 epoch total loss 1.70864892\n",
      "Trained batch 222 batch loss 1.65221989 epoch total loss 1.70839477\n",
      "Trained batch 223 batch loss 1.66051137 epoch total loss 1.70818007\n",
      "Trained batch 224 batch loss 1.70489919 epoch total loss 1.70816541\n",
      "Trained batch 225 batch loss 1.60840321 epoch total loss 1.70772207\n",
      "Trained batch 226 batch loss 1.68340719 epoch total loss 1.70761442\n",
      "Trained batch 227 batch loss 1.68079388 epoch total loss 1.70749629\n",
      "Trained batch 228 batch loss 1.64107108 epoch total loss 1.70720494\n",
      "Trained batch 229 batch loss 1.63367772 epoch total loss 1.70688391\n",
      "Trained batch 230 batch loss 1.66165006 epoch total loss 1.70668721\n",
      "Trained batch 231 batch loss 1.76208889 epoch total loss 1.70692706\n",
      "Trained batch 232 batch loss 1.63957608 epoch total loss 1.70663679\n",
      "Trained batch 233 batch loss 1.62467289 epoch total loss 1.706285\n",
      "Trained batch 234 batch loss 1.63804615 epoch total loss 1.70599329\n",
      "Trained batch 235 batch loss 1.65810227 epoch total loss 1.70578957\n",
      "Trained batch 236 batch loss 1.62410641 epoch total loss 1.70544338\n",
      "Trained batch 237 batch loss 1.67012095 epoch total loss 1.70529449\n",
      "Trained batch 238 batch loss 1.68128061 epoch total loss 1.70519352\n",
      "Trained batch 239 batch loss 1.66823864 epoch total loss 1.70503891\n",
      "Trained batch 240 batch loss 1.53656924 epoch total loss 1.70433688\n",
      "Trained batch 241 batch loss 1.57502246 epoch total loss 1.70380032\n",
      "Trained batch 242 batch loss 1.60471869 epoch total loss 1.70339084\n",
      "Trained batch 243 batch loss 1.68400264 epoch total loss 1.70331097\n",
      "Trained batch 244 batch loss 1.65916753 epoch total loss 1.70313013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 245 batch loss 1.53616035 epoch total loss 1.70244861\n",
      "Trained batch 246 batch loss 1.58769214 epoch total loss 1.70198202\n",
      "Trained batch 247 batch loss 1.63808835 epoch total loss 1.70172346\n",
      "Trained batch 248 batch loss 1.68336272 epoch total loss 1.70164931\n",
      "Trained batch 249 batch loss 1.75065064 epoch total loss 1.70184612\n",
      "Trained batch 250 batch loss 1.63657641 epoch total loss 1.70158494\n",
      "Trained batch 251 batch loss 1.61913705 epoch total loss 1.70125651\n",
      "Trained batch 252 batch loss 1.60006356 epoch total loss 1.7008549\n",
      "Trained batch 253 batch loss 1.51494312 epoch total loss 1.70012021\n",
      "Trained batch 254 batch loss 1.60660315 epoch total loss 1.69975197\n",
      "Trained batch 255 batch loss 1.51927125 epoch total loss 1.69904411\n",
      "Trained batch 256 batch loss 1.59053433 epoch total loss 1.69862032\n",
      "Trained batch 257 batch loss 1.59800434 epoch total loss 1.69822872\n",
      "Trained batch 258 batch loss 1.60598731 epoch total loss 1.69787121\n",
      "Trained batch 259 batch loss 1.57978868 epoch total loss 1.69741535\n",
      "Trained batch 260 batch loss 1.6257689 epoch total loss 1.69713974\n",
      "Trained batch 261 batch loss 1.63100576 epoch total loss 1.69688642\n",
      "Trained batch 262 batch loss 1.60772181 epoch total loss 1.69654608\n",
      "Trained batch 263 batch loss 1.57696795 epoch total loss 1.69609141\n",
      "Trained batch 264 batch loss 1.54294801 epoch total loss 1.69551134\n",
      "Trained batch 265 batch loss 1.67194462 epoch total loss 1.69542241\n",
      "Trained batch 266 batch loss 1.55517113 epoch total loss 1.69489515\n",
      "Trained batch 267 batch loss 1.5995357 epoch total loss 1.694538\n",
      "Trained batch 268 batch loss 1.56064796 epoch total loss 1.69403839\n",
      "Trained batch 269 batch loss 1.57279563 epoch total loss 1.69358766\n",
      "Trained batch 270 batch loss 1.66612887 epoch total loss 1.69348598\n",
      "Trained batch 271 batch loss 1.70038152 epoch total loss 1.69351137\n",
      "Trained batch 272 batch loss 1.64932775 epoch total loss 1.69334888\n",
      "Trained batch 273 batch loss 1.70034301 epoch total loss 1.69337463\n",
      "Trained batch 274 batch loss 1.65326607 epoch total loss 1.69322813\n",
      "Trained batch 275 batch loss 1.71166468 epoch total loss 1.69329524\n",
      "Trained batch 276 batch loss 1.59542549 epoch total loss 1.69294059\n",
      "Trained batch 277 batch loss 1.65979791 epoch total loss 1.69282091\n",
      "Trained batch 278 batch loss 1.67478871 epoch total loss 1.69275606\n",
      "Trained batch 279 batch loss 1.7096746 epoch total loss 1.69281673\n",
      "Trained batch 280 batch loss 1.69572556 epoch total loss 1.69282722\n",
      "Trained batch 281 batch loss 1.62914073 epoch total loss 1.69260061\n",
      "Trained batch 282 batch loss 1.65291989 epoch total loss 1.69245982\n",
      "Trained batch 283 batch loss 1.63068485 epoch total loss 1.69224155\n",
      "Trained batch 284 batch loss 1.59359503 epoch total loss 1.69189417\n",
      "Trained batch 285 batch loss 1.57993174 epoch total loss 1.69150138\n",
      "Trained batch 286 batch loss 1.56775403 epoch total loss 1.69106865\n",
      "Trained batch 287 batch loss 1.62513173 epoch total loss 1.69083881\n",
      "Trained batch 288 batch loss 1.61033607 epoch total loss 1.69055927\n",
      "Trained batch 289 batch loss 1.61067939 epoch total loss 1.69028294\n",
      "Trained batch 290 batch loss 1.63908434 epoch total loss 1.69010639\n",
      "Trained batch 291 batch loss 1.56416965 epoch total loss 1.68967366\n",
      "Trained batch 292 batch loss 1.63878453 epoch total loss 1.68949938\n",
      "Trained batch 293 batch loss 1.60113072 epoch total loss 1.6891979\n",
      "Trained batch 294 batch loss 1.62067091 epoch total loss 1.68896472\n",
      "Trained batch 295 batch loss 1.61267424 epoch total loss 1.68870616\n",
      "Trained batch 296 batch loss 1.56664038 epoch total loss 1.68829381\n",
      "Trained batch 297 batch loss 1.6956284 epoch total loss 1.68831837\n",
      "Trained batch 298 batch loss 1.64036727 epoch total loss 1.68815756\n",
      "Trained batch 299 batch loss 1.77430797 epoch total loss 1.68844569\n",
      "Trained batch 300 batch loss 1.77062774 epoch total loss 1.68871963\n",
      "Trained batch 301 batch loss 1.77633488 epoch total loss 1.68901074\n",
      "Trained batch 302 batch loss 1.69501102 epoch total loss 1.68903065\n",
      "Trained batch 303 batch loss 1.5885123 epoch total loss 1.68869889\n",
      "Trained batch 304 batch loss 1.57205462 epoch total loss 1.68831515\n",
      "Trained batch 305 batch loss 1.61597681 epoch total loss 1.68807793\n",
      "Trained batch 306 batch loss 1.60739481 epoch total loss 1.68781435\n",
      "Trained batch 307 batch loss 1.62655807 epoch total loss 1.68761492\n",
      "Trained batch 308 batch loss 1.63452423 epoch total loss 1.68744254\n",
      "Trained batch 309 batch loss 1.58983946 epoch total loss 1.68712664\n",
      "Trained batch 310 batch loss 1.6260519 epoch total loss 1.68692958\n",
      "Trained batch 311 batch loss 1.65852261 epoch total loss 1.68683827\n",
      "Trained batch 312 batch loss 1.6623528 epoch total loss 1.68675971\n",
      "Trained batch 313 batch loss 1.67594886 epoch total loss 1.68672526\n",
      "Trained batch 314 batch loss 1.64765549 epoch total loss 1.6866008\n",
      "Trained batch 315 batch loss 1.57169223 epoch total loss 1.68623614\n",
      "Trained batch 316 batch loss 1.49194503 epoch total loss 1.68562126\n",
      "Trained batch 317 batch loss 1.44983935 epoch total loss 1.6848774\n",
      "Trained batch 318 batch loss 1.38063133 epoch total loss 1.68392062\n",
      "Trained batch 319 batch loss 1.55026662 epoch total loss 1.68350172\n",
      "Trained batch 320 batch loss 1.49158823 epoch total loss 1.68290198\n",
      "Trained batch 321 batch loss 1.55771017 epoch total loss 1.68251204\n",
      "Trained batch 322 batch loss 1.52661109 epoch total loss 1.68202782\n",
      "Trained batch 323 batch loss 1.49298 epoch total loss 1.68144262\n",
      "Trained batch 324 batch loss 1.46831107 epoch total loss 1.68078482\n",
      "Trained batch 325 batch loss 1.51521134 epoch total loss 1.68027532\n",
      "Trained batch 326 batch loss 1.59372711 epoch total loss 1.68000984\n",
      "Trained batch 327 batch loss 1.64293957 epoch total loss 1.67989659\n",
      "Trained batch 328 batch loss 1.64236689 epoch total loss 1.67978227\n",
      "Trained batch 329 batch loss 1.58086145 epoch total loss 1.67948163\n",
      "Trained batch 330 batch loss 1.63606811 epoch total loss 1.6793499\n",
      "Trained batch 331 batch loss 1.62308967 epoch total loss 1.67918\n",
      "Trained batch 332 batch loss 1.55998921 epoch total loss 1.67882109\n",
      "Trained batch 333 batch loss 1.59685862 epoch total loss 1.67857492\n",
      "Trained batch 334 batch loss 1.59779549 epoch total loss 1.67833304\n",
      "Trained batch 335 batch loss 1.67417181 epoch total loss 1.67832065\n",
      "Trained batch 336 batch loss 1.65158474 epoch total loss 1.67824113\n",
      "Trained batch 337 batch loss 1.55984282 epoch total loss 1.67788982\n",
      "Trained batch 338 batch loss 1.47344041 epoch total loss 1.67728496\n",
      "Trained batch 339 batch loss 1.58476901 epoch total loss 1.67701209\n",
      "Trained batch 340 batch loss 1.63651586 epoch total loss 1.676893\n",
      "Trained batch 341 batch loss 1.57351673 epoch total loss 1.67658973\n",
      "Trained batch 342 batch loss 1.64116168 epoch total loss 1.67648613\n",
      "Trained batch 343 batch loss 1.65676224 epoch total loss 1.67642856\n",
      "Trained batch 344 batch loss 1.62094045 epoch total loss 1.67626727\n",
      "Trained batch 345 batch loss 1.67784381 epoch total loss 1.6762718\n",
      "Trained batch 346 batch loss 1.61570144 epoch total loss 1.6760968\n",
      "Trained batch 347 batch loss 1.55372524 epoch total loss 1.67574418\n",
      "Trained batch 348 batch loss 1.63388562 epoch total loss 1.67562389\n",
      "Trained batch 349 batch loss 1.62392592 epoch total loss 1.67547572\n",
      "Trained batch 350 batch loss 1.61117363 epoch total loss 1.6752919\n",
      "Trained batch 351 batch loss 1.64748192 epoch total loss 1.67521262\n",
      "Trained batch 352 batch loss 1.59788013 epoch total loss 1.67499304\n",
      "Trained batch 353 batch loss 1.61518121 epoch total loss 1.67482352\n",
      "Trained batch 354 batch loss 1.64508772 epoch total loss 1.67473948\n",
      "Trained batch 355 batch loss 1.60788918 epoch total loss 1.67455125\n",
      "Trained batch 356 batch loss 1.67648554 epoch total loss 1.67455673\n",
      "Trained batch 357 batch loss 1.6095835 epoch total loss 1.6743747\n",
      "Trained batch 358 batch loss 1.58077717 epoch total loss 1.67411315\n",
      "Trained batch 359 batch loss 1.59688401 epoch total loss 1.67389798\n",
      "Trained batch 360 batch loss 1.58491933 epoch total loss 1.67365074\n",
      "Trained batch 361 batch loss 1.59630418 epoch total loss 1.67343652\n",
      "Trained batch 362 batch loss 1.58573318 epoch total loss 1.67319429\n",
      "Trained batch 363 batch loss 1.58588052 epoch total loss 1.67295384\n",
      "Trained batch 364 batch loss 1.64965606 epoch total loss 1.67288983\n",
      "Trained batch 365 batch loss 1.65226698 epoch total loss 1.67283332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 366 batch loss 1.59428596 epoch total loss 1.67261875\n",
      "Trained batch 367 batch loss 1.55179965 epoch total loss 1.67228961\n",
      "Trained batch 368 batch loss 1.66152263 epoch total loss 1.67226028\n",
      "Trained batch 369 batch loss 1.607458 epoch total loss 1.67208469\n",
      "Trained batch 370 batch loss 1.62918448 epoch total loss 1.67196882\n",
      "Trained batch 371 batch loss 1.72104454 epoch total loss 1.67210126\n",
      "Trained batch 372 batch loss 1.67363489 epoch total loss 1.67210531\n",
      "Trained batch 373 batch loss 1.76307535 epoch total loss 1.67234921\n",
      "Trained batch 374 batch loss 1.66797495 epoch total loss 1.67233753\n",
      "Trained batch 375 batch loss 1.6529187 epoch total loss 1.67228568\n",
      "Trained batch 376 batch loss 1.57739186 epoch total loss 1.67203331\n",
      "Trained batch 377 batch loss 1.55051279 epoch total loss 1.67171097\n",
      "Trained batch 378 batch loss 1.65901351 epoch total loss 1.67167735\n",
      "Trained batch 379 batch loss 1.61757231 epoch total loss 1.67153454\n",
      "Trained batch 380 batch loss 1.6543026 epoch total loss 1.67148924\n",
      "Trained batch 381 batch loss 1.50889659 epoch total loss 1.67106247\n",
      "Trained batch 382 batch loss 1.56883049 epoch total loss 1.67079484\n",
      "Trained batch 383 batch loss 1.587749 epoch total loss 1.67057812\n",
      "Trained batch 384 batch loss 1.65959632 epoch total loss 1.67054951\n",
      "Trained batch 385 batch loss 1.73468733 epoch total loss 1.67071617\n",
      "Trained batch 386 batch loss 1.6709255 epoch total loss 1.67071664\n",
      "Trained batch 387 batch loss 1.6821084 epoch total loss 1.67074609\n",
      "Trained batch 388 batch loss 1.6069032 epoch total loss 1.67058158\n",
      "Trained batch 389 batch loss 1.70891988 epoch total loss 1.67068017\n",
      "Trained batch 390 batch loss 1.734797 epoch total loss 1.67084455\n",
      "Trained batch 391 batch loss 1.6267482 epoch total loss 1.6707319\n",
      "Trained batch 392 batch loss 1.64503789 epoch total loss 1.67066634\n",
      "Trained batch 393 batch loss 1.54777181 epoch total loss 1.67035365\n",
      "Trained batch 394 batch loss 1.52997637 epoch total loss 1.66999733\n",
      "Trained batch 395 batch loss 1.63461 epoch total loss 1.66990769\n",
      "Trained batch 396 batch loss 1.5974077 epoch total loss 1.66972458\n",
      "Trained batch 397 batch loss 1.58927941 epoch total loss 1.66952205\n",
      "Trained batch 398 batch loss 1.56018746 epoch total loss 1.66924727\n",
      "Trained batch 399 batch loss 1.54815149 epoch total loss 1.66894376\n",
      "Trained batch 400 batch loss 1.48576 epoch total loss 1.66848588\n",
      "Trained batch 401 batch loss 1.54716682 epoch total loss 1.66818333\n",
      "Trained batch 402 batch loss 1.57075703 epoch total loss 1.66794097\n",
      "Trained batch 403 batch loss 1.63157988 epoch total loss 1.66785073\n",
      "Trained batch 404 batch loss 1.66347265 epoch total loss 1.66783988\n",
      "Trained batch 405 batch loss 1.73957348 epoch total loss 1.66801703\n",
      "Trained batch 406 batch loss 1.65658462 epoch total loss 1.66798878\n",
      "Trained batch 407 batch loss 1.72673464 epoch total loss 1.66813314\n",
      "Trained batch 408 batch loss 1.52973938 epoch total loss 1.66779387\n",
      "Trained batch 409 batch loss 1.39372087 epoch total loss 1.66712379\n",
      "Trained batch 410 batch loss 1.31837916 epoch total loss 1.66627312\n",
      "Trained batch 411 batch loss 1.32096338 epoch total loss 1.66543305\n",
      "Trained batch 412 batch loss 1.44797611 epoch total loss 1.66490531\n",
      "Trained batch 413 batch loss 1.48128092 epoch total loss 1.66446066\n",
      "Trained batch 414 batch loss 1.50628 epoch total loss 1.66407859\n",
      "Trained batch 415 batch loss 1.60503852 epoch total loss 1.66393626\n",
      "Trained batch 416 batch loss 1.59806418 epoch total loss 1.66377807\n",
      "Trained batch 417 batch loss 1.51462364 epoch total loss 1.66342044\n",
      "Trained batch 418 batch loss 1.53064322 epoch total loss 1.66310275\n",
      "Trained batch 419 batch loss 1.52511442 epoch total loss 1.66277337\n",
      "Trained batch 420 batch loss 1.49092591 epoch total loss 1.66236413\n",
      "Trained batch 421 batch loss 1.51073802 epoch total loss 1.66200399\n",
      "Trained batch 422 batch loss 1.39971554 epoch total loss 1.66138244\n",
      "Trained batch 423 batch loss 1.53754759 epoch total loss 1.66108966\n",
      "Trained batch 424 batch loss 1.57521355 epoch total loss 1.66088712\n",
      "Trained batch 425 batch loss 1.59092736 epoch total loss 1.66072249\n",
      "Trained batch 426 batch loss 1.58966088 epoch total loss 1.66055572\n",
      "Trained batch 427 batch loss 1.56154418 epoch total loss 1.66032374\n",
      "Trained batch 428 batch loss 1.50363088 epoch total loss 1.65995765\n",
      "Trained batch 429 batch loss 1.51255083 epoch total loss 1.65961409\n",
      "Trained batch 430 batch loss 1.55718851 epoch total loss 1.65937591\n",
      "Trained batch 431 batch loss 1.48948455 epoch total loss 1.65898168\n",
      "Trained batch 432 batch loss 1.61200511 epoch total loss 1.65887296\n",
      "Trained batch 433 batch loss 1.57458556 epoch total loss 1.65867829\n",
      "Trained batch 434 batch loss 1.56844628 epoch total loss 1.65847027\n",
      "Trained batch 435 batch loss 1.52616131 epoch total loss 1.65816617\n",
      "Trained batch 436 batch loss 1.5857017 epoch total loss 1.658\n",
      "Trained batch 437 batch loss 1.60080969 epoch total loss 1.65786922\n",
      "Trained batch 438 batch loss 1.57433963 epoch total loss 1.65767848\n",
      "Trained batch 439 batch loss 1.67808056 epoch total loss 1.65772498\n",
      "Trained batch 440 batch loss 1.59871578 epoch total loss 1.65759087\n",
      "Trained batch 441 batch loss 1.4153111 epoch total loss 1.65704143\n",
      "Trained batch 442 batch loss 1.60161853 epoch total loss 1.65691602\n",
      "Trained batch 443 batch loss 1.63032341 epoch total loss 1.65685594\n",
      "Trained batch 444 batch loss 1.64677238 epoch total loss 1.65683329\n",
      "Trained batch 445 batch loss 1.51697969 epoch total loss 1.65651894\n",
      "Trained batch 446 batch loss 1.45741713 epoch total loss 1.6560725\n",
      "Trained batch 447 batch loss 1.48918366 epoch total loss 1.65569913\n",
      "Trained batch 448 batch loss 1.59112692 epoch total loss 1.65555501\n",
      "Trained batch 449 batch loss 1.56421685 epoch total loss 1.65535164\n",
      "Trained batch 450 batch loss 1.54599988 epoch total loss 1.65510869\n",
      "Trained batch 451 batch loss 1.49344611 epoch total loss 1.65475023\n",
      "Trained batch 452 batch loss 1.47359097 epoch total loss 1.65434945\n",
      "Trained batch 453 batch loss 1.55872297 epoch total loss 1.65413833\n",
      "Trained batch 454 batch loss 1.55570626 epoch total loss 1.65392148\n",
      "Trained batch 455 batch loss 1.5154506 epoch total loss 1.65361714\n",
      "Trained batch 456 batch loss 1.54449892 epoch total loss 1.65337789\n",
      "Trained batch 457 batch loss 1.40538096 epoch total loss 1.65283525\n",
      "Trained batch 458 batch loss 1.63844204 epoch total loss 1.65280378\n",
      "Trained batch 459 batch loss 1.6052947 epoch total loss 1.6527003\n",
      "Trained batch 460 batch loss 1.55541682 epoch total loss 1.65248883\n",
      "Trained batch 461 batch loss 1.68440843 epoch total loss 1.65255797\n",
      "Trained batch 462 batch loss 1.56309199 epoch total loss 1.65236437\n",
      "Trained batch 463 batch loss 1.56056631 epoch total loss 1.65216601\n",
      "Trained batch 464 batch loss 1.51519501 epoch total loss 1.65187085\n",
      "Trained batch 465 batch loss 1.48230267 epoch total loss 1.65150619\n",
      "Trained batch 466 batch loss 1.51722 epoch total loss 1.65121806\n",
      "Trained batch 467 batch loss 1.5879153 epoch total loss 1.6510824\n",
      "Trained batch 468 batch loss 1.60840285 epoch total loss 1.6509912\n",
      "Trained batch 469 batch loss 1.59759784 epoch total loss 1.65087736\n",
      "Trained batch 470 batch loss 1.70303619 epoch total loss 1.65098834\n",
      "Trained batch 471 batch loss 1.69150734 epoch total loss 1.65107441\n",
      "Trained batch 472 batch loss 1.64537036 epoch total loss 1.65106237\n",
      "Trained batch 473 batch loss 1.59760714 epoch total loss 1.65094936\n",
      "Trained batch 474 batch loss 1.61383891 epoch total loss 1.65087104\n",
      "Trained batch 475 batch loss 1.60162687 epoch total loss 1.65076733\n",
      "Trained batch 476 batch loss 1.57012486 epoch total loss 1.65059793\n",
      "Trained batch 477 batch loss 1.5019089 epoch total loss 1.6502862\n",
      "Trained batch 478 batch loss 1.31023228 epoch total loss 1.64957488\n",
      "Trained batch 479 batch loss 1.43099546 epoch total loss 1.64911842\n",
      "Trained batch 480 batch loss 1.54364228 epoch total loss 1.64889872\n",
      "Trained batch 481 batch loss 1.38732529 epoch total loss 1.64835489\n",
      "Trained batch 482 batch loss 1.59130573 epoch total loss 1.64823651\n",
      "Trained batch 483 batch loss 1.55905199 epoch total loss 1.64805198\n",
      "Trained batch 484 batch loss 1.54013157 epoch total loss 1.64782906\n",
      "Trained batch 485 batch loss 1.59576631 epoch total loss 1.64772165\n",
      "Trained batch 486 batch loss 1.53052211 epoch total loss 1.64748049\n",
      "Trained batch 487 batch loss 1.63344121 epoch total loss 1.64745164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 488 batch loss 1.50083423 epoch total loss 1.64715123\n",
      "Trained batch 489 batch loss 1.46401954 epoch total loss 1.64677668\n",
      "Trained batch 490 batch loss 1.49411774 epoch total loss 1.64646518\n",
      "Trained batch 491 batch loss 1.55402982 epoch total loss 1.64627695\n",
      "Trained batch 492 batch loss 1.43274903 epoch total loss 1.64584291\n",
      "Trained batch 493 batch loss 1.59998035 epoch total loss 1.64574981\n",
      "Trained batch 494 batch loss 1.7063694 epoch total loss 1.64587259\n",
      "Trained batch 495 batch loss 1.62057889 epoch total loss 1.64582145\n",
      "Trained batch 496 batch loss 1.46836674 epoch total loss 1.64546371\n",
      "Trained batch 497 batch loss 1.45973778 epoch total loss 1.64509\n",
      "Trained batch 498 batch loss 1.41008306 epoch total loss 1.64461815\n",
      "Trained batch 499 batch loss 1.45748043 epoch total loss 1.64424312\n",
      "Trained batch 500 batch loss 1.59814942 epoch total loss 1.64415085\n",
      "Trained batch 501 batch loss 1.36036015 epoch total loss 1.64358437\n",
      "Trained batch 502 batch loss 1.38442254 epoch total loss 1.64306808\n",
      "Trained batch 503 batch loss 1.34954643 epoch total loss 1.64248455\n",
      "Trained batch 504 batch loss 1.41240597 epoch total loss 1.64202809\n",
      "Trained batch 505 batch loss 1.57030988 epoch total loss 1.64188612\n",
      "Trained batch 506 batch loss 1.50990808 epoch total loss 1.64162517\n",
      "Trained batch 507 batch loss 1.67463374 epoch total loss 1.64169025\n",
      "Trained batch 508 batch loss 1.65401208 epoch total loss 1.64171445\n",
      "Trained batch 509 batch loss 1.6706897 epoch total loss 1.64177144\n",
      "Trained batch 510 batch loss 1.67918921 epoch total loss 1.64184487\n",
      "Trained batch 511 batch loss 1.61353827 epoch total loss 1.64178944\n",
      "Trained batch 512 batch loss 1.66735196 epoch total loss 1.64183939\n",
      "Trained batch 513 batch loss 1.61215746 epoch total loss 1.64178157\n",
      "Trained batch 514 batch loss 1.60082936 epoch total loss 1.64170194\n",
      "Trained batch 515 batch loss 1.68723011 epoch total loss 1.64179039\n",
      "Trained batch 516 batch loss 1.68398297 epoch total loss 1.64187205\n",
      "Trained batch 517 batch loss 1.61622214 epoch total loss 1.64182246\n",
      "Trained batch 518 batch loss 1.59800649 epoch total loss 1.64173794\n",
      "Trained batch 519 batch loss 1.41490114 epoch total loss 1.6413008\n",
      "Trained batch 520 batch loss 1.45965827 epoch total loss 1.64095151\n",
      "Trained batch 521 batch loss 1.56139421 epoch total loss 1.64079881\n",
      "Trained batch 522 batch loss 1.50747657 epoch total loss 1.64054334\n",
      "Trained batch 523 batch loss 1.53946471 epoch total loss 1.64035022\n",
      "Trained batch 524 batch loss 1.49778724 epoch total loss 1.64007819\n",
      "Trained batch 525 batch loss 1.42559576 epoch total loss 1.63966954\n",
      "Trained batch 526 batch loss 1.55273128 epoch total loss 1.63950431\n",
      "Trained batch 527 batch loss 1.42692947 epoch total loss 1.63910103\n",
      "Trained batch 528 batch loss 1.39954472 epoch total loss 1.6386472\n",
      "Trained batch 529 batch loss 1.37098694 epoch total loss 1.63814127\n",
      "Trained batch 530 batch loss 1.32571268 epoch total loss 1.63755167\n",
      "Trained batch 531 batch loss 1.36705899 epoch total loss 1.63704228\n",
      "Trained batch 532 batch loss 1.47212362 epoch total loss 1.63673234\n",
      "Trained batch 533 batch loss 1.3708992 epoch total loss 1.63623357\n",
      "Trained batch 534 batch loss 1.49780738 epoch total loss 1.63597429\n",
      "Trained batch 535 batch loss 1.49248993 epoch total loss 1.63570619\n",
      "Trained batch 536 batch loss 1.53458142 epoch total loss 1.63551748\n",
      "Trained batch 537 batch loss 1.45176911 epoch total loss 1.63517535\n",
      "Trained batch 538 batch loss 1.43931103 epoch total loss 1.6348114\n",
      "Trained batch 539 batch loss 1.47103405 epoch total loss 1.63450742\n",
      "Trained batch 540 batch loss 1.5883739 epoch total loss 1.63442206\n",
      "Trained batch 541 batch loss 1.64585698 epoch total loss 1.63444316\n",
      "Trained batch 542 batch loss 1.63518691 epoch total loss 1.63444459\n",
      "Trained batch 543 batch loss 1.6772958 epoch total loss 1.63452351\n",
      "Trained batch 544 batch loss 1.69588971 epoch total loss 1.63463628\n",
      "Trained batch 545 batch loss 1.62044334 epoch total loss 1.63461018\n",
      "Trained batch 546 batch loss 1.55569553 epoch total loss 1.63446569\n",
      "Trained batch 547 batch loss 1.52082682 epoch total loss 1.63425791\n",
      "Trained batch 548 batch loss 1.48728514 epoch total loss 1.63398981\n",
      "Trained batch 549 batch loss 1.55694187 epoch total loss 1.63384938\n",
      "Trained batch 550 batch loss 1.56901181 epoch total loss 1.6337316\n",
      "Trained batch 551 batch loss 1.54374993 epoch total loss 1.63356829\n",
      "Trained batch 552 batch loss 1.63304222 epoch total loss 1.63356733\n",
      "Trained batch 553 batch loss 1.4696027 epoch total loss 1.63327086\n",
      "Trained batch 554 batch loss 1.51778138 epoch total loss 1.63306236\n",
      "Trained batch 555 batch loss 1.5753988 epoch total loss 1.63295841\n",
      "Trained batch 556 batch loss 1.60569215 epoch total loss 1.63290942\n",
      "Trained batch 557 batch loss 1.54655099 epoch total loss 1.63275445\n",
      "Trained batch 558 batch loss 1.56338477 epoch total loss 1.63263011\n",
      "Trained batch 559 batch loss 1.58409476 epoch total loss 1.63254321\n",
      "Trained batch 560 batch loss 1.48101401 epoch total loss 1.63227272\n",
      "Trained batch 561 batch loss 1.53360772 epoch total loss 1.63209689\n",
      "Trained batch 562 batch loss 1.54306483 epoch total loss 1.63193846\n",
      "Trained batch 563 batch loss 1.58218884 epoch total loss 1.63185012\n",
      "Trained batch 564 batch loss 1.53980839 epoch total loss 1.63168693\n",
      "Trained batch 565 batch loss 1.52161169 epoch total loss 1.63149214\n",
      "Trained batch 566 batch loss 1.55101991 epoch total loss 1.63134992\n",
      "Trained batch 567 batch loss 1.481282 epoch total loss 1.63108516\n",
      "Trained batch 568 batch loss 1.48979402 epoch total loss 1.63083649\n",
      "Trained batch 569 batch loss 1.50075984 epoch total loss 1.63060784\n",
      "Trained batch 570 batch loss 1.51602542 epoch total loss 1.63040686\n",
      "Trained batch 571 batch loss 1.55422664 epoch total loss 1.63027334\n",
      "Trained batch 572 batch loss 1.39443421 epoch total loss 1.629861\n",
      "Trained batch 573 batch loss 1.28766465 epoch total loss 1.62926388\n",
      "Trained batch 574 batch loss 1.2508204 epoch total loss 1.62860453\n",
      "Trained batch 575 batch loss 1.53936863 epoch total loss 1.62844932\n",
      "Trained batch 576 batch loss 1.532547 epoch total loss 1.62828279\n",
      "Trained batch 577 batch loss 1.60386848 epoch total loss 1.62824047\n",
      "Trained batch 578 batch loss 1.60117805 epoch total loss 1.62819374\n",
      "Trained batch 579 batch loss 1.48382962 epoch total loss 1.62794435\n",
      "Trained batch 580 batch loss 1.61269641 epoch total loss 1.627918\n",
      "Trained batch 581 batch loss 1.44114459 epoch total loss 1.62759662\n",
      "Trained batch 582 batch loss 1.43612909 epoch total loss 1.6272676\n",
      "Trained batch 583 batch loss 1.44996643 epoch total loss 1.6269635\n",
      "Trained batch 584 batch loss 1.52341878 epoch total loss 1.62678623\n",
      "Trained batch 585 batch loss 1.37076354 epoch total loss 1.62634861\n",
      "Trained batch 586 batch loss 1.48132861 epoch total loss 1.62610114\n",
      "Trained batch 587 batch loss 1.56546211 epoch total loss 1.6259979\n",
      "Trained batch 588 batch loss 1.4921478 epoch total loss 1.62577021\n",
      "Trained batch 589 batch loss 1.60366762 epoch total loss 1.62573266\n",
      "Trained batch 590 batch loss 1.54804921 epoch total loss 1.62560093\n",
      "Trained batch 591 batch loss 1.37769961 epoch total loss 1.62518144\n",
      "Trained batch 592 batch loss 1.50964689 epoch total loss 1.62498629\n",
      "Trained batch 593 batch loss 1.51258349 epoch total loss 1.62479675\n",
      "Trained batch 594 batch loss 1.61875582 epoch total loss 1.62478662\n",
      "Trained batch 595 batch loss 1.5862869 epoch total loss 1.62472188\n",
      "Trained batch 596 batch loss 1.52053714 epoch total loss 1.624547\n",
      "Trained batch 597 batch loss 1.52446532 epoch total loss 1.6243794\n",
      "Trained batch 598 batch loss 1.52119517 epoch total loss 1.6242069\n",
      "Trained batch 599 batch loss 1.47229362 epoch total loss 1.62395322\n",
      "Trained batch 600 batch loss 1.43067598 epoch total loss 1.62363112\n",
      "Trained batch 601 batch loss 1.48044538 epoch total loss 1.62339282\n",
      "Trained batch 602 batch loss 1.46986675 epoch total loss 1.62313783\n",
      "Trained batch 603 batch loss 1.5544771 epoch total loss 1.62302399\n",
      "Trained batch 604 batch loss 1.56801105 epoch total loss 1.62293291\n",
      "Trained batch 605 batch loss 1.64277482 epoch total loss 1.62296569\n",
      "Trained batch 606 batch loss 1.57931364 epoch total loss 1.62289357\n",
      "Trained batch 607 batch loss 1.53495038 epoch total loss 1.62274873\n",
      "Trained batch 608 batch loss 1.64811158 epoch total loss 1.62279046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 609 batch loss 1.51471221 epoch total loss 1.62261307\n",
      "Trained batch 610 batch loss 1.58637381 epoch total loss 1.62255359\n",
      "Trained batch 611 batch loss 1.55592787 epoch total loss 1.62244451\n",
      "Trained batch 612 batch loss 1.58457327 epoch total loss 1.62238264\n",
      "Trained batch 613 batch loss 1.63941932 epoch total loss 1.62241042\n",
      "Trained batch 614 batch loss 1.62287688 epoch total loss 1.62241113\n",
      "Trained batch 615 batch loss 1.54974508 epoch total loss 1.622293\n",
      "Trained batch 616 batch loss 1.44672704 epoch total loss 1.62200797\n",
      "Trained batch 617 batch loss 1.36190021 epoch total loss 1.62158644\n",
      "Trained batch 618 batch loss 1.41405058 epoch total loss 1.62125063\n",
      "Trained batch 619 batch loss 1.38984931 epoch total loss 1.62087667\n",
      "Trained batch 620 batch loss 1.35753584 epoch total loss 1.62045205\n",
      "Trained batch 621 batch loss 1.30341184 epoch total loss 1.61994147\n",
      "Trained batch 622 batch loss 1.31689703 epoch total loss 1.61945426\n",
      "Trained batch 623 batch loss 1.24580026 epoch total loss 1.6188544\n",
      "Trained batch 624 batch loss 1.43431294 epoch total loss 1.61855876\n",
      "Trained batch 625 batch loss 1.56366634 epoch total loss 1.61847091\n",
      "Trained batch 626 batch loss 1.56948924 epoch total loss 1.61839271\n",
      "Trained batch 627 batch loss 1.45270777 epoch total loss 1.61812842\n",
      "Trained batch 628 batch loss 1.51786876 epoch total loss 1.6179688\n",
      "Trained batch 629 batch loss 1.56382871 epoch total loss 1.61788273\n",
      "Trained batch 630 batch loss 1.60244179 epoch total loss 1.61785817\n",
      "Trained batch 631 batch loss 1.53548121 epoch total loss 1.61772764\n",
      "Trained batch 632 batch loss 1.48225808 epoch total loss 1.6175133\n",
      "Trained batch 633 batch loss 1.30502439 epoch total loss 1.61701965\n",
      "Trained batch 634 batch loss 1.34867501 epoch total loss 1.61659646\n",
      "Trained batch 635 batch loss 1.47337842 epoch total loss 1.61637092\n",
      "Trained batch 636 batch loss 1.48181844 epoch total loss 1.61615932\n",
      "Trained batch 637 batch loss 1.51095974 epoch total loss 1.61599422\n",
      "Trained batch 638 batch loss 1.58808041 epoch total loss 1.61595058\n",
      "Trained batch 639 batch loss 1.53604043 epoch total loss 1.61582541\n",
      "Trained batch 640 batch loss 1.51185274 epoch total loss 1.61566293\n",
      "Trained batch 641 batch loss 1.57550216 epoch total loss 1.61560035\n",
      "Trained batch 642 batch loss 1.55465627 epoch total loss 1.61550546\n",
      "Trained batch 643 batch loss 1.50392747 epoch total loss 1.61533201\n",
      "Trained batch 644 batch loss 1.54798222 epoch total loss 1.61522734\n",
      "Trained batch 645 batch loss 1.5437851 epoch total loss 1.61511672\n",
      "Trained batch 646 batch loss 1.42832971 epoch total loss 1.61482751\n",
      "Trained batch 647 batch loss 1.47163892 epoch total loss 1.61460626\n",
      "Trained batch 648 batch loss 1.48807478 epoch total loss 1.614411\n",
      "Trained batch 649 batch loss 1.43475175 epoch total loss 1.61413407\n",
      "Trained batch 650 batch loss 1.47171855 epoch total loss 1.61391485\n",
      "Trained batch 651 batch loss 1.39637887 epoch total loss 1.6135807\n",
      "Trained batch 652 batch loss 1.42706215 epoch total loss 1.6132946\n",
      "Trained batch 653 batch loss 1.63613594 epoch total loss 1.61332953\n",
      "Trained batch 654 batch loss 1.49289727 epoch total loss 1.61314535\n",
      "Trained batch 655 batch loss 1.49247992 epoch total loss 1.61296105\n",
      "Trained batch 656 batch loss 1.49904847 epoch total loss 1.61278737\n",
      "Trained batch 657 batch loss 1.53786016 epoch total loss 1.61267328\n",
      "Trained batch 658 batch loss 1.52480471 epoch total loss 1.61253977\n",
      "Trained batch 659 batch loss 1.64726114 epoch total loss 1.61259234\n",
      "Trained batch 660 batch loss 1.60377288 epoch total loss 1.61257899\n",
      "Trained batch 661 batch loss 1.54682565 epoch total loss 1.61247957\n",
      "Trained batch 662 batch loss 1.59507143 epoch total loss 1.61245334\n",
      "Trained batch 663 batch loss 1.521505 epoch total loss 1.61231613\n",
      "Trained batch 664 batch loss 1.41285515 epoch total loss 1.61201572\n",
      "Trained batch 665 batch loss 1.52815175 epoch total loss 1.6118896\n",
      "Trained batch 666 batch loss 1.53953969 epoch total loss 1.611781\n",
      "Trained batch 667 batch loss 1.63417327 epoch total loss 1.61181462\n",
      "Trained batch 668 batch loss 1.61226988 epoch total loss 1.61181533\n",
      "Trained batch 669 batch loss 1.43183613 epoch total loss 1.6115464\n",
      "Trained batch 670 batch loss 1.49541426 epoch total loss 1.61137295\n",
      "Trained batch 671 batch loss 1.47098804 epoch total loss 1.61116362\n",
      "Trained batch 672 batch loss 1.56451321 epoch total loss 1.61109412\n",
      "Trained batch 673 batch loss 1.52365303 epoch total loss 1.6109643\n",
      "Trained batch 674 batch loss 1.53250062 epoch total loss 1.61084783\n",
      "Trained batch 675 batch loss 1.45082402 epoch total loss 1.61061072\n",
      "Trained batch 676 batch loss 1.49520779 epoch total loss 1.61044\n",
      "Trained batch 677 batch loss 1.60082066 epoch total loss 1.61042583\n",
      "Trained batch 678 batch loss 1.5868119 epoch total loss 1.61039102\n",
      "Trained batch 679 batch loss 1.51299214 epoch total loss 1.61024749\n",
      "Trained batch 680 batch loss 1.49588585 epoch total loss 1.61007929\n",
      "Trained batch 681 batch loss 1.31035542 epoch total loss 1.60963905\n",
      "Trained batch 682 batch loss 1.45979726 epoch total loss 1.60941935\n",
      "Trained batch 683 batch loss 1.51381361 epoch total loss 1.60927939\n",
      "Trained batch 684 batch loss 1.5198648 epoch total loss 1.60914874\n",
      "Trained batch 685 batch loss 1.53779984 epoch total loss 1.60904455\n",
      "Trained batch 686 batch loss 1.54791522 epoch total loss 1.60895562\n",
      "Trained batch 687 batch loss 1.571118 epoch total loss 1.60890055\n",
      "Trained batch 688 batch loss 1.52904236 epoch total loss 1.60878456\n",
      "Trained batch 689 batch loss 1.55838454 epoch total loss 1.60871136\n",
      "Trained batch 690 batch loss 1.47498965 epoch total loss 1.60851753\n",
      "Trained batch 691 batch loss 1.52671111 epoch total loss 1.60839915\n",
      "Trained batch 692 batch loss 1.4836545 epoch total loss 1.60821891\n",
      "Trained batch 693 batch loss 1.50190353 epoch total loss 1.60806549\n",
      "Trained batch 694 batch loss 1.58518243 epoch total loss 1.60803258\n",
      "Trained batch 695 batch loss 1.5236094 epoch total loss 1.60791099\n",
      "Trained batch 696 batch loss 1.49526882 epoch total loss 1.6077491\n",
      "Trained batch 697 batch loss 1.32846069 epoch total loss 1.60734844\n",
      "Trained batch 698 batch loss 1.36235261 epoch total loss 1.60699737\n",
      "Trained batch 699 batch loss 1.43762994 epoch total loss 1.60675514\n",
      "Trained batch 700 batch loss 1.44170272 epoch total loss 1.60651922\n",
      "Trained batch 701 batch loss 1.38039815 epoch total loss 1.60619664\n",
      "Trained batch 702 batch loss 1.42570472 epoch total loss 1.60593951\n",
      "Trained batch 703 batch loss 1.56017625 epoch total loss 1.60587442\n",
      "Trained batch 704 batch loss 1.44185185 epoch total loss 1.60564148\n",
      "Trained batch 705 batch loss 1.51475739 epoch total loss 1.6055125\n",
      "Trained batch 706 batch loss 1.4533627 epoch total loss 1.60529709\n",
      "Trained batch 707 batch loss 1.51631904 epoch total loss 1.6051712\n",
      "Trained batch 708 batch loss 1.60641623 epoch total loss 1.60517311\n",
      "Trained batch 709 batch loss 1.3244679 epoch total loss 1.6047771\n",
      "Trained batch 710 batch loss 1.17912 epoch total loss 1.60417759\n",
      "Trained batch 711 batch loss 1.2489239 epoch total loss 1.60367787\n",
      "Trained batch 712 batch loss 1.47939467 epoch total loss 1.60350323\n",
      "Trained batch 713 batch loss 1.72798812 epoch total loss 1.60367799\n",
      "Trained batch 714 batch loss 1.70857096 epoch total loss 1.60382485\n",
      "Trained batch 715 batch loss 1.59582758 epoch total loss 1.60381377\n",
      "Trained batch 716 batch loss 1.57150233 epoch total loss 1.60376859\n",
      "Trained batch 717 batch loss 1.57028961 epoch total loss 1.60372198\n",
      "Trained batch 718 batch loss 1.54555571 epoch total loss 1.60364091\n",
      "Trained batch 719 batch loss 1.59995794 epoch total loss 1.60363579\n",
      "Trained batch 720 batch loss 1.48690462 epoch total loss 1.60347378\n",
      "Trained batch 721 batch loss 1.48244333 epoch total loss 1.60330582\n",
      "Trained batch 722 batch loss 1.51090837 epoch total loss 1.60317779\n",
      "Trained batch 723 batch loss 1.44391906 epoch total loss 1.60295761\n",
      "Trained batch 724 batch loss 1.46543527 epoch total loss 1.60276771\n",
      "Trained batch 725 batch loss 1.49734521 epoch total loss 1.60262227\n",
      "Trained batch 726 batch loss 1.42253363 epoch total loss 1.60237408\n",
      "Trained batch 727 batch loss 1.45729125 epoch total loss 1.60217452\n",
      "Trained batch 728 batch loss 1.44894016 epoch total loss 1.60196412\n",
      "Trained batch 729 batch loss 1.53668737 epoch total loss 1.60187459\n",
      "Trained batch 730 batch loss 1.54928958 epoch total loss 1.60180259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 731 batch loss 1.64263153 epoch total loss 1.60185838\n",
      "Trained batch 732 batch loss 1.6143223 epoch total loss 1.60187554\n",
      "Trained batch 733 batch loss 1.56660545 epoch total loss 1.6018275\n",
      "Trained batch 734 batch loss 1.49365473 epoch total loss 1.60168\n",
      "Trained batch 735 batch loss 1.5816195 epoch total loss 1.60165286\n",
      "Trained batch 736 batch loss 1.56888306 epoch total loss 1.60160828\n",
      "Trained batch 737 batch loss 1.56062734 epoch total loss 1.60155272\n",
      "Trained batch 738 batch loss 1.52784657 epoch total loss 1.60145283\n",
      "Trained batch 739 batch loss 1.46849346 epoch total loss 1.60127294\n",
      "Trained batch 740 batch loss 1.51203108 epoch total loss 1.60115242\n",
      "Trained batch 741 batch loss 1.51968312 epoch total loss 1.60104239\n",
      "Trained batch 742 batch loss 1.52590978 epoch total loss 1.60094118\n",
      "Trained batch 743 batch loss 1.54265642 epoch total loss 1.60086262\n",
      "Trained batch 744 batch loss 1.71517277 epoch total loss 1.60101628\n",
      "Trained batch 745 batch loss 1.66082656 epoch total loss 1.60109651\n",
      "Trained batch 746 batch loss 1.59283531 epoch total loss 1.60108554\n",
      "Trained batch 747 batch loss 1.52621198 epoch total loss 1.60098529\n",
      "Trained batch 748 batch loss 1.58706987 epoch total loss 1.60096669\n",
      "Trained batch 749 batch loss 1.57882214 epoch total loss 1.60093713\n",
      "Trained batch 750 batch loss 1.60147703 epoch total loss 1.60093784\n",
      "Trained batch 751 batch loss 1.43750966 epoch total loss 1.60072017\n",
      "Trained batch 752 batch loss 1.53496146 epoch total loss 1.60063267\n",
      "Trained batch 753 batch loss 1.49978304 epoch total loss 1.60049868\n",
      "Trained batch 754 batch loss 1.66288829 epoch total loss 1.60058141\n",
      "Trained batch 755 batch loss 1.61055529 epoch total loss 1.60059464\n",
      "Trained batch 756 batch loss 1.47366047 epoch total loss 1.60042667\n",
      "Trained batch 757 batch loss 1.48949814 epoch total loss 1.60028017\n",
      "Trained batch 758 batch loss 1.46980143 epoch total loss 1.60010815\n",
      "Trained batch 759 batch loss 1.51347899 epoch total loss 1.59999394\n",
      "Trained batch 760 batch loss 1.50468922 epoch total loss 1.59986842\n",
      "Trained batch 761 batch loss 1.46023154 epoch total loss 1.59968495\n",
      "Trained batch 762 batch loss 1.49812078 epoch total loss 1.59955168\n",
      "Trained batch 763 batch loss 1.54194486 epoch total loss 1.59947622\n",
      "Trained batch 764 batch loss 1.54028583 epoch total loss 1.59939873\n",
      "Trained batch 765 batch loss 1.58882821 epoch total loss 1.59938502\n",
      "Trained batch 766 batch loss 1.53253651 epoch total loss 1.59929788\n",
      "Trained batch 767 batch loss 1.54918516 epoch total loss 1.59923255\n",
      "Trained batch 768 batch loss 1.48425019 epoch total loss 1.59908283\n",
      "Trained batch 769 batch loss 1.46016288 epoch total loss 1.59890223\n",
      "Trained batch 770 batch loss 1.49515557 epoch total loss 1.5987674\n",
      "Trained batch 771 batch loss 1.56223464 epoch total loss 1.59872007\n",
      "Trained batch 772 batch loss 1.47108102 epoch total loss 1.59855473\n",
      "Trained batch 773 batch loss 1.52988958 epoch total loss 1.59846592\n",
      "Trained batch 774 batch loss 1.5481112 epoch total loss 1.59840083\n",
      "Trained batch 775 batch loss 1.55379629 epoch total loss 1.59834325\n",
      "Trained batch 776 batch loss 1.47443581 epoch total loss 1.59818375\n",
      "Trained batch 777 batch loss 1.35574794 epoch total loss 1.59787166\n",
      "Trained batch 778 batch loss 1.47969294 epoch total loss 1.59771979\n",
      "Trained batch 779 batch loss 1.58190346 epoch total loss 1.59769952\n",
      "Trained batch 780 batch loss 1.5866046 epoch total loss 1.59768522\n",
      "Trained batch 781 batch loss 1.53190374 epoch total loss 1.59760094\n",
      "Trained batch 782 batch loss 1.50047612 epoch total loss 1.59747672\n",
      "Trained batch 783 batch loss 1.46332872 epoch total loss 1.59730542\n",
      "Trained batch 784 batch loss 1.49257827 epoch total loss 1.5971719\n",
      "Trained batch 785 batch loss 1.52965784 epoch total loss 1.59708583\n",
      "Trained batch 786 batch loss 1.45286775 epoch total loss 1.59690237\n",
      "Trained batch 787 batch loss 1.53289127 epoch total loss 1.59682095\n",
      "Trained batch 788 batch loss 1.4835664 epoch total loss 1.59667718\n",
      "Trained batch 789 batch loss 1.46046138 epoch total loss 1.59650457\n",
      "Trained batch 790 batch loss 1.43408489 epoch total loss 1.59629893\n",
      "Trained batch 791 batch loss 1.5681752 epoch total loss 1.59626329\n",
      "Trained batch 792 batch loss 1.5794245 epoch total loss 1.59624207\n",
      "Trained batch 793 batch loss 1.576 epoch total loss 1.59621668\n",
      "Trained batch 794 batch loss 1.52906179 epoch total loss 1.59613204\n",
      "Trained batch 795 batch loss 1.67767787 epoch total loss 1.59623468\n",
      "Trained batch 796 batch loss 1.5539856 epoch total loss 1.59618163\n",
      "Trained batch 797 batch loss 1.4537729 epoch total loss 1.59600282\n",
      "Trained batch 798 batch loss 1.49419892 epoch total loss 1.59587526\n",
      "Trained batch 799 batch loss 1.49187422 epoch total loss 1.59574497\n",
      "Trained batch 800 batch loss 1.38338554 epoch total loss 1.59547961\n",
      "Trained batch 801 batch loss 1.44030976 epoch total loss 1.59528589\n",
      "Trained batch 802 batch loss 1.44541693 epoch total loss 1.59509897\n",
      "Trained batch 803 batch loss 1.3570447 epoch total loss 1.59480262\n",
      "Trained batch 804 batch loss 1.3238287 epoch total loss 1.59446561\n",
      "Trained batch 805 batch loss 1.45415533 epoch total loss 1.59429121\n",
      "Trained batch 806 batch loss 1.60282242 epoch total loss 1.5943017\n",
      "Trained batch 807 batch loss 1.6152637 epoch total loss 1.59432769\n",
      "Trained batch 808 batch loss 1.6098429 epoch total loss 1.59434688\n",
      "Trained batch 809 batch loss 1.6168344 epoch total loss 1.59437466\n",
      "Trained batch 810 batch loss 1.60386682 epoch total loss 1.59438646\n",
      "Trained batch 811 batch loss 1.5492605 epoch total loss 1.59433091\n",
      "Trained batch 812 batch loss 1.53280187 epoch total loss 1.59425509\n",
      "Trained batch 813 batch loss 1.63356221 epoch total loss 1.59430349\n",
      "Trained batch 814 batch loss 1.51639628 epoch total loss 1.59420764\n",
      "Trained batch 815 batch loss 1.49421751 epoch total loss 1.5940851\n",
      "Trained batch 816 batch loss 1.54617488 epoch total loss 1.59402633\n",
      "Trained batch 817 batch loss 1.35509753 epoch total loss 1.59373391\n",
      "Trained batch 818 batch loss 1.38504422 epoch total loss 1.59347868\n",
      "Trained batch 819 batch loss 1.47829056 epoch total loss 1.59333801\n",
      "Trained batch 820 batch loss 1.54201007 epoch total loss 1.59327543\n",
      "Trained batch 821 batch loss 1.55052447 epoch total loss 1.59322333\n",
      "Trained batch 822 batch loss 1.54723501 epoch total loss 1.59316742\n",
      "Trained batch 823 batch loss 1.58889103 epoch total loss 1.59316218\n",
      "Trained batch 824 batch loss 1.4153192 epoch total loss 1.59294629\n",
      "Trained batch 825 batch loss 1.49923813 epoch total loss 1.5928328\n",
      "Trained batch 826 batch loss 1.46928012 epoch total loss 1.5926832\n",
      "Trained batch 827 batch loss 1.46805084 epoch total loss 1.5925324\n",
      "Trained batch 828 batch loss 1.54371941 epoch total loss 1.59247339\n",
      "Trained batch 829 batch loss 1.54063022 epoch total loss 1.59241092\n",
      "Trained batch 830 batch loss 1.54821 epoch total loss 1.59235764\n",
      "Trained batch 831 batch loss 1.55884576 epoch total loss 1.59231734\n",
      "Trained batch 832 batch loss 1.50446749 epoch total loss 1.59221184\n",
      "Trained batch 833 batch loss 1.53148353 epoch total loss 1.59213889\n",
      "Trained batch 834 batch loss 1.545205 epoch total loss 1.59208262\n",
      "Trained batch 835 batch loss 1.5566771 epoch total loss 1.59204018\n",
      "Trained batch 836 batch loss 1.50561726 epoch total loss 1.59193671\n",
      "Trained batch 837 batch loss 1.47268033 epoch total loss 1.59179425\n",
      "Trained batch 838 batch loss 1.52915287 epoch total loss 1.59171951\n",
      "Trained batch 839 batch loss 1.36655641 epoch total loss 1.59145117\n",
      "Trained batch 840 batch loss 1.42835307 epoch total loss 1.59125698\n",
      "Trained batch 841 batch loss 1.46855927 epoch total loss 1.59111106\n",
      "Trained batch 842 batch loss 1.53023016 epoch total loss 1.59103882\n",
      "Trained batch 843 batch loss 1.52792907 epoch total loss 1.59096396\n",
      "Trained batch 844 batch loss 1.44224763 epoch total loss 1.59078777\n",
      "Trained batch 845 batch loss 1.36157572 epoch total loss 1.59051657\n",
      "Trained batch 846 batch loss 1.46858919 epoch total loss 1.59037244\n",
      "Trained batch 847 batch loss 1.47807992 epoch total loss 1.59023976\n",
      "Trained batch 848 batch loss 1.49183416 epoch total loss 1.59012377\n",
      "Trained batch 849 batch loss 1.57451141 epoch total loss 1.5901053\n",
      "Trained batch 850 batch loss 1.7608583 epoch total loss 1.59030616\n",
      "Trained batch 851 batch loss 1.75272477 epoch total loss 1.59049702\n",
      "Trained batch 852 batch loss 1.67241323 epoch total loss 1.5905931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 853 batch loss 1.56141019 epoch total loss 1.59055889\n",
      "Trained batch 854 batch loss 1.52040136 epoch total loss 1.59047663\n",
      "Trained batch 855 batch loss 1.41629124 epoch total loss 1.5902729\n",
      "Trained batch 856 batch loss 1.43113899 epoch total loss 1.59008706\n",
      "Trained batch 857 batch loss 1.4999404 epoch total loss 1.58998191\n",
      "Trained batch 858 batch loss 1.64308834 epoch total loss 1.59004378\n",
      "Trained batch 859 batch loss 1.44006383 epoch total loss 1.58986914\n",
      "Trained batch 860 batch loss 1.42398298 epoch total loss 1.58967626\n",
      "Trained batch 861 batch loss 1.49019051 epoch total loss 1.58956075\n",
      "Trained batch 862 batch loss 1.48947859 epoch total loss 1.58944464\n",
      "Trained batch 863 batch loss 1.43008161 epoch total loss 1.58926\n",
      "Trained batch 864 batch loss 1.48249984 epoch total loss 1.58913648\n",
      "Trained batch 865 batch loss 1.5177002 epoch total loss 1.58905387\n",
      "Trained batch 866 batch loss 1.46577501 epoch total loss 1.58891153\n",
      "Trained batch 867 batch loss 1.50187826 epoch total loss 1.58881116\n",
      "Trained batch 868 batch loss 1.46629894 epoch total loss 1.58867\n",
      "Trained batch 869 batch loss 1.56247973 epoch total loss 1.58863986\n",
      "Trained batch 870 batch loss 1.56221986 epoch total loss 1.58860958\n",
      "Trained batch 871 batch loss 1.48742986 epoch total loss 1.58849347\n",
      "Trained batch 872 batch loss 1.47695065 epoch total loss 1.58836544\n",
      "Trained batch 873 batch loss 1.37472248 epoch total loss 1.58812082\n",
      "Trained batch 874 batch loss 1.53149843 epoch total loss 1.58805597\n",
      "Trained batch 875 batch loss 1.49805212 epoch total loss 1.58795309\n",
      "Trained batch 876 batch loss 1.50070655 epoch total loss 1.58785355\n",
      "Trained batch 877 batch loss 1.56679416 epoch total loss 1.58782947\n",
      "Trained batch 878 batch loss 1.52061629 epoch total loss 1.58775294\n",
      "Trained batch 879 batch loss 1.49471271 epoch total loss 1.5876472\n",
      "Trained batch 880 batch loss 1.43068147 epoch total loss 1.58746874\n",
      "Trained batch 881 batch loss 1.41779208 epoch total loss 1.58727622\n",
      "Trained batch 882 batch loss 1.55052817 epoch total loss 1.58723462\n",
      "Trained batch 883 batch loss 1.56110871 epoch total loss 1.58720505\n",
      "Trained batch 884 batch loss 1.50480032 epoch total loss 1.58711183\n",
      "Trained batch 885 batch loss 1.54068387 epoch total loss 1.58705926\n",
      "Trained batch 886 batch loss 1.53721094 epoch total loss 1.58700311\n",
      "Trained batch 887 batch loss 1.27831328 epoch total loss 1.58665502\n",
      "Trained batch 888 batch loss 1.45922375 epoch total loss 1.58651161\n",
      "Trained batch 889 batch loss 1.48960352 epoch total loss 1.58640254\n",
      "Trained batch 890 batch loss 1.46893132 epoch total loss 1.58627057\n",
      "Trained batch 891 batch loss 1.53221416 epoch total loss 1.58620989\n",
      "Trained batch 892 batch loss 1.67658162 epoch total loss 1.58631122\n",
      "Trained batch 893 batch loss 1.43895817 epoch total loss 1.58614624\n",
      "Trained batch 894 batch loss 1.49954844 epoch total loss 1.58604932\n",
      "Trained batch 895 batch loss 1.52631164 epoch total loss 1.58598268\n",
      "Trained batch 896 batch loss 1.49851823 epoch total loss 1.58588505\n",
      "Trained batch 897 batch loss 1.48666787 epoch total loss 1.58577442\n",
      "Trained batch 898 batch loss 1.5166086 epoch total loss 1.58569741\n",
      "Trained batch 899 batch loss 1.54402018 epoch total loss 1.58565116\n",
      "Trained batch 900 batch loss 1.3728596 epoch total loss 1.58541465\n",
      "Trained batch 901 batch loss 1.4756813 epoch total loss 1.58529282\n",
      "Trained batch 902 batch loss 1.55391669 epoch total loss 1.58525813\n",
      "Trained batch 903 batch loss 1.50394762 epoch total loss 1.585168\n",
      "Trained batch 904 batch loss 1.43929446 epoch total loss 1.58500671\n",
      "Trained batch 905 batch loss 1.50875866 epoch total loss 1.58492255\n",
      "Trained batch 906 batch loss 1.46767926 epoch total loss 1.58479309\n",
      "Trained batch 907 batch loss 1.50474417 epoch total loss 1.58470488\n",
      "Trained batch 908 batch loss 1.51945138 epoch total loss 1.58463287\n",
      "Trained batch 909 batch loss 1.53055561 epoch total loss 1.58457339\n",
      "Trained batch 910 batch loss 1.52738595 epoch total loss 1.58451045\n",
      "Trained batch 911 batch loss 1.39992619 epoch total loss 1.58430779\n",
      "Trained batch 912 batch loss 1.54162073 epoch total loss 1.58426106\n",
      "Trained batch 913 batch loss 1.54047775 epoch total loss 1.58421314\n",
      "Trained batch 914 batch loss 1.58666229 epoch total loss 1.58421588\n",
      "Trained batch 915 batch loss 1.54056954 epoch total loss 1.58416808\n",
      "Trained batch 916 batch loss 1.7089752 epoch total loss 1.58430433\n",
      "Trained batch 917 batch loss 1.5702728 epoch total loss 1.58428907\n",
      "Trained batch 918 batch loss 1.53240752 epoch total loss 1.58423245\n",
      "Trained batch 919 batch loss 1.51957071 epoch total loss 1.58416212\n",
      "Trained batch 920 batch loss 1.33808017 epoch total loss 1.58389473\n",
      "Trained batch 921 batch loss 1.26150417 epoch total loss 1.58354461\n",
      "Trained batch 922 batch loss 1.47481465 epoch total loss 1.58342671\n",
      "Trained batch 923 batch loss 1.24949288 epoch total loss 1.58306491\n",
      "Trained batch 924 batch loss 1.18565714 epoch total loss 1.58263481\n",
      "Trained batch 925 batch loss 1.23721611 epoch total loss 1.58226144\n",
      "Trained batch 926 batch loss 1.29694104 epoch total loss 1.58195329\n",
      "Trained batch 927 batch loss 1.33175528 epoch total loss 1.58168352\n",
      "Trained batch 928 batch loss 1.45551062 epoch total loss 1.58154762\n",
      "Trained batch 929 batch loss 1.53860152 epoch total loss 1.58150136\n",
      "Trained batch 930 batch loss 1.57627153 epoch total loss 1.58149576\n",
      "Trained batch 931 batch loss 1.55627275 epoch total loss 1.58146858\n",
      "Trained batch 932 batch loss 1.6181829 epoch total loss 1.58150804\n",
      "Trained batch 933 batch loss 1.53170395 epoch total loss 1.58145463\n",
      "Trained batch 934 batch loss 1.5628289 epoch total loss 1.58143473\n",
      "Trained batch 935 batch loss 1.55834627 epoch total loss 1.58141\n",
      "Trained batch 936 batch loss 1.4639442 epoch total loss 1.58128464\n",
      "Trained batch 937 batch loss 1.47810912 epoch total loss 1.58117449\n",
      "Trained batch 938 batch loss 1.4310894 epoch total loss 1.58101451\n",
      "Trained batch 939 batch loss 1.48306739 epoch total loss 1.58091009\n",
      "Trained batch 940 batch loss 1.59777701 epoch total loss 1.58092809\n",
      "Trained batch 941 batch loss 1.56079602 epoch total loss 1.58090663\n",
      "Trained batch 942 batch loss 1.66053712 epoch total loss 1.58099115\n",
      "Trained batch 943 batch loss 1.54274 epoch total loss 1.58095062\n",
      "Trained batch 944 batch loss 1.60386419 epoch total loss 1.58097494\n",
      "Trained batch 945 batch loss 1.56673074 epoch total loss 1.58095992\n",
      "Trained batch 946 batch loss 1.460953 epoch total loss 1.58083296\n",
      "Trained batch 947 batch loss 1.47161222 epoch total loss 1.58071756\n",
      "Trained batch 948 batch loss 1.51711273 epoch total loss 1.58065045\n",
      "Trained batch 949 batch loss 1.53911614 epoch total loss 1.5806067\n",
      "Trained batch 950 batch loss 1.53826022 epoch total loss 1.580562\n",
      "Trained batch 951 batch loss 1.5476222 epoch total loss 1.58052742\n",
      "Trained batch 952 batch loss 1.5140121 epoch total loss 1.58045757\n",
      "Trained batch 953 batch loss 1.48622298 epoch total loss 1.58035862\n",
      "Trained batch 954 batch loss 1.45402825 epoch total loss 1.58022618\n",
      "Trained batch 955 batch loss 1.25614762 epoch total loss 1.57988679\n",
      "Trained batch 956 batch loss 1.23158383 epoch total loss 1.57952237\n",
      "Trained batch 957 batch loss 1.31463122 epoch total loss 1.57924557\n",
      "Trained batch 958 batch loss 1.49630737 epoch total loss 1.57915902\n",
      "Trained batch 959 batch loss 1.67664242 epoch total loss 1.57926071\n",
      "Trained batch 960 batch loss 1.74823058 epoch total loss 1.57943678\n",
      "Trained batch 961 batch loss 1.44053721 epoch total loss 1.57929218\n",
      "Trained batch 962 batch loss 1.45024037 epoch total loss 1.57915807\n",
      "Trained batch 963 batch loss 1.50022328 epoch total loss 1.57907605\n",
      "Trained batch 964 batch loss 1.56333137 epoch total loss 1.57905972\n",
      "Trained batch 965 batch loss 1.60307443 epoch total loss 1.57908463\n",
      "Trained batch 966 batch loss 1.48139524 epoch total loss 1.57898355\n",
      "Trained batch 967 batch loss 1.52767634 epoch total loss 1.5789305\n",
      "Trained batch 968 batch loss 1.56269813 epoch total loss 1.57891381\n",
      "Trained batch 969 batch loss 1.50128067 epoch total loss 1.57883358\n",
      "Trained batch 970 batch loss 1.53347337 epoch total loss 1.57878685\n",
      "Trained batch 971 batch loss 1.39265168 epoch total loss 1.57859516\n",
      "Trained batch 972 batch loss 1.52512646 epoch total loss 1.57854021\n",
      "Trained batch 973 batch loss 1.56776547 epoch total loss 1.57852912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 974 batch loss 1.44691575 epoch total loss 1.57839394\n",
      "Trained batch 975 batch loss 1.37109685 epoch total loss 1.57818139\n",
      "Trained batch 976 batch loss 1.45415962 epoch total loss 1.57805419\n",
      "Trained batch 977 batch loss 1.34552372 epoch total loss 1.57781625\n",
      "Trained batch 978 batch loss 1.38234949 epoch total loss 1.57761633\n",
      "Trained batch 979 batch loss 1.51372159 epoch total loss 1.57755101\n",
      "Trained batch 980 batch loss 1.52538192 epoch total loss 1.57749784\n",
      "Trained batch 981 batch loss 1.49763513 epoch total loss 1.57741642\n",
      "Trained batch 982 batch loss 1.41310871 epoch total loss 1.57724917\n",
      "Trained batch 983 batch loss 1.39468193 epoch total loss 1.57706332\n",
      "Trained batch 984 batch loss 1.45947933 epoch total loss 1.57694387\n",
      "Trained batch 985 batch loss 1.3794477 epoch total loss 1.57674336\n",
      "Trained batch 986 batch loss 1.43225503 epoch total loss 1.57659674\n",
      "Trained batch 987 batch loss 1.35580325 epoch total loss 1.5763731\n",
      "Trained batch 988 batch loss 1.41315877 epoch total loss 1.576208\n",
      "Trained batch 989 batch loss 1.44051 epoch total loss 1.57607079\n",
      "Trained batch 990 batch loss 1.27808738 epoch total loss 1.57576978\n",
      "Trained batch 991 batch loss 1.40591538 epoch total loss 1.57559836\n",
      "Trained batch 992 batch loss 1.53173101 epoch total loss 1.57555413\n",
      "Trained batch 993 batch loss 1.34137523 epoch total loss 1.57531834\n",
      "Trained batch 994 batch loss 1.55084574 epoch total loss 1.57529378\n",
      "Trained batch 995 batch loss 1.54133916 epoch total loss 1.57525969\n",
      "Trained batch 996 batch loss 1.4589479 epoch total loss 1.57514298\n",
      "Trained batch 997 batch loss 1.45783389 epoch total loss 1.57502532\n",
      "Trained batch 998 batch loss 1.40191555 epoch total loss 1.57485187\n",
      "Trained batch 999 batch loss 1.48098624 epoch total loss 1.57475781\n",
      "Trained batch 1000 batch loss 1.49167597 epoch total loss 1.57467484\n",
      "Trained batch 1001 batch loss 1.45635152 epoch total loss 1.57455659\n",
      "Trained batch 1002 batch loss 1.58459926 epoch total loss 1.5745666\n",
      "Trained batch 1003 batch loss 1.5273006 epoch total loss 1.57451952\n",
      "Trained batch 1004 batch loss 1.54169512 epoch total loss 1.57448685\n",
      "Trained batch 1005 batch loss 1.49505949 epoch total loss 1.57440782\n",
      "Trained batch 1006 batch loss 1.47033358 epoch total loss 1.57430446\n",
      "Trained batch 1007 batch loss 1.56188476 epoch total loss 1.57429206\n",
      "Trained batch 1008 batch loss 1.44898522 epoch total loss 1.57416773\n",
      "Trained batch 1009 batch loss 1.57201719 epoch total loss 1.57416558\n",
      "Trained batch 1010 batch loss 1.57399774 epoch total loss 1.57416546\n",
      "Trained batch 1011 batch loss 1.6987617 epoch total loss 1.57428861\n",
      "Trained batch 1012 batch loss 1.63692248 epoch total loss 1.5743506\n",
      "Trained batch 1013 batch loss 1.68175197 epoch total loss 1.57445657\n",
      "Trained batch 1014 batch loss 1.57563388 epoch total loss 1.57445788\n",
      "Trained batch 1015 batch loss 1.54523993 epoch total loss 1.57442915\n",
      "Trained batch 1016 batch loss 1.64547026 epoch total loss 1.57449901\n",
      "Trained batch 1017 batch loss 1.60327435 epoch total loss 1.57452738\n",
      "Trained batch 1018 batch loss 1.52277088 epoch total loss 1.5744766\n",
      "Trained batch 1019 batch loss 1.39765215 epoch total loss 1.57430303\n",
      "Trained batch 1020 batch loss 1.49803615 epoch total loss 1.57422829\n",
      "Trained batch 1021 batch loss 1.43969643 epoch total loss 1.57409656\n",
      "Trained batch 1022 batch loss 1.45903504 epoch total loss 1.57398391\n",
      "Trained batch 1023 batch loss 1.45438707 epoch total loss 1.57386696\n",
      "Trained batch 1024 batch loss 1.51647949 epoch total loss 1.57381094\n",
      "Trained batch 1025 batch loss 1.3785845 epoch total loss 1.57362044\n",
      "Trained batch 1026 batch loss 1.40479434 epoch total loss 1.57345581\n",
      "Trained batch 1027 batch loss 1.52177572 epoch total loss 1.5734055\n",
      "Trained batch 1028 batch loss 1.51075912 epoch total loss 1.57334459\n",
      "Trained batch 1029 batch loss 1.34472549 epoch total loss 1.57312238\n",
      "Trained batch 1030 batch loss 1.28538358 epoch total loss 1.57284307\n",
      "Trained batch 1031 batch loss 1.34112501 epoch total loss 1.57261825\n",
      "Trained batch 1032 batch loss 1.38777924 epoch total loss 1.57243919\n",
      "Trained batch 1033 batch loss 1.36878645 epoch total loss 1.57224202\n",
      "Trained batch 1034 batch loss 1.40962458 epoch total loss 1.57208478\n",
      "Trained batch 1035 batch loss 1.48464704 epoch total loss 1.57200027\n",
      "Trained batch 1036 batch loss 1.45809937 epoch total loss 1.57189035\n",
      "Trained batch 1037 batch loss 1.62451839 epoch total loss 1.57194114\n",
      "Trained batch 1038 batch loss 1.4038595 epoch total loss 1.57177913\n",
      "Trained batch 1039 batch loss 1.65773153 epoch total loss 1.57186186\n",
      "Trained batch 1040 batch loss 1.5904566 epoch total loss 1.57187974\n",
      "Trained batch 1041 batch loss 1.6619724 epoch total loss 1.57196629\n",
      "Trained batch 1042 batch loss 1.62157583 epoch total loss 1.57201385\n",
      "Trained batch 1043 batch loss 1.42979479 epoch total loss 1.57187748\n",
      "Trained batch 1044 batch loss 1.46755695 epoch total loss 1.57177758\n",
      "Trained batch 1045 batch loss 1.61935747 epoch total loss 1.57182312\n",
      "Trained batch 1046 batch loss 1.50563085 epoch total loss 1.57175982\n",
      "Trained batch 1047 batch loss 1.48480237 epoch total loss 1.57167685\n",
      "Trained batch 1048 batch loss 1.47516346 epoch total loss 1.57158482\n",
      "Trained batch 1049 batch loss 1.51642537 epoch total loss 1.57153225\n",
      "Trained batch 1050 batch loss 1.46385598 epoch total loss 1.57142973\n",
      "Trained batch 1051 batch loss 1.45625 epoch total loss 1.57132018\n",
      "Trained batch 1052 batch loss 1.54752278 epoch total loss 1.57129753\n",
      "Trained batch 1053 batch loss 1.58165848 epoch total loss 1.57130742\n",
      "Trained batch 1054 batch loss 1.58954966 epoch total loss 1.57132471\n",
      "Trained batch 1055 batch loss 1.53258312 epoch total loss 1.57128799\n",
      "Trained batch 1056 batch loss 1.54974151 epoch total loss 1.5712676\n",
      "Trained batch 1057 batch loss 1.5567441 epoch total loss 1.5712539\n",
      "Trained batch 1058 batch loss 1.50289404 epoch total loss 1.57118928\n",
      "Trained batch 1059 batch loss 1.47354817 epoch total loss 1.57109702\n",
      "Trained batch 1060 batch loss 1.51112866 epoch total loss 1.57104039\n",
      "Trained batch 1061 batch loss 1.50221503 epoch total loss 1.57097554\n",
      "Trained batch 1062 batch loss 1.49010205 epoch total loss 1.57089937\n",
      "Trained batch 1063 batch loss 1.45721841 epoch total loss 1.57079256\n",
      "Trained batch 1064 batch loss 1.45184565 epoch total loss 1.57068074\n",
      "Trained batch 1065 batch loss 1.50707924 epoch total loss 1.57062101\n",
      "Trained batch 1066 batch loss 1.54881489 epoch total loss 1.57060063\n",
      "Trained batch 1067 batch loss 1.5698216 epoch total loss 1.57059991\n",
      "Trained batch 1068 batch loss 1.64933467 epoch total loss 1.57067358\n",
      "Trained batch 1069 batch loss 1.63124251 epoch total loss 1.57073021\n",
      "Trained batch 1070 batch loss 1.58108842 epoch total loss 1.57073987\n",
      "Trained batch 1071 batch loss 1.3881427 epoch total loss 1.5705694\n",
      "Trained batch 1072 batch loss 1.43546808 epoch total loss 1.57044339\n",
      "Trained batch 1073 batch loss 1.42977941 epoch total loss 1.57031226\n",
      "Trained batch 1074 batch loss 1.45508194 epoch total loss 1.57020497\n",
      "Trained batch 1075 batch loss 1.41769552 epoch total loss 1.57006311\n",
      "Trained batch 1076 batch loss 1.44126844 epoch total loss 1.56994343\n",
      "Trained batch 1077 batch loss 1.50255239 epoch total loss 1.56988084\n",
      "Trained batch 1078 batch loss 1.44096398 epoch total loss 1.56976128\n",
      "Trained batch 1079 batch loss 1.37348378 epoch total loss 1.56957936\n",
      "Trained batch 1080 batch loss 1.39955914 epoch total loss 1.56942201\n",
      "Trained batch 1081 batch loss 1.47978377 epoch total loss 1.56933904\n",
      "Trained batch 1082 batch loss 1.48127496 epoch total loss 1.56925762\n",
      "Trained batch 1083 batch loss 1.45624948 epoch total loss 1.56915331\n",
      "Trained batch 1084 batch loss 1.37487411 epoch total loss 1.56897414\n",
      "Trained batch 1085 batch loss 1.27401936 epoch total loss 1.56870234\n",
      "Trained batch 1086 batch loss 1.38538516 epoch total loss 1.56853354\n",
      "Trained batch 1087 batch loss 1.69318485 epoch total loss 1.56864822\n",
      "Trained batch 1088 batch loss 1.59360933 epoch total loss 1.56867123\n",
      "Trained batch 1089 batch loss 1.60004854 epoch total loss 1.56870008\n",
      "Trained batch 1090 batch loss 1.6069001 epoch total loss 1.56873512\n",
      "Trained batch 1091 batch loss 1.58279133 epoch total loss 1.568748\n",
      "Trained batch 1092 batch loss 1.46812952 epoch total loss 1.56865585\n",
      "Trained batch 1093 batch loss 1.36908078 epoch total loss 1.56847334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1094 batch loss 1.4438796 epoch total loss 1.56835938\n",
      "Trained batch 1095 batch loss 1.51571131 epoch total loss 1.56831133\n",
      "Trained batch 1096 batch loss 1.46383536 epoch total loss 1.56821609\n",
      "Trained batch 1097 batch loss 1.58743382 epoch total loss 1.56823349\n",
      "Trained batch 1098 batch loss 1.50344729 epoch total loss 1.56817448\n",
      "Trained batch 1099 batch loss 1.549896 epoch total loss 1.56815791\n",
      "Trained batch 1100 batch loss 1.50816059 epoch total loss 1.56810331\n",
      "Trained batch 1101 batch loss 1.59148288 epoch total loss 1.56812453\n",
      "Trained batch 1102 batch loss 1.59325683 epoch total loss 1.5681473\n",
      "Trained batch 1103 batch loss 1.53757095 epoch total loss 1.56811965\n",
      "Trained batch 1104 batch loss 1.58921754 epoch total loss 1.56813884\n",
      "Trained batch 1105 batch loss 1.5822165 epoch total loss 1.56815159\n",
      "Trained batch 1106 batch loss 1.44427669 epoch total loss 1.56803966\n",
      "Trained batch 1107 batch loss 1.40976369 epoch total loss 1.56789672\n",
      "Trained batch 1108 batch loss 1.59554935 epoch total loss 1.56792164\n",
      "Trained batch 1109 batch loss 1.51334357 epoch total loss 1.56787241\n",
      "Trained batch 1110 batch loss 1.54755497 epoch total loss 1.56785417\n",
      "Trained batch 1111 batch loss 1.60718751 epoch total loss 1.56788957\n",
      "Trained batch 1112 batch loss 1.53748131 epoch total loss 1.56786215\n",
      "Trained batch 1113 batch loss 1.48125792 epoch total loss 1.56778431\n",
      "Trained batch 1114 batch loss 1.55407274 epoch total loss 1.56777203\n",
      "Trained batch 1115 batch loss 1.42743576 epoch total loss 1.56764627\n",
      "Trained batch 1116 batch loss 1.44065917 epoch total loss 1.56753242\n",
      "Trained batch 1117 batch loss 1.52263 epoch total loss 1.56749225\n",
      "Trained batch 1118 batch loss 1.46559799 epoch total loss 1.56740105\n",
      "Trained batch 1119 batch loss 1.42703235 epoch total loss 1.56727552\n",
      "Trained batch 1120 batch loss 1.33945286 epoch total loss 1.56707215\n",
      "Trained batch 1121 batch loss 1.37831533 epoch total loss 1.56690383\n",
      "Trained batch 1122 batch loss 1.305866 epoch total loss 1.56667113\n",
      "Trained batch 1123 batch loss 1.26210189 epoch total loss 1.56639993\n",
      "Trained batch 1124 batch loss 1.40011406 epoch total loss 1.56625199\n",
      "Trained batch 1125 batch loss 1.49182868 epoch total loss 1.56618583\n",
      "Trained batch 1126 batch loss 1.65971756 epoch total loss 1.56626892\n",
      "Trained batch 1127 batch loss 1.5603199 epoch total loss 1.56626356\n",
      "Trained batch 1128 batch loss 1.71409357 epoch total loss 1.56639469\n",
      "Trained batch 1129 batch loss 1.6785289 epoch total loss 1.56649399\n",
      "Trained batch 1130 batch loss 1.5994271 epoch total loss 1.56652319\n",
      "Trained batch 1131 batch loss 1.41022742 epoch total loss 1.56638515\n",
      "Trained batch 1132 batch loss 1.4093051 epoch total loss 1.56624627\n",
      "Trained batch 1133 batch loss 1.4352926 epoch total loss 1.56613076\n",
      "Trained batch 1134 batch loss 1.42123163 epoch total loss 1.56600296\n",
      "Trained batch 1135 batch loss 1.46483195 epoch total loss 1.56591392\n",
      "Trained batch 1136 batch loss 1.43240404 epoch total loss 1.56579638\n",
      "Trained batch 1137 batch loss 1.43757415 epoch total loss 1.5656836\n",
      "Trained batch 1138 batch loss 1.56347 epoch total loss 1.5656817\n",
      "Trained batch 1139 batch loss 1.56231725 epoch total loss 1.56567872\n",
      "Trained batch 1140 batch loss 1.53009164 epoch total loss 1.5656476\n",
      "Trained batch 1141 batch loss 1.47253668 epoch total loss 1.56556594\n",
      "Trained batch 1142 batch loss 1.47119319 epoch total loss 1.56548333\n",
      "Trained batch 1143 batch loss 1.44379914 epoch total loss 1.56537688\n",
      "Trained batch 1144 batch loss 1.51090169 epoch total loss 1.56532931\n",
      "Trained batch 1145 batch loss 1.36704898 epoch total loss 1.5651561\n",
      "Trained batch 1146 batch loss 1.42804992 epoch total loss 1.56503654\n",
      "Trained batch 1147 batch loss 1.44340789 epoch total loss 1.56493044\n",
      "Trained batch 1148 batch loss 1.35827804 epoch total loss 1.56475043\n",
      "Trained batch 1149 batch loss 1.40224397 epoch total loss 1.56460893\n",
      "Trained batch 1150 batch loss 1.48751593 epoch total loss 1.56454194\n",
      "Trained batch 1151 batch loss 1.54215086 epoch total loss 1.5645225\n",
      "Trained batch 1152 batch loss 1.41943681 epoch total loss 1.5643965\n",
      "Trained batch 1153 batch loss 1.37943 epoch total loss 1.56423604\n",
      "Trained batch 1154 batch loss 1.35711837 epoch total loss 1.56405663\n",
      "Trained batch 1155 batch loss 1.43678594 epoch total loss 1.56394649\n",
      "Trained batch 1156 batch loss 1.47214365 epoch total loss 1.56386709\n",
      "Trained batch 1157 batch loss 1.39594007 epoch total loss 1.56372201\n",
      "Trained batch 1158 batch loss 1.45937061 epoch total loss 1.56363189\n",
      "Trained batch 1159 batch loss 1.35259104 epoch total loss 1.56344974\n",
      "Trained batch 1160 batch loss 1.43590617 epoch total loss 1.56333971\n",
      "Trained batch 1161 batch loss 1.50452268 epoch total loss 1.56328905\n",
      "Trained batch 1162 batch loss 1.50930882 epoch total loss 1.56324255\n",
      "Trained batch 1163 batch loss 1.62146211 epoch total loss 1.56329262\n",
      "Trained batch 1164 batch loss 1.60528207 epoch total loss 1.56332874\n",
      "Trained batch 1165 batch loss 1.66439772 epoch total loss 1.56341553\n",
      "Trained batch 1166 batch loss 1.3675375 epoch total loss 1.56324744\n",
      "Trained batch 1167 batch loss 1.31342 epoch total loss 1.56303346\n",
      "Trained batch 1168 batch loss 1.53601265 epoch total loss 1.56301033\n",
      "Trained batch 1169 batch loss 1.60526 epoch total loss 1.56304646\n",
      "Trained batch 1170 batch loss 1.55458617 epoch total loss 1.56303918\n",
      "Trained batch 1171 batch loss 1.52504146 epoch total loss 1.56300676\n",
      "Trained batch 1172 batch loss 1.47352207 epoch total loss 1.56293035\n",
      "Trained batch 1173 batch loss 1.36478794 epoch total loss 1.56276143\n",
      "Trained batch 1174 batch loss 1.46543622 epoch total loss 1.56267858\n",
      "Trained batch 1175 batch loss 1.41997337 epoch total loss 1.56255698\n",
      "Trained batch 1176 batch loss 1.44524014 epoch total loss 1.5624572\n",
      "Trained batch 1177 batch loss 1.41496134 epoch total loss 1.56233191\n",
      "Trained batch 1178 batch loss 1.34017277 epoch total loss 1.56214333\n",
      "Trained batch 1179 batch loss 1.44669497 epoch total loss 1.56204534\n",
      "Trained batch 1180 batch loss 1.45052731 epoch total loss 1.56195092\n",
      "Trained batch 1181 batch loss 1.52446961 epoch total loss 1.56191909\n",
      "Trained batch 1182 batch loss 1.42685938 epoch total loss 1.56180489\n",
      "Trained batch 1183 batch loss 1.34297848 epoch total loss 1.56161988\n",
      "Trained batch 1184 batch loss 1.36072254 epoch total loss 1.56145024\n",
      "Trained batch 1185 batch loss 1.48829663 epoch total loss 1.56138849\n",
      "Trained batch 1186 batch loss 1.52394366 epoch total loss 1.5613569\n",
      "Trained batch 1187 batch loss 1.47123837 epoch total loss 1.56128097\n",
      "Trained batch 1188 batch loss 1.45351613 epoch total loss 1.56119025\n",
      "Trained batch 1189 batch loss 1.37346601 epoch total loss 1.5610323\n",
      "Trained batch 1190 batch loss 1.34357 epoch total loss 1.56084955\n",
      "Trained batch 1191 batch loss 1.4570713 epoch total loss 1.56076241\n",
      "Trained batch 1192 batch loss 1.43957651 epoch total loss 1.56066072\n",
      "Trained batch 1193 batch loss 1.34670186 epoch total loss 1.56048143\n",
      "Trained batch 1194 batch loss 1.49938536 epoch total loss 1.56043017\n",
      "Trained batch 1195 batch loss 1.39954984 epoch total loss 1.56029558\n",
      "Trained batch 1196 batch loss 1.35841012 epoch total loss 1.56012678\n",
      "Trained batch 1197 batch loss 1.35065675 epoch total loss 1.55995178\n",
      "Trained batch 1198 batch loss 1.52101016 epoch total loss 1.55991936\n",
      "Trained batch 1199 batch loss 1.44582832 epoch total loss 1.55982411\n",
      "Trained batch 1200 batch loss 1.38863897 epoch total loss 1.55968153\n",
      "Trained batch 1201 batch loss 1.51474571 epoch total loss 1.5596441\n",
      "Trained batch 1202 batch loss 1.41205335 epoch total loss 1.55952132\n",
      "Trained batch 1203 batch loss 1.3521651 epoch total loss 1.55934906\n",
      "Trained batch 1204 batch loss 1.42621946 epoch total loss 1.55923843\n",
      "Trained batch 1205 batch loss 1.39592767 epoch total loss 1.55910289\n",
      "Trained batch 1206 batch loss 1.40974355 epoch total loss 1.55897903\n",
      "Trained batch 1207 batch loss 1.40196896 epoch total loss 1.55884898\n",
      "Trained batch 1208 batch loss 1.52391243 epoch total loss 1.55882013\n",
      "Trained batch 1209 batch loss 1.47349918 epoch total loss 1.55874956\n",
      "Trained batch 1210 batch loss 1.47096956 epoch total loss 1.55867696\n",
      "Trained batch 1211 batch loss 1.47949529 epoch total loss 1.55861163\n",
      "Trained batch 1212 batch loss 1.43688691 epoch total loss 1.55851114\n",
      "Trained batch 1213 batch loss 1.5050621 epoch total loss 1.55846703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1214 batch loss 1.36151373 epoch total loss 1.55830491\n",
      "Trained batch 1215 batch loss 1.47003031 epoch total loss 1.55823219\n",
      "Trained batch 1216 batch loss 1.49066937 epoch total loss 1.55817664\n",
      "Trained batch 1217 batch loss 1.49259794 epoch total loss 1.55812275\n",
      "Trained batch 1218 batch loss 1.42729402 epoch total loss 1.55801523\n",
      "Trained batch 1219 batch loss 1.42894948 epoch total loss 1.55790937\n",
      "Trained batch 1220 batch loss 1.42349219 epoch total loss 1.55779922\n",
      "Trained batch 1221 batch loss 1.47961617 epoch total loss 1.5577352\n",
      "Trained batch 1222 batch loss 1.46752369 epoch total loss 1.55766129\n",
      "Trained batch 1223 batch loss 1.46925199 epoch total loss 1.55758905\n",
      "Trained batch 1224 batch loss 1.41266382 epoch total loss 1.55747068\n",
      "Trained batch 1225 batch loss 1.55173767 epoch total loss 1.55746603\n",
      "Trained batch 1226 batch loss 1.48983335 epoch total loss 1.55741084\n",
      "Trained batch 1227 batch loss 1.46577203 epoch total loss 1.55733621\n",
      "Trained batch 1228 batch loss 1.43997931 epoch total loss 1.55724061\n",
      "Trained batch 1229 batch loss 1.45653594 epoch total loss 1.55715871\n",
      "Trained batch 1230 batch loss 1.52398884 epoch total loss 1.55713177\n",
      "Trained batch 1231 batch loss 1.53206706 epoch total loss 1.5571115\n",
      "Trained batch 1232 batch loss 1.50151193 epoch total loss 1.55706632\n",
      "Trained batch 1233 batch loss 1.40194738 epoch total loss 1.55694056\n",
      "Trained batch 1234 batch loss 1.45258439 epoch total loss 1.55685604\n",
      "Trained batch 1235 batch loss 1.50565517 epoch total loss 1.55681443\n",
      "Trained batch 1236 batch loss 1.45217824 epoch total loss 1.55672979\n",
      "Trained batch 1237 batch loss 1.51873529 epoch total loss 1.55669904\n",
      "Trained batch 1238 batch loss 1.30806768 epoch total loss 1.55649829\n",
      "Trained batch 1239 batch loss 1.41306448 epoch total loss 1.55638254\n",
      "Trained batch 1240 batch loss 1.38111496 epoch total loss 1.55624115\n",
      "Trained batch 1241 batch loss 1.2778275 epoch total loss 1.5560168\n",
      "Trained batch 1242 batch loss 1.53793216 epoch total loss 1.55600226\n",
      "Trained batch 1243 batch loss 1.60552657 epoch total loss 1.55604208\n",
      "Trained batch 1244 batch loss 1.62433541 epoch total loss 1.55609703\n",
      "Trained batch 1245 batch loss 1.40953875 epoch total loss 1.55597925\n",
      "Trained batch 1246 batch loss 1.49201047 epoch total loss 1.55592799\n",
      "Trained batch 1247 batch loss 1.42055619 epoch total loss 1.55581939\n",
      "Trained batch 1248 batch loss 1.47427261 epoch total loss 1.55575407\n",
      "Trained batch 1249 batch loss 1.37877643 epoch total loss 1.55561233\n",
      "Trained batch 1250 batch loss 1.46499121 epoch total loss 1.55553985\n",
      "Trained batch 1251 batch loss 1.41480589 epoch total loss 1.55542731\n",
      "Trained batch 1252 batch loss 1.4899894 epoch total loss 1.5553751\n",
      "Trained batch 1253 batch loss 1.48005831 epoch total loss 1.55531502\n",
      "Trained batch 1254 batch loss 1.48057652 epoch total loss 1.55525541\n",
      "Trained batch 1255 batch loss 1.43994403 epoch total loss 1.5551635\n",
      "Trained batch 1256 batch loss 1.45468879 epoch total loss 1.55508351\n",
      "Trained batch 1257 batch loss 1.38233459 epoch total loss 1.55494606\n",
      "Trained batch 1258 batch loss 1.50123119 epoch total loss 1.55490339\n",
      "Trained batch 1259 batch loss 1.40633655 epoch total loss 1.55478537\n",
      "Trained batch 1260 batch loss 1.45478344 epoch total loss 1.5547061\n",
      "Trained batch 1261 batch loss 1.42602623 epoch total loss 1.55460405\n",
      "Trained batch 1262 batch loss 1.5222621 epoch total loss 1.55457842\n",
      "Trained batch 1263 batch loss 1.46565473 epoch total loss 1.55450797\n",
      "Trained batch 1264 batch loss 1.54312825 epoch total loss 1.55449903\n",
      "Trained batch 1265 batch loss 1.48745918 epoch total loss 1.55444598\n",
      "Trained batch 1266 batch loss 1.53816116 epoch total loss 1.55443311\n",
      "Trained batch 1267 batch loss 1.49158716 epoch total loss 1.55438352\n",
      "Trained batch 1268 batch loss 1.47861278 epoch total loss 1.55432379\n",
      "Trained batch 1269 batch loss 1.483989 epoch total loss 1.55426836\n",
      "Trained batch 1270 batch loss 1.45054638 epoch total loss 1.5541867\n",
      "Trained batch 1271 batch loss 1.4927541 epoch total loss 1.55413842\n",
      "Trained batch 1272 batch loss 1.48139381 epoch total loss 1.55408132\n",
      "Trained batch 1273 batch loss 1.46281552 epoch total loss 1.55400956\n",
      "Trained batch 1274 batch loss 1.47135913 epoch total loss 1.55394459\n",
      "Trained batch 1275 batch loss 1.3962096 epoch total loss 1.55382097\n",
      "Trained batch 1276 batch loss 1.47867072 epoch total loss 1.55376208\n",
      "Trained batch 1277 batch loss 1.47827756 epoch total loss 1.55370295\n",
      "Trained batch 1278 batch loss 1.38091207 epoch total loss 1.55356765\n",
      "Trained batch 1279 batch loss 1.28047466 epoch total loss 1.55335414\n",
      "Trained batch 1280 batch loss 1.52404654 epoch total loss 1.55333126\n",
      "Trained batch 1281 batch loss 1.38136649 epoch total loss 1.55319703\n",
      "Trained batch 1282 batch loss 1.4382391 epoch total loss 1.55310738\n",
      "Trained batch 1283 batch loss 1.4594121 epoch total loss 1.55303442\n",
      "Trained batch 1284 batch loss 1.41965556 epoch total loss 1.55293047\n",
      "Trained batch 1285 batch loss 1.31832457 epoch total loss 1.55274796\n",
      "Trained batch 1286 batch loss 1.4699471 epoch total loss 1.55268359\n",
      "Trained batch 1287 batch loss 1.41758323 epoch total loss 1.55257869\n",
      "Trained batch 1288 batch loss 1.44732666 epoch total loss 1.55249691\n",
      "Trained batch 1289 batch loss 1.32344711 epoch total loss 1.55231917\n",
      "Trained batch 1290 batch loss 1.41784286 epoch total loss 1.55221498\n",
      "Trained batch 1291 batch loss 1.28526521 epoch total loss 1.55200815\n",
      "Trained batch 1292 batch loss 1.39268661 epoch total loss 1.55188489\n",
      "Trained batch 1293 batch loss 1.40112019 epoch total loss 1.5517683\n",
      "Trained batch 1294 batch loss 1.35222411 epoch total loss 1.55161405\n",
      "Trained batch 1295 batch loss 1.44720244 epoch total loss 1.55153334\n",
      "Trained batch 1296 batch loss 1.42249191 epoch total loss 1.5514338\n",
      "Trained batch 1297 batch loss 1.49037707 epoch total loss 1.55138671\n",
      "Trained batch 1298 batch loss 1.56000662 epoch total loss 1.55139339\n",
      "Trained batch 1299 batch loss 1.65088606 epoch total loss 1.55146992\n",
      "Trained batch 1300 batch loss 1.45701241 epoch total loss 1.55139732\n",
      "Trained batch 1301 batch loss 1.48264122 epoch total loss 1.55134451\n",
      "Trained batch 1302 batch loss 1.48406863 epoch total loss 1.55129278\n",
      "Trained batch 1303 batch loss 1.48128402 epoch total loss 1.55123913\n",
      "Trained batch 1304 batch loss 1.32690632 epoch total loss 1.55106699\n",
      "Trained batch 1305 batch loss 1.19012392 epoch total loss 1.55079043\n",
      "Trained batch 1306 batch loss 1.44178975 epoch total loss 1.55070698\n",
      "Trained batch 1307 batch loss 1.53419042 epoch total loss 1.55069435\n",
      "Trained batch 1308 batch loss 1.46275043 epoch total loss 1.55062711\n",
      "Trained batch 1309 batch loss 1.40598559 epoch total loss 1.55051661\n",
      "Trained batch 1310 batch loss 1.47673118 epoch total loss 1.55046022\n",
      "Trained batch 1311 batch loss 1.45987153 epoch total loss 1.55039108\n",
      "Trained batch 1312 batch loss 1.44096875 epoch total loss 1.55030763\n",
      "Trained batch 1313 batch loss 1.40709281 epoch total loss 1.55019855\n",
      "Trained batch 1314 batch loss 1.44061542 epoch total loss 1.55011523\n",
      "Trained batch 1315 batch loss 1.34214401 epoch total loss 1.54995716\n",
      "Trained batch 1316 batch loss 1.40049148 epoch total loss 1.54984355\n",
      "Trained batch 1317 batch loss 1.43071187 epoch total loss 1.54975307\n",
      "Trained batch 1318 batch loss 1.38540649 epoch total loss 1.54962838\n",
      "Trained batch 1319 batch loss 1.46494222 epoch total loss 1.54956412\n",
      "Trained batch 1320 batch loss 1.42986238 epoch total loss 1.5494734\n",
      "Trained batch 1321 batch loss 1.47920513 epoch total loss 1.54942024\n",
      "Trained batch 1322 batch loss 1.47078609 epoch total loss 1.54936075\n",
      "Trained batch 1323 batch loss 1.37448168 epoch total loss 1.54922855\n",
      "Trained batch 1324 batch loss 1.28615594 epoch total loss 1.54902983\n",
      "Trained batch 1325 batch loss 1.36739767 epoch total loss 1.54889286\n",
      "Trained batch 1326 batch loss 1.47906864 epoch total loss 1.54884\n",
      "Trained batch 1327 batch loss 1.4555614 epoch total loss 1.54876983\n",
      "Trained batch 1328 batch loss 1.39186847 epoch total loss 1.5486517\n",
      "Trained batch 1329 batch loss 1.44577718 epoch total loss 1.54857421\n",
      "Trained batch 1330 batch loss 1.35944438 epoch total loss 1.54843199\n",
      "Trained batch 1331 batch loss 1.36841071 epoch total loss 1.54829669\n",
      "Trained batch 1332 batch loss 1.41484773 epoch total loss 1.54819655\n",
      "Trained batch 1333 batch loss 1.47446895 epoch total loss 1.54814112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1334 batch loss 1.36719942 epoch total loss 1.54800546\n",
      "Trained batch 1335 batch loss 1.33006597 epoch total loss 1.54784226\n",
      "Trained batch 1336 batch loss 1.33601117 epoch total loss 1.5476836\n",
      "Trained batch 1337 batch loss 1.48117864 epoch total loss 1.54763389\n",
      "Trained batch 1338 batch loss 1.43654585 epoch total loss 1.54755092\n",
      "Trained batch 1339 batch loss 1.482342 epoch total loss 1.54750216\n",
      "Trained batch 1340 batch loss 1.41313839 epoch total loss 1.54740191\n",
      "Trained batch 1341 batch loss 1.37492847 epoch total loss 1.5472734\n",
      "Trained batch 1342 batch loss 1.35273671 epoch total loss 1.54712844\n",
      "Trained batch 1343 batch loss 1.329445 epoch total loss 1.54696631\n",
      "Trained batch 1344 batch loss 1.29972029 epoch total loss 1.54678237\n",
      "Trained batch 1345 batch loss 1.39445162 epoch total loss 1.54666913\n",
      "Trained batch 1346 batch loss 1.43676388 epoch total loss 1.54658747\n",
      "Trained batch 1347 batch loss 1.45772564 epoch total loss 1.54652154\n",
      "Trained batch 1348 batch loss 1.30164719 epoch total loss 1.54634\n",
      "Trained batch 1349 batch loss 1.35461318 epoch total loss 1.54619777\n",
      "Trained batch 1350 batch loss 1.35337 epoch total loss 1.54605484\n",
      "Trained batch 1351 batch loss 1.44465017 epoch total loss 1.54597974\n",
      "Trained batch 1352 batch loss 1.34054494 epoch total loss 1.54582787\n",
      "Trained batch 1353 batch loss 1.38260543 epoch total loss 1.54570723\n",
      "Trained batch 1354 batch loss 1.26357579 epoch total loss 1.54549885\n",
      "Trained batch 1355 batch loss 1.35535252 epoch total loss 1.54535866\n",
      "Trained batch 1356 batch loss 1.28476238 epoch total loss 1.54516637\n",
      "Trained batch 1357 batch loss 1.25407207 epoch total loss 1.54495192\n",
      "Trained batch 1358 batch loss 1.28750587 epoch total loss 1.54476237\n",
      "Trained batch 1359 batch loss 1.35234308 epoch total loss 1.54462075\n",
      "Trained batch 1360 batch loss 1.38229346 epoch total loss 1.54450142\n",
      "Trained batch 1361 batch loss 1.42431152 epoch total loss 1.54441309\n",
      "Trained batch 1362 batch loss 1.50185609 epoch total loss 1.54438198\n",
      "Trained batch 1363 batch loss 1.47880304 epoch total loss 1.54433382\n",
      "Trained batch 1364 batch loss 1.50114942 epoch total loss 1.54430223\n",
      "Trained batch 1365 batch loss 1.47541702 epoch total loss 1.54425168\n",
      "Trained batch 1366 batch loss 1.4954921 epoch total loss 1.54421604\n",
      "Trained batch 1367 batch loss 1.69731939 epoch total loss 1.54432809\n",
      "Trained batch 1368 batch loss 1.53151298 epoch total loss 1.54431868\n",
      "Trained batch 1369 batch loss 1.42335367 epoch total loss 1.54423034\n",
      "Trained batch 1370 batch loss 1.54759085 epoch total loss 1.54423273\n",
      "Trained batch 1371 batch loss 1.5125953 epoch total loss 1.54420972\n",
      "Trained batch 1372 batch loss 1.56042838 epoch total loss 1.54422164\n",
      "Trained batch 1373 batch loss 1.37626553 epoch total loss 1.54409933\n",
      "Trained batch 1374 batch loss 1.39695835 epoch total loss 1.54399228\n",
      "Trained batch 1375 batch loss 1.45897 epoch total loss 1.54393041\n",
      "Trained batch 1376 batch loss 1.33668649 epoch total loss 1.54377973\n",
      "Trained batch 1377 batch loss 1.41391873 epoch total loss 1.54368544\n",
      "Trained batch 1378 batch loss 1.50408435 epoch total loss 1.54365671\n",
      "Trained batch 1379 batch loss 1.50838315 epoch total loss 1.54363108\n",
      "Trained batch 1380 batch loss 1.50120759 epoch total loss 1.54360032\n",
      "Trained batch 1381 batch loss 1.57112336 epoch total loss 1.54362023\n",
      "Trained batch 1382 batch loss 1.48085523 epoch total loss 1.54357481\n",
      "Trained batch 1383 batch loss 1.41317964 epoch total loss 1.54348052\n",
      "Trained batch 1384 batch loss 1.35842752 epoch total loss 1.54334676\n",
      "Trained batch 1385 batch loss 1.40260231 epoch total loss 1.5432452\n",
      "Trained batch 1386 batch loss 1.41396308 epoch total loss 1.54315197\n",
      "Trained batch 1387 batch loss 1.4088515 epoch total loss 1.54305518\n",
      "Trained batch 1388 batch loss 1.38195121 epoch total loss 1.54293907\n",
      "Epoch 1 train loss 1.5429390668869019\n",
      "Validated batch 1 batch loss 1.42138505\n",
      "Validated batch 2 batch loss 1.44098794\n",
      "Validated batch 3 batch loss 1.33017695\n",
      "Validated batch 4 batch loss 1.53954983\n",
      "Validated batch 5 batch loss 1.39674091\n",
      "Validated batch 6 batch loss 1.43657255\n",
      "Validated batch 7 batch loss 1.52298665\n",
      "Validated batch 8 batch loss 1.48469615\n",
      "Validated batch 9 batch loss 1.4733727\n",
      "Validated batch 10 batch loss 1.47030568\n",
      "Validated batch 11 batch loss 1.51183319\n",
      "Validated batch 12 batch loss 1.4302454\n",
      "Validated batch 13 batch loss 1.40491092\n",
      "Validated batch 14 batch loss 1.51071358\n",
      "Validated batch 15 batch loss 1.44572663\n",
      "Validated batch 16 batch loss 1.42507076\n",
      "Validated batch 17 batch loss 1.53535926\n",
      "Validated batch 18 batch loss 1.28411865\n",
      "Validated batch 19 batch loss 1.4442817\n",
      "Validated batch 20 batch loss 1.29241312\n",
      "Validated batch 21 batch loss 1.45142233\n",
      "Validated batch 22 batch loss 1.5201695\n",
      "Validated batch 23 batch loss 1.35289192\n",
      "Validated batch 24 batch loss 1.49191356\n",
      "Validated batch 25 batch loss 1.47854376\n",
      "Validated batch 26 batch loss 1.35659289\n",
      "Validated batch 27 batch loss 1.32266831\n",
      "Validated batch 28 batch loss 1.39946914\n",
      "Validated batch 29 batch loss 1.427109\n",
      "Validated batch 30 batch loss 1.34923458\n",
      "Validated batch 31 batch loss 1.38081241\n",
      "Validated batch 32 batch loss 1.43805146\n",
      "Validated batch 33 batch loss 1.44490647\n",
      "Validated batch 34 batch loss 1.35051572\n",
      "Validated batch 35 batch loss 1.33521628\n",
      "Validated batch 36 batch loss 1.42395711\n",
      "Validated batch 37 batch loss 1.43941712\n",
      "Validated batch 38 batch loss 1.49620569\n",
      "Validated batch 39 batch loss 1.47729158\n",
      "Validated batch 40 batch loss 1.36043572\n",
      "Validated batch 41 batch loss 1.53983903\n",
      "Validated batch 42 batch loss 1.42500699\n",
      "Validated batch 43 batch loss 1.43109179\n",
      "Validated batch 44 batch loss 1.48115027\n",
      "Validated batch 45 batch loss 1.20927823\n",
      "Validated batch 46 batch loss 1.47819877\n",
      "Validated batch 47 batch loss 1.3293817\n",
      "Validated batch 48 batch loss 1.48386574\n",
      "Validated batch 49 batch loss 1.48689473\n",
      "Validated batch 50 batch loss 1.37491834\n",
      "Validated batch 51 batch loss 1.46720397\n",
      "Validated batch 52 batch loss 1.53711593\n",
      "Validated batch 53 batch loss 1.28991091\n",
      "Validated batch 54 batch loss 1.49278116\n",
      "Validated batch 55 batch loss 1.39391208\n",
      "Validated batch 56 batch loss 1.46724987\n",
      "Validated batch 57 batch loss 1.46188498\n",
      "Validated batch 58 batch loss 1.32125354\n",
      "Validated batch 59 batch loss 1.27820492\n",
      "Validated batch 60 batch loss 1.44553232\n",
      "Validated batch 61 batch loss 1.4209789\n",
      "Validated batch 62 batch loss 1.3697468\n",
      "Validated batch 63 batch loss 1.43889689\n",
      "Validated batch 64 batch loss 1.40502191\n",
      "Validated batch 65 batch loss 1.44406462\n",
      "Validated batch 66 batch loss 1.52610195\n",
      "Validated batch 67 batch loss 1.45510209\n",
      "Validated batch 68 batch loss 1.45020032\n",
      "Validated batch 69 batch loss 1.34433591\n",
      "Validated batch 70 batch loss 1.50070548\n",
      "Validated batch 71 batch loss 1.41038585\n",
      "Validated batch 72 batch loss 1.42607498\n",
      "Validated batch 73 batch loss 1.40226197\n",
      "Validated batch 74 batch loss 1.38798344\n",
      "Validated batch 75 batch loss 1.44715738\n",
      "Validated batch 76 batch loss 1.4139235\n",
      "Validated batch 77 batch loss 1.33635926\n",
      "Validated batch 78 batch loss 1.3766346\n",
      "Validated batch 79 batch loss 1.40876782\n",
      "Validated batch 80 batch loss 1.45375538\n",
      "Validated batch 81 batch loss 1.43838906\n",
      "Validated batch 82 batch loss 1.39966047\n",
      "Validated batch 83 batch loss 1.34093237\n",
      "Validated batch 84 batch loss 1.390028\n",
      "Validated batch 85 batch loss 1.45378876\n",
      "Validated batch 86 batch loss 1.42046094\n",
      "Validated batch 87 batch loss 1.42470801\n",
      "Validated batch 88 batch loss 1.52648282\n",
      "Validated batch 89 batch loss 1.65706575\n",
      "Validated batch 90 batch loss 1.53646886\n",
      "Validated batch 91 batch loss 1.42298961\n",
      "Validated batch 92 batch loss 1.32948637\n",
      "Validated batch 93 batch loss 1.31509018\n",
      "Validated batch 94 batch loss 1.43593943\n",
      "Validated batch 95 batch loss 1.39728093\n",
      "Validated batch 96 batch loss 1.25325394\n",
      "Validated batch 97 batch loss 1.32294714\n",
      "Validated batch 98 batch loss 1.52729511\n",
      "Validated batch 99 batch loss 1.35858321\n",
      "Validated batch 100 batch loss 1.34205019\n",
      "Validated batch 101 batch loss 1.39305818\n",
      "Validated batch 102 batch loss 1.41568625\n",
      "Validated batch 103 batch loss 1.40178955\n",
      "Validated batch 104 batch loss 1.40418136\n",
      "Validated batch 105 batch loss 1.45194364\n",
      "Validated batch 106 batch loss 1.37676632\n",
      "Validated batch 107 batch loss 1.42815161\n",
      "Validated batch 108 batch loss 1.57061207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 109 batch loss 1.36023879\n",
      "Validated batch 110 batch loss 1.51587844\n",
      "Validated batch 111 batch loss 1.34535885\n",
      "Validated batch 112 batch loss 1.36459064\n",
      "Validated batch 113 batch loss 1.39508748\n",
      "Validated batch 114 batch loss 1.39475834\n",
      "Validated batch 115 batch loss 1.52507389\n",
      "Validated batch 116 batch loss 1.49853396\n",
      "Validated batch 117 batch loss 1.42243767\n",
      "Validated batch 118 batch loss 1.38974535\n",
      "Validated batch 119 batch loss 1.36719191\n",
      "Validated batch 120 batch loss 1.38490713\n",
      "Validated batch 121 batch loss 1.49579239\n",
      "Validated batch 122 batch loss 1.35586631\n",
      "Validated batch 123 batch loss 1.44762337\n",
      "Validated batch 124 batch loss 1.42034328\n",
      "Validated batch 125 batch loss 1.43607497\n",
      "Validated batch 126 batch loss 1.41924667\n",
      "Validated batch 127 batch loss 1.31886041\n",
      "Validated batch 128 batch loss 1.43666744\n",
      "Validated batch 129 batch loss 1.52038109\n",
      "Validated batch 130 batch loss 1.44359875\n",
      "Validated batch 131 batch loss 1.38862538\n",
      "Validated batch 132 batch loss 1.44327223\n",
      "Validated batch 133 batch loss 1.33492076\n",
      "Validated batch 134 batch loss 1.33351398\n",
      "Validated batch 135 batch loss 1.41030598\n",
      "Validated batch 136 batch loss 1.35229826\n",
      "Validated batch 137 batch loss 1.41002023\n",
      "Validated batch 138 batch loss 1.42648327\n",
      "Validated batch 139 batch loss 1.49390113\n",
      "Validated batch 140 batch loss 1.32375777\n",
      "Validated batch 141 batch loss 1.41218042\n",
      "Validated batch 142 batch loss 1.37728727\n",
      "Validated batch 143 batch loss 1.38236618\n",
      "Validated batch 144 batch loss 1.41356587\n",
      "Validated batch 145 batch loss 1.39936113\n",
      "Validated batch 146 batch loss 1.42912078\n",
      "Validated batch 147 batch loss 1.4750216\n",
      "Validated batch 148 batch loss 1.35511875\n",
      "Validated batch 149 batch loss 1.55041027\n",
      "Validated batch 150 batch loss 1.49045277\n",
      "Validated batch 151 batch loss 1.33048546\n",
      "Validated batch 152 batch loss 1.41908944\n",
      "Validated batch 153 batch loss 1.43829131\n",
      "Validated batch 154 batch loss 1.3913343\n",
      "Validated batch 155 batch loss 1.52874434\n",
      "Validated batch 156 batch loss 1.3979075\n",
      "Validated batch 157 batch loss 1.47399938\n",
      "Validated batch 158 batch loss 1.38541687\n",
      "Validated batch 159 batch loss 1.39839566\n",
      "Validated batch 160 batch loss 1.38913262\n",
      "Validated batch 161 batch loss 1.38845873\n",
      "Validated batch 162 batch loss 1.42444658\n",
      "Validated batch 163 batch loss 1.33847904\n",
      "Validated batch 164 batch loss 1.3971833\n",
      "Validated batch 165 batch loss 1.391945\n",
      "Validated batch 166 batch loss 1.37008739\n",
      "Validated batch 167 batch loss 1.48236036\n",
      "Validated batch 168 batch loss 1.44421268\n",
      "Validated batch 169 batch loss 1.44251919\n",
      "Validated batch 170 batch loss 1.53719568\n",
      "Validated batch 171 batch loss 1.49298716\n",
      "Validated batch 172 batch loss 1.45339131\n",
      "Validated batch 173 batch loss 1.46899211\n",
      "Validated batch 174 batch loss 1.44914007\n",
      "Validated batch 175 batch loss 1.48047328\n",
      "Validated batch 176 batch loss 1.48946\n",
      "Validated batch 177 batch loss 1.53323829\n",
      "Validated batch 178 batch loss 1.44565272\n",
      "Validated batch 179 batch loss 1.33715904\n",
      "Validated batch 180 batch loss 1.34677863\n",
      "Validated batch 181 batch loss 1.41524696\n",
      "Validated batch 182 batch loss 1.5001471\n",
      "Validated batch 183 batch loss 1.41335654\n",
      "Validated batch 184 batch loss 1.43490815\n",
      "Validated batch 185 batch loss 1.37962866\n",
      "Epoch 1 val loss 1.422253966331482\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-1-loss-1.4223.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.52756166 epoch total loss 1.52756166\n",
      "Trained batch 2 batch loss 1.39697683 epoch total loss 1.46226931\n",
      "Trained batch 3 batch loss 1.2240684 epoch total loss 1.38286912\n",
      "Trained batch 4 batch loss 1.22626936 epoch total loss 1.34371912\n",
      "Trained batch 5 batch loss 1.43842173 epoch total loss 1.36265969\n",
      "Trained batch 6 batch loss 1.49387467 epoch total loss 1.38452876\n",
      "Trained batch 7 batch loss 1.4092176 epoch total loss 1.3880558\n",
      "Trained batch 8 batch loss 1.37220728 epoch total loss 1.38607478\n",
      "Trained batch 9 batch loss 1.41258192 epoch total loss 1.38902009\n",
      "Trained batch 10 batch loss 1.46016991 epoch total loss 1.39613509\n",
      "Trained batch 11 batch loss 1.38596416 epoch total loss 1.39521039\n",
      "Trained batch 12 batch loss 1.34694064 epoch total loss 1.39118803\n",
      "Trained batch 13 batch loss 1.34907675 epoch total loss 1.38794875\n",
      "Trained batch 14 batch loss 1.35418594 epoch total loss 1.38553703\n",
      "Trained batch 15 batch loss 1.38933706 epoch total loss 1.38579035\n",
      "Trained batch 16 batch loss 1.4107703 epoch total loss 1.38735163\n",
      "Trained batch 17 batch loss 1.4220612 epoch total loss 1.38939345\n",
      "Trained batch 18 batch loss 1.39423168 epoch total loss 1.38966227\n",
      "Trained batch 19 batch loss 1.41596699 epoch total loss 1.39104664\n",
      "Trained batch 20 batch loss 1.40081954 epoch total loss 1.39153528\n",
      "Trained batch 21 batch loss 1.39819944 epoch total loss 1.39185262\n",
      "Trained batch 22 batch loss 1.37126982 epoch total loss 1.39091694\n",
      "Trained batch 23 batch loss 1.44929218 epoch total loss 1.39345503\n",
      "Trained batch 24 batch loss 1.55248237 epoch total loss 1.40008116\n",
      "Trained batch 25 batch loss 1.49671316 epoch total loss 1.4039464\n",
      "Trained batch 26 batch loss 1.39701486 epoch total loss 1.40367973\n",
      "Trained batch 27 batch loss 1.43189251 epoch total loss 1.40472472\n",
      "Trained batch 28 batch loss 1.4481461 epoch total loss 1.40627551\n",
      "Trained batch 29 batch loss 1.40717 epoch total loss 1.40630627\n",
      "Trained batch 30 batch loss 1.46041739 epoch total loss 1.40811\n",
      "Trained batch 31 batch loss 1.51398826 epoch total loss 1.41152549\n",
      "Trained batch 32 batch loss 1.34363604 epoch total loss 1.40940392\n",
      "Trained batch 33 batch loss 1.40825284 epoch total loss 1.40936899\n",
      "Trained batch 34 batch loss 1.44544768 epoch total loss 1.41043007\n",
      "Trained batch 35 batch loss 1.45755029 epoch total loss 1.41177642\n",
      "Trained batch 36 batch loss 1.45768261 epoch total loss 1.41305161\n",
      "Trained batch 37 batch loss 1.43424451 epoch total loss 1.41362441\n",
      "Trained batch 38 batch loss 1.39411533 epoch total loss 1.41311109\n",
      "Trained batch 39 batch loss 1.41640472 epoch total loss 1.41319549\n",
      "Trained batch 40 batch loss 1.46468985 epoch total loss 1.41448283\n",
      "Trained batch 41 batch loss 1.54498434 epoch total loss 1.41766584\n",
      "Trained batch 42 batch loss 1.43892944 epoch total loss 1.41817212\n",
      "Trained batch 43 batch loss 1.46826935 epoch total loss 1.41933715\n",
      "Trained batch 44 batch loss 1.27854908 epoch total loss 1.41613746\n",
      "Trained batch 45 batch loss 1.37957489 epoch total loss 1.41532493\n",
      "Trained batch 46 batch loss 1.40389228 epoch total loss 1.41507638\n",
      "Trained batch 47 batch loss 1.40044522 epoch total loss 1.414765\n",
      "Trained batch 48 batch loss 1.41135216 epoch total loss 1.41469395\n",
      "Trained batch 49 batch loss 1.38888383 epoch total loss 1.41416728\n",
      "Trained batch 50 batch loss 1.49259245 epoch total loss 1.41573572\n",
      "Trained batch 51 batch loss 1.51442528 epoch total loss 1.41767085\n",
      "Trained batch 52 batch loss 1.47497439 epoch total loss 1.41877294\n",
      "Trained batch 53 batch loss 1.44407892 epoch total loss 1.41925037\n",
      "Trained batch 54 batch loss 1.45700288 epoch total loss 1.41994941\n",
      "Trained batch 55 batch loss 1.31565642 epoch total loss 1.41805327\n",
      "Trained batch 56 batch loss 1.28948236 epoch total loss 1.4157573\n",
      "Trained batch 57 batch loss 1.36285889 epoch total loss 1.41482937\n",
      "Trained batch 58 batch loss 1.28834558 epoch total loss 1.41264856\n",
      "Trained batch 59 batch loss 1.47246861 epoch total loss 1.41366243\n",
      "Trained batch 60 batch loss 1.41002727 epoch total loss 1.41360176\n",
      "Trained batch 61 batch loss 1.40946651 epoch total loss 1.41353405\n",
      "Trained batch 62 batch loss 1.34192348 epoch total loss 1.41237915\n",
      "Trained batch 63 batch loss 1.4682976 epoch total loss 1.41326678\n",
      "Trained batch 64 batch loss 1.2568872 epoch total loss 1.41082335\n",
      "Trained batch 65 batch loss 1.2937063 epoch total loss 1.40902162\n",
      "Trained batch 66 batch loss 1.37261808 epoch total loss 1.40847\n",
      "Trained batch 67 batch loss 1.42105436 epoch total loss 1.40865779\n",
      "Trained batch 68 batch loss 1.44167876 epoch total loss 1.40914345\n",
      "Trained batch 69 batch loss 1.40098417 epoch total loss 1.40902519\n",
      "Trained batch 70 batch loss 1.37250209 epoch total loss 1.40850353\n",
      "Trained batch 71 batch loss 1.42913103 epoch total loss 1.40879405\n",
      "Trained batch 72 batch loss 1.38358319 epoch total loss 1.40844393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 73 batch loss 1.28644443 epoch total loss 1.40677261\n",
      "Trained batch 74 batch loss 1.30288196 epoch total loss 1.40536869\n",
      "Trained batch 75 batch loss 1.1070019 epoch total loss 1.40139043\n",
      "Trained batch 76 batch loss 1.20976043 epoch total loss 1.39886904\n",
      "Trained batch 77 batch loss 1.53081894 epoch total loss 1.40058267\n",
      "Trained batch 78 batch loss 1.54038644 epoch total loss 1.4023751\n",
      "Trained batch 79 batch loss 1.50690758 epoch total loss 1.40369821\n",
      "Trained batch 80 batch loss 1.47523618 epoch total loss 1.40459239\n",
      "Trained batch 81 batch loss 1.47920728 epoch total loss 1.40551364\n",
      "Trained batch 82 batch loss 1.28791869 epoch total loss 1.40407956\n",
      "Trained batch 83 batch loss 1.22824693 epoch total loss 1.40196109\n",
      "Trained batch 84 batch loss 1.33814538 epoch total loss 1.40120137\n",
      "Trained batch 85 batch loss 1.46052742 epoch total loss 1.40189922\n",
      "Trained batch 86 batch loss 1.34385955 epoch total loss 1.40122437\n",
      "Trained batch 87 batch loss 1.39942205 epoch total loss 1.40120363\n",
      "Trained batch 88 batch loss 1.45100605 epoch total loss 1.40176952\n",
      "Trained batch 89 batch loss 1.4110347 epoch total loss 1.40187359\n",
      "Trained batch 90 batch loss 1.40378773 epoch total loss 1.40189493\n",
      "Trained batch 91 batch loss 1.4495455 epoch total loss 1.40241849\n",
      "Trained batch 92 batch loss 1.3990128 epoch total loss 1.40238154\n",
      "Trained batch 93 batch loss 1.3893224 epoch total loss 1.40224123\n",
      "Trained batch 94 batch loss 1.42668056 epoch total loss 1.40250123\n",
      "Trained batch 95 batch loss 1.45676136 epoch total loss 1.40307236\n",
      "Trained batch 96 batch loss 1.40540433 epoch total loss 1.40309668\n",
      "Trained batch 97 batch loss 1.43767822 epoch total loss 1.40345323\n",
      "Trained batch 98 batch loss 1.46532583 epoch total loss 1.40408468\n",
      "Trained batch 99 batch loss 1.49805 epoch total loss 1.40503371\n",
      "Trained batch 100 batch loss 1.54882491 epoch total loss 1.40647173\n",
      "Trained batch 101 batch loss 1.55287457 epoch total loss 1.4079212\n",
      "Trained batch 102 batch loss 1.36248815 epoch total loss 1.40747583\n",
      "Trained batch 103 batch loss 1.37841022 epoch total loss 1.40719354\n",
      "Trained batch 104 batch loss 1.37457085 epoch total loss 1.4068799\n",
      "Trained batch 105 batch loss 1.33272278 epoch total loss 1.40617359\n",
      "Trained batch 106 batch loss 1.35132599 epoch total loss 1.4056561\n",
      "Trained batch 107 batch loss 1.31877255 epoch total loss 1.40484405\n",
      "Trained batch 108 batch loss 1.49822617 epoch total loss 1.40570879\n",
      "Trained batch 109 batch loss 1.39227664 epoch total loss 1.40558553\n",
      "Trained batch 110 batch loss 1.44790637 epoch total loss 1.40597022\n",
      "Trained batch 111 batch loss 1.40461397 epoch total loss 1.40595806\n",
      "Trained batch 112 batch loss 1.39188993 epoch total loss 1.40583241\n",
      "Trained batch 113 batch loss 1.50822234 epoch total loss 1.40673852\n",
      "Trained batch 114 batch loss 1.39041185 epoch total loss 1.40659535\n",
      "Trained batch 115 batch loss 1.43727291 epoch total loss 1.40686202\n",
      "Trained batch 116 batch loss 1.40322959 epoch total loss 1.40683079\n",
      "Trained batch 117 batch loss 1.37385488 epoch total loss 1.40654886\n",
      "Trained batch 118 batch loss 1.35960519 epoch total loss 1.40615106\n",
      "Trained batch 119 batch loss 1.3339951 epoch total loss 1.40554476\n",
      "Trained batch 120 batch loss 1.37675428 epoch total loss 1.40530479\n",
      "Trained batch 121 batch loss 1.50009513 epoch total loss 1.40608823\n",
      "Trained batch 122 batch loss 1.57795739 epoch total loss 1.40749693\n",
      "Trained batch 123 batch loss 1.59063876 epoch total loss 1.40898597\n",
      "Trained batch 124 batch loss 1.63727951 epoch total loss 1.41082704\n",
      "Trained batch 125 batch loss 1.53679347 epoch total loss 1.41183472\n",
      "Trained batch 126 batch loss 1.36421967 epoch total loss 1.41145682\n",
      "Trained batch 127 batch loss 1.30001116 epoch total loss 1.41057944\n",
      "Trained batch 128 batch loss 1.19950175 epoch total loss 1.40893042\n",
      "Trained batch 129 batch loss 1.24201488 epoch total loss 1.40763652\n",
      "Trained batch 130 batch loss 1.22511733 epoch total loss 1.40623248\n",
      "Trained batch 131 batch loss 1.19767904 epoch total loss 1.40464044\n",
      "Trained batch 132 batch loss 1.50849104 epoch total loss 1.40542722\n",
      "Trained batch 133 batch loss 1.31268024 epoch total loss 1.40472984\n",
      "Trained batch 134 batch loss 1.54685402 epoch total loss 1.40579057\n",
      "Trained batch 135 batch loss 1.45355237 epoch total loss 1.40614426\n",
      "Trained batch 136 batch loss 1.50228941 epoch total loss 1.40685129\n",
      "Trained batch 137 batch loss 1.43579984 epoch total loss 1.40706265\n",
      "Trained batch 138 batch loss 1.26339293 epoch total loss 1.4060216\n",
      "Trained batch 139 batch loss 1.33543503 epoch total loss 1.40551376\n",
      "Trained batch 140 batch loss 1.40098453 epoch total loss 1.40548134\n",
      "Trained batch 141 batch loss 1.47635627 epoch total loss 1.40598404\n",
      "Trained batch 142 batch loss 1.41531694 epoch total loss 1.40604973\n",
      "Trained batch 143 batch loss 1.4337889 epoch total loss 1.40624368\n",
      "Trained batch 144 batch loss 1.54155207 epoch total loss 1.40718329\n",
      "Trained batch 145 batch loss 1.45949531 epoch total loss 1.40754402\n",
      "Trained batch 146 batch loss 1.50003076 epoch total loss 1.4081775\n",
      "Trained batch 147 batch loss 1.4902854 epoch total loss 1.40873599\n",
      "Trained batch 148 batch loss 1.53178239 epoch total loss 1.40956748\n",
      "Trained batch 149 batch loss 1.43087339 epoch total loss 1.40971053\n",
      "Trained batch 150 batch loss 1.55034781 epoch total loss 1.41064811\n",
      "Trained batch 151 batch loss 1.49901199 epoch total loss 1.41123331\n",
      "Trained batch 152 batch loss 1.46332669 epoch total loss 1.41157603\n",
      "Trained batch 153 batch loss 1.37554121 epoch total loss 1.41134048\n",
      "Trained batch 154 batch loss 1.41585314 epoch total loss 1.41136968\n",
      "Trained batch 155 batch loss 1.3117466 epoch total loss 1.41072702\n",
      "Trained batch 156 batch loss 1.47474074 epoch total loss 1.41113746\n",
      "Trained batch 157 batch loss 1.4512372 epoch total loss 1.41139281\n",
      "Trained batch 158 batch loss 1.53369045 epoch total loss 1.41216683\n",
      "Trained batch 159 batch loss 1.36879194 epoch total loss 1.41189408\n",
      "Trained batch 160 batch loss 1.31206822 epoch total loss 1.41127014\n",
      "Trained batch 161 batch loss 1.2999748 epoch total loss 1.41057885\n",
      "Trained batch 162 batch loss 1.32222581 epoch total loss 1.41003346\n",
      "Trained batch 163 batch loss 1.32446361 epoch total loss 1.40950847\n",
      "Trained batch 164 batch loss 1.47017407 epoch total loss 1.40987837\n",
      "Trained batch 165 batch loss 1.49377012 epoch total loss 1.4103868\n",
      "Trained batch 166 batch loss 1.46235466 epoch total loss 1.41069984\n",
      "Trained batch 167 batch loss 1.36172223 epoch total loss 1.41040659\n",
      "Trained batch 168 batch loss 1.37351894 epoch total loss 1.41018701\n",
      "Trained batch 169 batch loss 1.39384258 epoch total loss 1.41009033\n",
      "Trained batch 170 batch loss 1.38604093 epoch total loss 1.40994895\n",
      "Trained batch 171 batch loss 1.42645586 epoch total loss 1.41004539\n",
      "Trained batch 172 batch loss 1.39850533 epoch total loss 1.40997827\n",
      "Trained batch 173 batch loss 1.48831403 epoch total loss 1.41043103\n",
      "Trained batch 174 batch loss 1.55501676 epoch total loss 1.41126204\n",
      "Trained batch 175 batch loss 1.37709737 epoch total loss 1.41106677\n",
      "Trained batch 176 batch loss 1.50892937 epoch total loss 1.41162288\n",
      "Trained batch 177 batch loss 1.30137825 epoch total loss 1.411\n",
      "Trained batch 178 batch loss 1.32627904 epoch total loss 1.41052401\n",
      "Trained batch 179 batch loss 1.38335514 epoch total loss 1.41037226\n",
      "Trained batch 180 batch loss 1.3870734 epoch total loss 1.4102428\n",
      "Trained batch 181 batch loss 1.35412955 epoch total loss 1.40993273\n",
      "Trained batch 182 batch loss 1.20758939 epoch total loss 1.40882099\n",
      "Trained batch 183 batch loss 1.28488636 epoch total loss 1.40814376\n",
      "Trained batch 184 batch loss 1.23881304 epoch total loss 1.40722346\n",
      "Trained batch 185 batch loss 1.28052402 epoch total loss 1.40653849\n",
      "Trained batch 186 batch loss 1.15703285 epoch total loss 1.40519714\n",
      "Trained batch 187 batch loss 1.34984756 epoch total loss 1.40490115\n",
      "Trained batch 188 batch loss 1.37501633 epoch total loss 1.40474236\n",
      "Trained batch 189 batch loss 1.39945602 epoch total loss 1.40471423\n",
      "Trained batch 190 batch loss 1.34389973 epoch total loss 1.40439427\n",
      "Trained batch 191 batch loss 1.31130385 epoch total loss 1.40390682\n",
      "Trained batch 192 batch loss 1.38745499 epoch total loss 1.40382111\n",
      "Trained batch 193 batch loss 1.41538191 epoch total loss 1.40388107\n",
      "Trained batch 194 batch loss 1.4570576 epoch total loss 1.40415514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 195 batch loss 1.27930641 epoch total loss 1.40351486\n",
      "Trained batch 196 batch loss 1.32770443 epoch total loss 1.40312803\n",
      "Trained batch 197 batch loss 1.18066788 epoch total loss 1.40199876\n",
      "Trained batch 198 batch loss 1.42199659 epoch total loss 1.40209973\n",
      "Trained batch 199 batch loss 1.26764786 epoch total loss 1.40142405\n",
      "Trained batch 200 batch loss 1.40502119 epoch total loss 1.40144205\n",
      "Trained batch 201 batch loss 1.36599076 epoch total loss 1.40126574\n",
      "Trained batch 202 batch loss 1.36812556 epoch total loss 1.40110171\n",
      "Trained batch 203 batch loss 1.3883338 epoch total loss 1.40103889\n",
      "Trained batch 204 batch loss 1.38856256 epoch total loss 1.40097761\n",
      "Trained batch 205 batch loss 1.40916252 epoch total loss 1.40101755\n",
      "Trained batch 206 batch loss 1.35409343 epoch total loss 1.40078974\n",
      "Trained batch 207 batch loss 1.26208746 epoch total loss 1.40011966\n",
      "Trained batch 208 batch loss 1.27365887 epoch total loss 1.39951158\n",
      "Trained batch 209 batch loss 1.35939336 epoch total loss 1.39931977\n",
      "Trained batch 210 batch loss 1.38266969 epoch total loss 1.39924037\n",
      "Trained batch 211 batch loss 1.51376581 epoch total loss 1.39978313\n",
      "Trained batch 212 batch loss 1.61360192 epoch total loss 1.40079176\n",
      "Trained batch 213 batch loss 1.60744 epoch total loss 1.40176201\n",
      "Trained batch 214 batch loss 1.48993111 epoch total loss 1.402174\n",
      "Trained batch 215 batch loss 1.2432543 epoch total loss 1.4014349\n",
      "Trained batch 216 batch loss 1.37244153 epoch total loss 1.40130067\n",
      "Trained batch 217 batch loss 1.39818609 epoch total loss 1.40128636\n",
      "Trained batch 218 batch loss 1.40085149 epoch total loss 1.40128434\n",
      "Trained batch 219 batch loss 1.43738651 epoch total loss 1.40144908\n",
      "Trained batch 220 batch loss 1.47027755 epoch total loss 1.40176201\n",
      "Trained batch 221 batch loss 1.38405776 epoch total loss 1.4016819\n",
      "Trained batch 222 batch loss 1.34460211 epoch total loss 1.40142477\n",
      "Trained batch 223 batch loss 1.3398385 epoch total loss 1.40114868\n",
      "Trained batch 224 batch loss 1.41537559 epoch total loss 1.4012121\n",
      "Trained batch 225 batch loss 1.36640036 epoch total loss 1.40105736\n",
      "Trained batch 226 batch loss 1.35262179 epoch total loss 1.40084314\n",
      "Trained batch 227 batch loss 1.29010391 epoch total loss 1.40035522\n",
      "Trained batch 228 batch loss 1.35096788 epoch total loss 1.40013874\n",
      "Trained batch 229 batch loss 1.39790118 epoch total loss 1.40012884\n",
      "Trained batch 230 batch loss 1.35440063 epoch total loss 1.39993012\n",
      "Trained batch 231 batch loss 1.25110555 epoch total loss 1.39928579\n",
      "Trained batch 232 batch loss 1.34441698 epoch total loss 1.39904928\n",
      "Trained batch 233 batch loss 1.39424157 epoch total loss 1.39902878\n",
      "Trained batch 234 batch loss 1.52157831 epoch total loss 1.39955246\n",
      "Trained batch 235 batch loss 1.50892377 epoch total loss 1.40001774\n",
      "Trained batch 236 batch loss 1.50811839 epoch total loss 1.40047586\n",
      "Trained batch 237 batch loss 1.44112909 epoch total loss 1.4006474\n",
      "Trained batch 238 batch loss 1.42791104 epoch total loss 1.40076196\n",
      "Trained batch 239 batch loss 1.41996 epoch total loss 1.40084231\n",
      "Trained batch 240 batch loss 1.57317483 epoch total loss 1.40156031\n",
      "Trained batch 241 batch loss 1.49215698 epoch total loss 1.40193629\n",
      "Trained batch 242 batch loss 1.45124555 epoch total loss 1.40214\n",
      "Trained batch 243 batch loss 1.40331006 epoch total loss 1.40214479\n",
      "Trained batch 244 batch loss 1.38788462 epoch total loss 1.40208638\n",
      "Trained batch 245 batch loss 1.39224362 epoch total loss 1.4020462\n",
      "Trained batch 246 batch loss 1.38548756 epoch total loss 1.40197885\n",
      "Trained batch 247 batch loss 1.34988785 epoch total loss 1.40176797\n",
      "Trained batch 248 batch loss 1.47620261 epoch total loss 1.40206814\n",
      "Trained batch 249 batch loss 1.40785146 epoch total loss 1.40209126\n",
      "Trained batch 250 batch loss 1.44080758 epoch total loss 1.40224612\n",
      "Trained batch 251 batch loss 1.42022622 epoch total loss 1.40231776\n",
      "Trained batch 252 batch loss 1.35085881 epoch total loss 1.40211356\n",
      "Trained batch 253 batch loss 1.35600746 epoch total loss 1.40193129\n",
      "Trained batch 254 batch loss 1.39074719 epoch total loss 1.4018873\n",
      "Trained batch 255 batch loss 1.37190866 epoch total loss 1.40176976\n",
      "Trained batch 256 batch loss 1.37562346 epoch total loss 1.40166759\n",
      "Trained batch 257 batch loss 1.32522166 epoch total loss 1.40137017\n",
      "Trained batch 258 batch loss 1.32713044 epoch total loss 1.4010824\n",
      "Trained batch 259 batch loss 1.3529588 epoch total loss 1.40089655\n",
      "Trained batch 260 batch loss 1.3832016 epoch total loss 1.4008286\n",
      "Trained batch 261 batch loss 1.44531906 epoch total loss 1.40099895\n",
      "Trained batch 262 batch loss 1.35256958 epoch total loss 1.40081418\n",
      "Trained batch 263 batch loss 1.3548336 epoch total loss 1.4006393\n",
      "Trained batch 264 batch loss 1.3642509 epoch total loss 1.40050149\n",
      "Trained batch 265 batch loss 1.44532251 epoch total loss 1.40067053\n",
      "Trained batch 266 batch loss 1.42421305 epoch total loss 1.4007591\n",
      "Trained batch 267 batch loss 1.76165223 epoch total loss 1.40211082\n",
      "Trained batch 268 batch loss 1.51931083 epoch total loss 1.40254819\n",
      "Trained batch 269 batch loss 1.47409952 epoch total loss 1.40281415\n",
      "Trained batch 270 batch loss 1.54685462 epoch total loss 1.40334761\n",
      "Trained batch 271 batch loss 1.51982534 epoch total loss 1.40377736\n",
      "Trained batch 272 batch loss 1.45410657 epoch total loss 1.40396237\n",
      "Trained batch 273 batch loss 1.42527771 epoch total loss 1.40404046\n",
      "Trained batch 274 batch loss 1.47992706 epoch total loss 1.40431738\n",
      "Trained batch 275 batch loss 1.45050156 epoch total loss 1.40448534\n",
      "Trained batch 276 batch loss 1.41674447 epoch total loss 1.40452969\n",
      "Trained batch 277 batch loss 1.47833395 epoch total loss 1.40479612\n",
      "Trained batch 278 batch loss 1.46271193 epoch total loss 1.4050045\n",
      "Trained batch 279 batch loss 1.38612819 epoch total loss 1.40493691\n",
      "Trained batch 280 batch loss 1.34738588 epoch total loss 1.40473127\n",
      "Trained batch 281 batch loss 1.40536094 epoch total loss 1.40473354\n",
      "Trained batch 282 batch loss 1.34694588 epoch total loss 1.40452874\n",
      "Trained batch 283 batch loss 1.37097037 epoch total loss 1.40441012\n",
      "Trained batch 284 batch loss 1.40518928 epoch total loss 1.40441287\n",
      "Trained batch 285 batch loss 1.44621062 epoch total loss 1.40455937\n",
      "Trained batch 286 batch loss 1.35925186 epoch total loss 1.40440106\n",
      "Trained batch 287 batch loss 1.35570109 epoch total loss 1.40423143\n",
      "Trained batch 288 batch loss 1.37457144 epoch total loss 1.40412843\n",
      "Trained batch 289 batch loss 1.39204693 epoch total loss 1.40408659\n",
      "Trained batch 290 batch loss 1.37966859 epoch total loss 1.40400243\n",
      "Trained batch 291 batch loss 1.36153328 epoch total loss 1.40385652\n",
      "Trained batch 292 batch loss 1.40383482 epoch total loss 1.40385652\n",
      "Trained batch 293 batch loss 1.56944871 epoch total loss 1.40442169\n",
      "Trained batch 294 batch loss 1.31956315 epoch total loss 1.40413296\n",
      "Trained batch 295 batch loss 1.37334657 epoch total loss 1.40402865\n",
      "Trained batch 296 batch loss 1.30349064 epoch total loss 1.40368903\n",
      "Trained batch 297 batch loss 1.48576117 epoch total loss 1.40396523\n",
      "Trained batch 298 batch loss 1.5035665 epoch total loss 1.4042995\n",
      "Trained batch 299 batch loss 1.439394 epoch total loss 1.40441692\n",
      "Trained batch 300 batch loss 1.40993071 epoch total loss 1.40443528\n",
      "Trained batch 301 batch loss 1.4068594 epoch total loss 1.40444338\n",
      "Trained batch 302 batch loss 1.35652709 epoch total loss 1.40428472\n",
      "Trained batch 303 batch loss 1.43623245 epoch total loss 1.4043901\n",
      "Trained batch 304 batch loss 1.44195664 epoch total loss 1.40451372\n",
      "Trained batch 305 batch loss 1.44307709 epoch total loss 1.4046402\n",
      "Trained batch 306 batch loss 1.42832494 epoch total loss 1.40471756\n",
      "Trained batch 307 batch loss 1.52275622 epoch total loss 1.40510201\n",
      "Trained batch 308 batch loss 1.40876389 epoch total loss 1.40511394\n",
      "Trained batch 309 batch loss 1.44583809 epoch total loss 1.40524566\n",
      "Trained batch 310 batch loss 1.41282094 epoch total loss 1.4052701\n",
      "Trained batch 311 batch loss 1.45394874 epoch total loss 1.40542662\n",
      "Trained batch 312 batch loss 1.48747373 epoch total loss 1.4056896\n",
      "Trained batch 313 batch loss 1.40437913 epoch total loss 1.40568542\n",
      "Trained batch 314 batch loss 1.38272119 epoch total loss 1.40561235\n",
      "Trained batch 315 batch loss 1.40656495 epoch total loss 1.40561533\n",
      "Trained batch 316 batch loss 1.36145782 epoch total loss 1.40547562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 317 batch loss 1.35322499 epoch total loss 1.40531075\n",
      "Trained batch 318 batch loss 1.3648057 epoch total loss 1.40518332\n",
      "Trained batch 319 batch loss 1.30670369 epoch total loss 1.40487456\n",
      "Trained batch 320 batch loss 1.35819125 epoch total loss 1.40472865\n",
      "Trained batch 321 batch loss 1.42571378 epoch total loss 1.4047941\n",
      "Trained batch 322 batch loss 1.39481473 epoch total loss 1.4047631\n",
      "Trained batch 323 batch loss 1.40851545 epoch total loss 1.40477467\n",
      "Trained batch 324 batch loss 1.42055881 epoch total loss 1.40482342\n",
      "Trained batch 325 batch loss 1.42511141 epoch total loss 1.40488577\n",
      "Trained batch 326 batch loss 1.37075329 epoch total loss 1.4047811\n",
      "Trained batch 327 batch loss 1.35272121 epoch total loss 1.40462196\n",
      "Trained batch 328 batch loss 1.25382161 epoch total loss 1.40416217\n",
      "Trained batch 329 batch loss 1.21796656 epoch total loss 1.40359616\n",
      "Trained batch 330 batch loss 1.28015208 epoch total loss 1.40322208\n",
      "Trained batch 331 batch loss 1.37698305 epoch total loss 1.40314281\n",
      "Trained batch 332 batch loss 1.31242931 epoch total loss 1.40286958\n",
      "Trained batch 333 batch loss 1.35619569 epoch total loss 1.40272951\n",
      "Trained batch 334 batch loss 1.44334447 epoch total loss 1.4028511\n",
      "Trained batch 335 batch loss 1.39215171 epoch total loss 1.40281916\n",
      "Trained batch 336 batch loss 1.38333237 epoch total loss 1.40276122\n",
      "Trained batch 337 batch loss 1.30164063 epoch total loss 1.40246105\n",
      "Trained batch 338 batch loss 1.37464952 epoch total loss 1.40237892\n",
      "Trained batch 339 batch loss 1.44941962 epoch total loss 1.40251768\n",
      "Trained batch 340 batch loss 1.32212901 epoch total loss 1.40228128\n",
      "Trained batch 341 batch loss 1.04811621 epoch total loss 1.40124273\n",
      "Trained batch 342 batch loss 1.10460925 epoch total loss 1.40037537\n",
      "Trained batch 343 batch loss 1.27014804 epoch total loss 1.39999568\n",
      "Trained batch 344 batch loss 1.54108143 epoch total loss 1.40040576\n",
      "Trained batch 345 batch loss 1.59329259 epoch total loss 1.40096486\n",
      "Trained batch 346 batch loss 1.57118535 epoch total loss 1.40145683\n",
      "Trained batch 347 batch loss 1.35680985 epoch total loss 1.40132821\n",
      "Trained batch 348 batch loss 1.33698773 epoch total loss 1.40114331\n",
      "Trained batch 349 batch loss 1.30034554 epoch total loss 1.40085447\n",
      "Trained batch 350 batch loss 1.32614124 epoch total loss 1.40064108\n",
      "Trained batch 351 batch loss 1.4492321 epoch total loss 1.40077949\n",
      "Trained batch 352 batch loss 1.35580516 epoch total loss 1.40065169\n",
      "Trained batch 353 batch loss 1.49091077 epoch total loss 1.4009074\n",
      "Trained batch 354 batch loss 1.44987953 epoch total loss 1.40104568\n",
      "Trained batch 355 batch loss 1.56950521 epoch total loss 1.40152025\n",
      "Trained batch 356 batch loss 1.46127963 epoch total loss 1.4016881\n",
      "Trained batch 357 batch loss 1.52048111 epoch total loss 1.40202093\n",
      "Trained batch 358 batch loss 1.37107205 epoch total loss 1.40193439\n",
      "Trained batch 359 batch loss 1.34272671 epoch total loss 1.4017694\n",
      "Trained batch 360 batch loss 1.40108657 epoch total loss 1.40176761\n",
      "Trained batch 361 batch loss 1.4086647 epoch total loss 1.40178668\n",
      "Trained batch 362 batch loss 1.36868 epoch total loss 1.40169525\n",
      "Trained batch 363 batch loss 1.36317635 epoch total loss 1.40158916\n",
      "Trained batch 364 batch loss 1.30545104 epoch total loss 1.40132499\n",
      "Trained batch 365 batch loss 1.32017303 epoch total loss 1.40110266\n",
      "Trained batch 366 batch loss 1.28174353 epoch total loss 1.40077651\n",
      "Trained batch 367 batch loss 1.32619476 epoch total loss 1.40057325\n",
      "Trained batch 368 batch loss 1.43481731 epoch total loss 1.40066624\n",
      "Trained batch 369 batch loss 1.41248727 epoch total loss 1.4006983\n",
      "Trained batch 370 batch loss 1.44125593 epoch total loss 1.40080798\n",
      "Trained batch 371 batch loss 1.3900528 epoch total loss 1.40077901\n",
      "Trained batch 372 batch loss 1.4193579 epoch total loss 1.40082908\n",
      "Trained batch 373 batch loss 1.3747797 epoch total loss 1.4007591\n",
      "Trained batch 374 batch loss 1.24279177 epoch total loss 1.40033674\n",
      "Trained batch 375 batch loss 1.28447545 epoch total loss 1.40002787\n",
      "Trained batch 376 batch loss 1.2421726 epoch total loss 1.39960802\n",
      "Trained batch 377 batch loss 1.30782795 epoch total loss 1.39936447\n",
      "Trained batch 378 batch loss 1.31054974 epoch total loss 1.39912951\n",
      "Trained batch 379 batch loss 1.24894977 epoch total loss 1.39873338\n",
      "Trained batch 380 batch loss 1.32771897 epoch total loss 1.39854646\n",
      "Trained batch 381 batch loss 1.29003859 epoch total loss 1.39826155\n",
      "Trained batch 382 batch loss 1.4522531 epoch total loss 1.39840293\n",
      "Trained batch 383 batch loss 1.52495372 epoch total loss 1.39873338\n",
      "Trained batch 384 batch loss 1.33519816 epoch total loss 1.39856803\n",
      "Trained batch 385 batch loss 1.34622669 epoch total loss 1.39843214\n",
      "Trained batch 386 batch loss 1.39967299 epoch total loss 1.39843524\n",
      "Trained batch 387 batch loss 1.45777154 epoch total loss 1.39858854\n",
      "Trained batch 388 batch loss 1.39549387 epoch total loss 1.39858067\n",
      "Trained batch 389 batch loss 1.39032257 epoch total loss 1.39855945\n",
      "Trained batch 390 batch loss 1.50239611 epoch total loss 1.39882565\n",
      "Trained batch 391 batch loss 1.46953583 epoch total loss 1.39900649\n",
      "Trained batch 392 batch loss 1.34762013 epoch total loss 1.39887536\n",
      "Trained batch 393 batch loss 1.49800885 epoch total loss 1.39912748\n",
      "Trained batch 394 batch loss 1.39055 epoch total loss 1.39910579\n",
      "Trained batch 395 batch loss 1.29232061 epoch total loss 1.39883542\n",
      "Trained batch 396 batch loss 1.36688173 epoch total loss 1.39875472\n",
      "Trained batch 397 batch loss 1.27423739 epoch total loss 1.39844108\n",
      "Trained batch 398 batch loss 1.41552806 epoch total loss 1.39848399\n",
      "Trained batch 399 batch loss 1.37199616 epoch total loss 1.39841759\n",
      "Trained batch 400 batch loss 1.52531195 epoch total loss 1.39873493\n",
      "Trained batch 401 batch loss 1.37526059 epoch total loss 1.39867628\n",
      "Trained batch 402 batch loss 1.35160947 epoch total loss 1.39855921\n",
      "Trained batch 403 batch loss 1.34109211 epoch total loss 1.39841664\n",
      "Trained batch 404 batch loss 1.37095344 epoch total loss 1.39834869\n",
      "Trained batch 405 batch loss 1.34180593 epoch total loss 1.39820898\n",
      "Trained batch 406 batch loss 1.36100566 epoch total loss 1.39811742\n",
      "Trained batch 407 batch loss 1.27793157 epoch total loss 1.39782214\n",
      "Trained batch 408 batch loss 1.43865013 epoch total loss 1.39792228\n",
      "Trained batch 409 batch loss 1.35526311 epoch total loss 1.39781809\n",
      "Trained batch 410 batch loss 1.36796308 epoch total loss 1.39774525\n",
      "Trained batch 411 batch loss 1.32825351 epoch total loss 1.39757621\n",
      "Trained batch 412 batch loss 1.28233492 epoch total loss 1.39729655\n",
      "Trained batch 413 batch loss 1.34467602 epoch total loss 1.39716911\n",
      "Trained batch 414 batch loss 1.37328315 epoch total loss 1.39711142\n",
      "Trained batch 415 batch loss 1.48493505 epoch total loss 1.39732301\n",
      "Trained batch 416 batch loss 1.36136925 epoch total loss 1.39723659\n",
      "Trained batch 417 batch loss 1.26673734 epoch total loss 1.39692366\n",
      "Trained batch 418 batch loss 1.30009186 epoch total loss 1.39669204\n",
      "Trained batch 419 batch loss 1.32622588 epoch total loss 1.39652383\n",
      "Trained batch 420 batch loss 1.45049751 epoch total loss 1.39665234\n",
      "Trained batch 421 batch loss 1.47305107 epoch total loss 1.39683378\n",
      "Trained batch 422 batch loss 1.39484978 epoch total loss 1.39682901\n",
      "Trained batch 423 batch loss 1.45706344 epoch total loss 1.39697146\n",
      "Trained batch 424 batch loss 1.38407731 epoch total loss 1.39694118\n",
      "Trained batch 425 batch loss 1.36590981 epoch total loss 1.39686811\n",
      "Trained batch 426 batch loss 1.43167734 epoch total loss 1.39694989\n",
      "Trained batch 427 batch loss 1.32838416 epoch total loss 1.39678931\n",
      "Trained batch 428 batch loss 1.4795655 epoch total loss 1.39698267\n",
      "Trained batch 429 batch loss 1.52346134 epoch total loss 1.39727736\n",
      "Trained batch 430 batch loss 1.43298423 epoch total loss 1.39736044\n",
      "Trained batch 431 batch loss 1.41202915 epoch total loss 1.39739454\n",
      "Trained batch 432 batch loss 1.24003708 epoch total loss 1.39703035\n",
      "Trained batch 433 batch loss 1.42581558 epoch total loss 1.39709687\n",
      "Trained batch 434 batch loss 1.44580412 epoch total loss 1.39720905\n",
      "Trained batch 435 batch loss 1.44625092 epoch total loss 1.3973217\n",
      "Trained batch 436 batch loss 1.46618509 epoch total loss 1.39747965\n",
      "Trained batch 437 batch loss 1.24546981 epoch total loss 1.39713192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 438 batch loss 1.26846254 epoch total loss 1.39683807\n",
      "Trained batch 439 batch loss 1.40371978 epoch total loss 1.3968538\n",
      "Trained batch 440 batch loss 1.31918502 epoch total loss 1.39667737\n",
      "Trained batch 441 batch loss 1.33927631 epoch total loss 1.3965472\n",
      "Trained batch 442 batch loss 1.36324942 epoch total loss 1.39647186\n",
      "Trained batch 443 batch loss 1.50695729 epoch total loss 1.39672124\n",
      "Trained batch 444 batch loss 1.37591243 epoch total loss 1.39667439\n",
      "Trained batch 445 batch loss 1.34144735 epoch total loss 1.39655018\n",
      "Trained batch 446 batch loss 1.46694601 epoch total loss 1.39670801\n",
      "Trained batch 447 batch loss 1.48707092 epoch total loss 1.39691007\n",
      "Trained batch 448 batch loss 1.50296307 epoch total loss 1.39714694\n",
      "Trained batch 449 batch loss 1.53805423 epoch total loss 1.3974607\n",
      "Trained batch 450 batch loss 1.56081223 epoch total loss 1.39782357\n",
      "Trained batch 451 batch loss 1.20350838 epoch total loss 1.39739275\n",
      "Trained batch 452 batch loss 1.28851461 epoch total loss 1.39715183\n",
      "Trained batch 453 batch loss 1.57981205 epoch total loss 1.39755511\n",
      "Trained batch 454 batch loss 1.63259077 epoch total loss 1.39807272\n",
      "Trained batch 455 batch loss 1.38053167 epoch total loss 1.39803421\n",
      "Trained batch 456 batch loss 1.46094465 epoch total loss 1.39817226\n",
      "Trained batch 457 batch loss 1.38430524 epoch total loss 1.39814186\n",
      "Trained batch 458 batch loss 1.41296268 epoch total loss 1.39817417\n",
      "Trained batch 459 batch loss 1.42105162 epoch total loss 1.39822412\n",
      "Trained batch 460 batch loss 1.41197467 epoch total loss 1.39825404\n",
      "Trained batch 461 batch loss 1.52390945 epoch total loss 1.39852655\n",
      "Trained batch 462 batch loss 1.36334729 epoch total loss 1.39845049\n",
      "Trained batch 463 batch loss 1.4823606 epoch total loss 1.39863169\n",
      "Trained batch 464 batch loss 1.52846909 epoch total loss 1.39891148\n",
      "Trained batch 465 batch loss 1.4885869 epoch total loss 1.39910424\n",
      "Trained batch 466 batch loss 1.38667071 epoch total loss 1.39907753\n",
      "Trained batch 467 batch loss 1.45348883 epoch total loss 1.39919412\n",
      "Trained batch 468 batch loss 1.32208967 epoch total loss 1.39902937\n",
      "Trained batch 469 batch loss 1.33879411 epoch total loss 1.39890087\n",
      "Trained batch 470 batch loss 1.36549854 epoch total loss 1.39882982\n",
      "Trained batch 471 batch loss 1.41576254 epoch total loss 1.39886582\n",
      "Trained batch 472 batch loss 1.40981841 epoch total loss 1.39888895\n",
      "Trained batch 473 batch loss 1.29150474 epoch total loss 1.39866185\n",
      "Trained batch 474 batch loss 1.36526656 epoch total loss 1.39859152\n",
      "Trained batch 475 batch loss 1.35224056 epoch total loss 1.39849389\n",
      "Trained batch 476 batch loss 1.32783794 epoch total loss 1.39834547\n",
      "Trained batch 477 batch loss 1.26408195 epoch total loss 1.39806402\n",
      "Trained batch 478 batch loss 1.14932764 epoch total loss 1.39754367\n",
      "Trained batch 479 batch loss 1.06057453 epoch total loss 1.3968401\n",
      "Trained batch 480 batch loss 1.22616 epoch total loss 1.39648449\n",
      "Trained batch 481 batch loss 1.52043593 epoch total loss 1.39674222\n",
      "Trained batch 482 batch loss 1.56447649 epoch total loss 1.3970902\n",
      "Trained batch 483 batch loss 1.65037477 epoch total loss 1.3976146\n",
      "Trained batch 484 batch loss 1.53844082 epoch total loss 1.39790559\n",
      "Trained batch 485 batch loss 1.27548206 epoch total loss 1.3976531\n",
      "Trained batch 486 batch loss 1.39792216 epoch total loss 1.3976537\n",
      "Trained batch 487 batch loss 1.37674022 epoch total loss 1.39761078\n",
      "Trained batch 488 batch loss 1.44342184 epoch total loss 1.39770472\n",
      "Trained batch 489 batch loss 1.37144542 epoch total loss 1.39765108\n",
      "Trained batch 490 batch loss 1.42845047 epoch total loss 1.3977139\n",
      "Trained batch 491 batch loss 1.41095364 epoch total loss 1.39774084\n",
      "Trained batch 492 batch loss 1.37491775 epoch total loss 1.39769459\n",
      "Trained batch 493 batch loss 1.44162524 epoch total loss 1.39778364\n",
      "Trained batch 494 batch loss 1.34574938 epoch total loss 1.39767838\n",
      "Trained batch 495 batch loss 1.31568551 epoch total loss 1.39751267\n",
      "Trained batch 496 batch loss 1.35467589 epoch total loss 1.39742637\n",
      "Trained batch 497 batch loss 1.3659544 epoch total loss 1.39736307\n",
      "Trained batch 498 batch loss 1.18858945 epoch total loss 1.39694381\n",
      "Trained batch 499 batch loss 1.32432032 epoch total loss 1.39679837\n",
      "Trained batch 500 batch loss 1.28366065 epoch total loss 1.39657199\n",
      "Trained batch 501 batch loss 1.2674787 epoch total loss 1.39631426\n",
      "Trained batch 502 batch loss 1.4246881 epoch total loss 1.39637077\n",
      "Trained batch 503 batch loss 1.36778367 epoch total loss 1.39631402\n",
      "Trained batch 504 batch loss 1.39899468 epoch total loss 1.39631927\n",
      "Trained batch 505 batch loss 1.31791067 epoch total loss 1.39616406\n",
      "Trained batch 506 batch loss 1.32111311 epoch total loss 1.39601576\n",
      "Trained batch 507 batch loss 1.29233623 epoch total loss 1.39581132\n",
      "Trained batch 508 batch loss 1.38187313 epoch total loss 1.3957839\n",
      "Trained batch 509 batch loss 1.36682892 epoch total loss 1.39572704\n",
      "Trained batch 510 batch loss 1.30583072 epoch total loss 1.39555073\n",
      "Trained batch 511 batch loss 1.26519918 epoch total loss 1.39529574\n",
      "Trained batch 512 batch loss 1.39813948 epoch total loss 1.39530122\n",
      "Trained batch 513 batch loss 1.2249558 epoch total loss 1.39496922\n",
      "Trained batch 514 batch loss 1.44913256 epoch total loss 1.39507461\n",
      "Trained batch 515 batch loss 1.44063663 epoch total loss 1.39516306\n",
      "Trained batch 516 batch loss 1.3489821 epoch total loss 1.39507353\n",
      "Trained batch 517 batch loss 1.1760987 epoch total loss 1.39465\n",
      "Trained batch 518 batch loss 1.26521969 epoch total loss 1.39440012\n",
      "Trained batch 519 batch loss 1.2145977 epoch total loss 1.3940537\n",
      "Trained batch 520 batch loss 1.29223108 epoch total loss 1.39385784\n",
      "Trained batch 521 batch loss 1.34211433 epoch total loss 1.39375854\n",
      "Trained batch 522 batch loss 1.2559098 epoch total loss 1.39349449\n",
      "Trained batch 523 batch loss 1.43065751 epoch total loss 1.39356554\n",
      "Trained batch 524 batch loss 1.37902081 epoch total loss 1.39353776\n",
      "Trained batch 525 batch loss 1.33104682 epoch total loss 1.39341879\n",
      "Trained batch 526 batch loss 1.4280405 epoch total loss 1.39348459\n",
      "Trained batch 527 batch loss 1.35971832 epoch total loss 1.39342058\n",
      "Trained batch 528 batch loss 1.39675629 epoch total loss 1.3934269\n",
      "Trained batch 529 batch loss 1.38275719 epoch total loss 1.39340663\n",
      "Trained batch 530 batch loss 1.17519832 epoch total loss 1.39299488\n",
      "Trained batch 531 batch loss 1.19188845 epoch total loss 1.39261615\n",
      "Trained batch 532 batch loss 1.30429459 epoch total loss 1.39245021\n",
      "Trained batch 533 batch loss 1.35929704 epoch total loss 1.39238799\n",
      "Trained batch 534 batch loss 1.17098451 epoch total loss 1.39197338\n",
      "Trained batch 535 batch loss 1.33707619 epoch total loss 1.39187086\n",
      "Trained batch 536 batch loss 1.30056739 epoch total loss 1.39170039\n",
      "Trained batch 537 batch loss 1.46764088 epoch total loss 1.39184189\n",
      "Trained batch 538 batch loss 1.37646627 epoch total loss 1.39181328\n",
      "Trained batch 539 batch loss 1.4069277 epoch total loss 1.39184129\n",
      "Trained batch 540 batch loss 1.51819396 epoch total loss 1.3920753\n",
      "Trained batch 541 batch loss 1.56136298 epoch total loss 1.39238811\n",
      "Trained batch 542 batch loss 1.59826636 epoch total loss 1.39276803\n",
      "Trained batch 543 batch loss 1.47467577 epoch total loss 1.39291883\n",
      "Trained batch 544 batch loss 1.30352342 epoch total loss 1.39275455\n",
      "Trained batch 545 batch loss 1.35285974 epoch total loss 1.39268124\n",
      "Trained batch 546 batch loss 1.28605843 epoch total loss 1.39248598\n",
      "Trained batch 547 batch loss 1.34086454 epoch total loss 1.39239168\n",
      "Trained batch 548 batch loss 1.37396288 epoch total loss 1.39235806\n",
      "Trained batch 549 batch loss 1.46242881 epoch total loss 1.39248562\n",
      "Trained batch 550 batch loss 1.30653226 epoch total loss 1.39232934\n",
      "Trained batch 551 batch loss 1.34971511 epoch total loss 1.39225197\n",
      "Trained batch 552 batch loss 1.3590312 epoch total loss 1.39219177\n",
      "Trained batch 553 batch loss 1.3452158 epoch total loss 1.39210689\n",
      "Trained batch 554 batch loss 1.38343596 epoch total loss 1.39209116\n",
      "Trained batch 555 batch loss 1.34265244 epoch total loss 1.39200211\n",
      "Trained batch 556 batch loss 1.28656089 epoch total loss 1.39181244\n",
      "Trained batch 557 batch loss 1.41111326 epoch total loss 1.39184713\n",
      "Trained batch 558 batch loss 1.36082458 epoch total loss 1.39179158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 559 batch loss 1.45281339 epoch total loss 1.39190078\n",
      "Trained batch 560 batch loss 1.36988389 epoch total loss 1.39186144\n",
      "Trained batch 561 batch loss 1.45715737 epoch total loss 1.39197779\n",
      "Trained batch 562 batch loss 1.4519608 epoch total loss 1.39208448\n",
      "Trained batch 563 batch loss 1.39601064 epoch total loss 1.39209151\n",
      "Trained batch 564 batch loss 1.35901284 epoch total loss 1.39203286\n",
      "Trained batch 565 batch loss 1.32115817 epoch total loss 1.39190733\n",
      "Trained batch 566 batch loss 1.2887944 epoch total loss 1.3917253\n",
      "Trained batch 567 batch loss 1.43482053 epoch total loss 1.39180124\n",
      "Trained batch 568 batch loss 1.38192868 epoch total loss 1.39178395\n",
      "Trained batch 569 batch loss 1.48004317 epoch total loss 1.39193904\n",
      "Trained batch 570 batch loss 1.25189722 epoch total loss 1.39169335\n",
      "Trained batch 571 batch loss 1.33484375 epoch total loss 1.39159381\n",
      "Trained batch 572 batch loss 1.29426432 epoch total loss 1.39142358\n",
      "Trained batch 573 batch loss 1.23704219 epoch total loss 1.39115417\n",
      "Trained batch 574 batch loss 1.31273699 epoch total loss 1.39101756\n",
      "Trained batch 575 batch loss 1.27263761 epoch total loss 1.39081168\n",
      "Trained batch 576 batch loss 1.29871511 epoch total loss 1.39065182\n",
      "Trained batch 577 batch loss 1.23360181 epoch total loss 1.39037955\n",
      "Trained batch 578 batch loss 1.24856162 epoch total loss 1.39013422\n",
      "Trained batch 579 batch loss 1.35386682 epoch total loss 1.39007163\n",
      "Trained batch 580 batch loss 1.31637359 epoch total loss 1.38994443\n",
      "Trained batch 581 batch loss 1.27295089 epoch total loss 1.38974309\n",
      "Trained batch 582 batch loss 1.35623085 epoch total loss 1.38968551\n",
      "Trained batch 583 batch loss 1.39214039 epoch total loss 1.38968968\n",
      "Trained batch 584 batch loss 1.35777259 epoch total loss 1.38963509\n",
      "Trained batch 585 batch loss 1.36449945 epoch total loss 1.38959205\n",
      "Trained batch 586 batch loss 1.44761288 epoch total loss 1.38969111\n",
      "Trained batch 587 batch loss 1.41901088 epoch total loss 1.38974106\n",
      "Trained batch 588 batch loss 1.34689343 epoch total loss 1.38966823\n",
      "Trained batch 589 batch loss 1.33986771 epoch total loss 1.38958371\n",
      "Trained batch 590 batch loss 1.38043618 epoch total loss 1.38956821\n",
      "Trained batch 591 batch loss 1.37928772 epoch total loss 1.38955069\n",
      "Trained batch 592 batch loss 1.34220195 epoch total loss 1.38947082\n",
      "Trained batch 593 batch loss 1.35869849 epoch total loss 1.38941896\n",
      "Trained batch 594 batch loss 1.37749672 epoch total loss 1.38939881\n",
      "Trained batch 595 batch loss 1.43856502 epoch total loss 1.38948143\n",
      "Trained batch 596 batch loss 1.52597427 epoch total loss 1.38971055\n",
      "Trained batch 597 batch loss 1.38322234 epoch total loss 1.3896997\n",
      "Trained batch 598 batch loss 1.48272586 epoch total loss 1.38985527\n",
      "Trained batch 599 batch loss 1.31944752 epoch total loss 1.38973773\n",
      "Trained batch 600 batch loss 1.26097655 epoch total loss 1.38952315\n",
      "Trained batch 601 batch loss 1.39310551 epoch total loss 1.38952911\n",
      "Trained batch 602 batch loss 1.37786746 epoch total loss 1.3895098\n",
      "Trained batch 603 batch loss 1.35258889 epoch total loss 1.38944852\n",
      "Trained batch 604 batch loss 1.40948296 epoch total loss 1.38948166\n",
      "Trained batch 605 batch loss 1.3603034 epoch total loss 1.3894335\n",
      "Trained batch 606 batch loss 1.39030933 epoch total loss 1.38943493\n",
      "Trained batch 607 batch loss 1.28653955 epoch total loss 1.38926542\n",
      "Trained batch 608 batch loss 1.16598141 epoch total loss 1.38889813\n",
      "Trained batch 609 batch loss 1.25339854 epoch total loss 1.38867569\n",
      "Trained batch 610 batch loss 1.35903513 epoch total loss 1.38862705\n",
      "Trained batch 611 batch loss 1.41044176 epoch total loss 1.38866282\n",
      "Trained batch 612 batch loss 1.44568 epoch total loss 1.38875592\n",
      "Trained batch 613 batch loss 1.45079982 epoch total loss 1.38885713\n",
      "Trained batch 614 batch loss 1.50550818 epoch total loss 1.38904715\n",
      "Trained batch 615 batch loss 1.49930036 epoch total loss 1.38922644\n",
      "Trained batch 616 batch loss 1.36516368 epoch total loss 1.38918746\n",
      "Trained batch 617 batch loss 1.4116329 epoch total loss 1.38922381\n",
      "Trained batch 618 batch loss 1.45869744 epoch total loss 1.38933611\n",
      "Trained batch 619 batch loss 1.43564725 epoch total loss 1.38941097\n",
      "Trained batch 620 batch loss 1.34724164 epoch total loss 1.38934302\n",
      "Trained batch 621 batch loss 1.36037588 epoch total loss 1.38929629\n",
      "Trained batch 622 batch loss 1.22508073 epoch total loss 1.38903224\n",
      "Trained batch 623 batch loss 1.26092458 epoch total loss 1.38882661\n",
      "Trained batch 624 batch loss 1.3763454 epoch total loss 1.3888067\n",
      "Trained batch 625 batch loss 1.35797632 epoch total loss 1.38875735\n",
      "Trained batch 626 batch loss 1.35114145 epoch total loss 1.38869727\n",
      "Trained batch 627 batch loss 1.4801507 epoch total loss 1.38884306\n",
      "Trained batch 628 batch loss 1.51468372 epoch total loss 1.38904357\n",
      "Trained batch 629 batch loss 1.45430255 epoch total loss 1.38914728\n",
      "Trained batch 630 batch loss 1.42566526 epoch total loss 1.38920522\n",
      "Trained batch 631 batch loss 1.3717289 epoch total loss 1.38917744\n",
      "Trained batch 632 batch loss 1.43507993 epoch total loss 1.38925\n",
      "Trained batch 633 batch loss 1.46067 epoch total loss 1.38936293\n",
      "Trained batch 634 batch loss 1.44480348 epoch total loss 1.38945043\n",
      "Trained batch 635 batch loss 1.45202279 epoch total loss 1.3895489\n",
      "Trained batch 636 batch loss 1.39232838 epoch total loss 1.38955331\n",
      "Trained batch 637 batch loss 1.3727957 epoch total loss 1.38952708\n",
      "Trained batch 638 batch loss 1.40205145 epoch total loss 1.38954663\n",
      "Trained batch 639 batch loss 1.31823874 epoch total loss 1.38943505\n",
      "Trained batch 640 batch loss 1.38179564 epoch total loss 1.38942313\n",
      "Trained batch 641 batch loss 1.40255451 epoch total loss 1.38944352\n",
      "Trained batch 642 batch loss 1.42018509 epoch total loss 1.38949144\n",
      "Trained batch 643 batch loss 1.35300577 epoch total loss 1.3894347\n",
      "Trained batch 644 batch loss 1.28868008 epoch total loss 1.38927829\n",
      "Trained batch 645 batch loss 1.25995028 epoch total loss 1.38907778\n",
      "Trained batch 646 batch loss 1.240713 epoch total loss 1.38884807\n",
      "Trained batch 647 batch loss 1.28749704 epoch total loss 1.38869143\n",
      "Trained batch 648 batch loss 1.31471741 epoch total loss 1.38857722\n",
      "Trained batch 649 batch loss 1.23161185 epoch total loss 1.38833535\n",
      "Trained batch 650 batch loss 1.2233355 epoch total loss 1.38808155\n",
      "Trained batch 651 batch loss 1.32160747 epoch total loss 1.38797939\n",
      "Trained batch 652 batch loss 1.32663512 epoch total loss 1.38788533\n",
      "Trained batch 653 batch loss 1.19463336 epoch total loss 1.38758945\n",
      "Trained batch 654 batch loss 1.28568423 epoch total loss 1.38743365\n",
      "Trained batch 655 batch loss 1.41899776 epoch total loss 1.38748181\n",
      "Trained batch 656 batch loss 1.45709014 epoch total loss 1.3875879\n",
      "Trained batch 657 batch loss 1.45133436 epoch total loss 1.38768494\n",
      "Trained batch 658 batch loss 1.42799771 epoch total loss 1.38774621\n",
      "Trained batch 659 batch loss 1.28284168 epoch total loss 1.38758707\n",
      "Trained batch 660 batch loss 1.4351207 epoch total loss 1.38765907\n",
      "Trained batch 661 batch loss 1.47369432 epoch total loss 1.38778925\n",
      "Trained batch 662 batch loss 1.40246868 epoch total loss 1.38781142\n",
      "Trained batch 663 batch loss 1.3592788 epoch total loss 1.38776827\n",
      "Trained batch 664 batch loss 1.39861488 epoch total loss 1.3877846\n",
      "Trained batch 665 batch loss 1.29180229 epoch total loss 1.38764036\n",
      "Trained batch 666 batch loss 1.24820828 epoch total loss 1.38743103\n",
      "Trained batch 667 batch loss 1.37063575 epoch total loss 1.38740575\n",
      "Trained batch 668 batch loss 1.41185498 epoch total loss 1.38744235\n",
      "Trained batch 669 batch loss 1.38888478 epoch total loss 1.3874445\n",
      "Trained batch 670 batch loss 1.28187835 epoch total loss 1.3872869\n",
      "Trained batch 671 batch loss 1.28200197 epoch total loss 1.38713\n",
      "Trained batch 672 batch loss 1.32354796 epoch total loss 1.38703537\n",
      "Trained batch 673 batch loss 1.38062441 epoch total loss 1.38702583\n",
      "Trained batch 674 batch loss 1.32386851 epoch total loss 1.38693213\n",
      "Trained batch 675 batch loss 1.43495095 epoch total loss 1.38700318\n",
      "Trained batch 676 batch loss 1.32453442 epoch total loss 1.3869108\n",
      "Trained batch 677 batch loss 1.30394614 epoch total loss 1.38678825\n",
      "Trained batch 678 batch loss 1.34082699 epoch total loss 1.38672042\n",
      "Trained batch 679 batch loss 1.36477351 epoch total loss 1.38668811\n",
      "Trained batch 680 batch loss 1.3211906 epoch total loss 1.38659179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 681 batch loss 1.41220367 epoch total loss 1.38662934\n",
      "Trained batch 682 batch loss 1.40826881 epoch total loss 1.38666117\n",
      "Trained batch 683 batch loss 1.41571379 epoch total loss 1.38670361\n",
      "Trained batch 684 batch loss 1.4530071 epoch total loss 1.38680053\n",
      "Trained batch 685 batch loss 1.40372169 epoch total loss 1.38682532\n",
      "Trained batch 686 batch loss 1.39937723 epoch total loss 1.38684356\n",
      "Trained batch 687 batch loss 1.44757152 epoch total loss 1.38693202\n",
      "Trained batch 688 batch loss 1.41678286 epoch total loss 1.38697541\n",
      "Trained batch 689 batch loss 1.46571422 epoch total loss 1.38708961\n",
      "Trained batch 690 batch loss 1.35905516 epoch total loss 1.38704908\n",
      "Trained batch 691 batch loss 1.30756962 epoch total loss 1.38693404\n",
      "Trained batch 692 batch loss 1.3485539 epoch total loss 1.38687861\n",
      "Trained batch 693 batch loss 1.25001216 epoch total loss 1.38668108\n",
      "Trained batch 694 batch loss 1.33004808 epoch total loss 1.38659954\n",
      "Trained batch 695 batch loss 1.32320464 epoch total loss 1.38650823\n",
      "Trained batch 696 batch loss 1.38947308 epoch total loss 1.38651252\n",
      "Trained batch 697 batch loss 1.43749082 epoch total loss 1.38658559\n",
      "Trained batch 698 batch loss 1.34243369 epoch total loss 1.38652241\n",
      "Trained batch 699 batch loss 1.27121651 epoch total loss 1.38635743\n",
      "Trained batch 700 batch loss 1.19153476 epoch total loss 1.38607907\n",
      "Trained batch 701 batch loss 1.31720459 epoch total loss 1.38598084\n",
      "Trained batch 702 batch loss 1.38233149 epoch total loss 1.3859756\n",
      "Trained batch 703 batch loss 1.39130127 epoch total loss 1.38598323\n",
      "Trained batch 704 batch loss 1.40789104 epoch total loss 1.38601434\n",
      "Trained batch 705 batch loss 1.46933794 epoch total loss 1.3861326\n",
      "Trained batch 706 batch loss 1.48115301 epoch total loss 1.38626707\n",
      "Trained batch 707 batch loss 1.41440427 epoch total loss 1.386307\n",
      "Trained batch 708 batch loss 1.4348228 epoch total loss 1.38637543\n",
      "Trained batch 709 batch loss 1.39752841 epoch total loss 1.38639116\n",
      "Trained batch 710 batch loss 1.41750729 epoch total loss 1.38643503\n",
      "Trained batch 711 batch loss 1.3301245 epoch total loss 1.38635576\n",
      "Trained batch 712 batch loss 1.42843044 epoch total loss 1.38641489\n",
      "Trained batch 713 batch loss 1.50615788 epoch total loss 1.38658285\n",
      "Trained batch 714 batch loss 1.40688324 epoch total loss 1.38661122\n",
      "Trained batch 715 batch loss 1.34164524 epoch total loss 1.3865484\n",
      "Trained batch 716 batch loss 1.37308931 epoch total loss 1.38652956\n",
      "Trained batch 717 batch loss 1.35174584 epoch total loss 1.38648105\n",
      "Trained batch 718 batch loss 1.43109894 epoch total loss 1.38654315\n",
      "Trained batch 719 batch loss 1.37871838 epoch total loss 1.38653231\n",
      "Trained batch 720 batch loss 1.48561966 epoch total loss 1.38666987\n",
      "Trained batch 721 batch loss 1.65138531 epoch total loss 1.38703704\n",
      "Trained batch 722 batch loss 1.48242664 epoch total loss 1.38716912\n",
      "Trained batch 723 batch loss 1.38075292 epoch total loss 1.3871603\n",
      "Trained batch 724 batch loss 1.26563036 epoch total loss 1.38699245\n",
      "Trained batch 725 batch loss 1.16240144 epoch total loss 1.38668263\n",
      "Trained batch 726 batch loss 1.22390783 epoch total loss 1.38645852\n",
      "Trained batch 727 batch loss 1.32214272 epoch total loss 1.38637\n",
      "Trained batch 728 batch loss 1.15983438 epoch total loss 1.38605881\n",
      "Trained batch 729 batch loss 1.13599789 epoch total loss 1.38571584\n",
      "Trained batch 730 batch loss 1.08796835 epoch total loss 1.38530791\n",
      "Trained batch 731 batch loss 1.19521904 epoch total loss 1.38504779\n",
      "Trained batch 732 batch loss 1.25510359 epoch total loss 1.38487041\n",
      "Trained batch 733 batch loss 1.27665603 epoch total loss 1.38472271\n",
      "Trained batch 734 batch loss 1.37285435 epoch total loss 1.38470662\n",
      "Trained batch 735 batch loss 1.44280577 epoch total loss 1.38478565\n",
      "Trained batch 736 batch loss 1.51356113 epoch total loss 1.38496053\n",
      "Trained batch 737 batch loss 1.35800099 epoch total loss 1.38492393\n",
      "Trained batch 738 batch loss 1.39300871 epoch total loss 1.3849349\n",
      "Trained batch 739 batch loss 1.39315128 epoch total loss 1.38494599\n",
      "Trained batch 740 batch loss 1.45988357 epoch total loss 1.3850472\n",
      "Trained batch 741 batch loss 1.49696 epoch total loss 1.38519824\n",
      "Trained batch 742 batch loss 1.33573115 epoch total loss 1.38513148\n",
      "Trained batch 743 batch loss 1.45434761 epoch total loss 1.3852247\n",
      "Trained batch 744 batch loss 1.34377027 epoch total loss 1.38516891\n",
      "Trained batch 745 batch loss 1.41537619 epoch total loss 1.38520956\n",
      "Trained batch 746 batch loss 1.41464162 epoch total loss 1.38524902\n",
      "Trained batch 747 batch loss 1.40091705 epoch total loss 1.38526988\n",
      "Trained batch 748 batch loss 1.42841637 epoch total loss 1.3853277\n",
      "Trained batch 749 batch loss 1.32499909 epoch total loss 1.38524711\n",
      "Trained batch 750 batch loss 1.42021167 epoch total loss 1.3852936\n",
      "Trained batch 751 batch loss 1.40553617 epoch total loss 1.38532054\n",
      "Trained batch 752 batch loss 1.44128788 epoch total loss 1.38539493\n",
      "Trained batch 753 batch loss 1.37011993 epoch total loss 1.38537467\n",
      "Trained batch 754 batch loss 1.3470453 epoch total loss 1.38532388\n",
      "Trained batch 755 batch loss 1.41280293 epoch total loss 1.38536024\n",
      "Trained batch 756 batch loss 1.39303756 epoch total loss 1.38537049\n",
      "Trained batch 757 batch loss 1.28192019 epoch total loss 1.38523376\n",
      "Trained batch 758 batch loss 1.26346028 epoch total loss 1.38507307\n",
      "Trained batch 759 batch loss 1.18416858 epoch total loss 1.38480842\n",
      "Trained batch 760 batch loss 1.45021713 epoch total loss 1.38489449\n",
      "Trained batch 761 batch loss 1.23105955 epoch total loss 1.38469231\n",
      "Trained batch 762 batch loss 1.27182877 epoch total loss 1.38454425\n",
      "Trained batch 763 batch loss 1.31656051 epoch total loss 1.38445508\n",
      "Trained batch 764 batch loss 1.39660251 epoch total loss 1.38447094\n",
      "Trained batch 765 batch loss 1.2732116 epoch total loss 1.3843255\n",
      "Trained batch 766 batch loss 1.34599662 epoch total loss 1.38427544\n",
      "Trained batch 767 batch loss 1.50076926 epoch total loss 1.38442731\n",
      "Trained batch 768 batch loss 1.39722 epoch total loss 1.38444388\n",
      "Trained batch 769 batch loss 1.27744627 epoch total loss 1.38430476\n",
      "Trained batch 770 batch loss 1.23110509 epoch total loss 1.3841058\n",
      "Trained batch 771 batch loss 1.23911202 epoch total loss 1.38391781\n",
      "Trained batch 772 batch loss 1.24060106 epoch total loss 1.38373208\n",
      "Trained batch 773 batch loss 1.26343274 epoch total loss 1.38357651\n",
      "Trained batch 774 batch loss 1.20377648 epoch total loss 1.38334417\n",
      "Trained batch 775 batch loss 1.26633239 epoch total loss 1.38319325\n",
      "Trained batch 776 batch loss 1.34225547 epoch total loss 1.38314044\n",
      "Trained batch 777 batch loss 1.31503952 epoch total loss 1.38305283\n",
      "Trained batch 778 batch loss 1.34553492 epoch total loss 1.38300467\n",
      "Trained batch 779 batch loss 1.42734027 epoch total loss 1.38306165\n",
      "Trained batch 780 batch loss 1.43590176 epoch total loss 1.38312936\n",
      "Trained batch 781 batch loss 1.38641739 epoch total loss 1.38313365\n",
      "Trained batch 782 batch loss 1.29647112 epoch total loss 1.3830229\n",
      "Trained batch 783 batch loss 1.28189373 epoch total loss 1.38289368\n",
      "Trained batch 784 batch loss 1.14956582 epoch total loss 1.38259602\n",
      "Trained batch 785 batch loss 1.14948428 epoch total loss 1.38229918\n",
      "Trained batch 786 batch loss 1.22332656 epoch total loss 1.38209689\n",
      "Trained batch 787 batch loss 1.38820314 epoch total loss 1.38210464\n",
      "Trained batch 788 batch loss 1.43744254 epoch total loss 1.38217485\n",
      "Trained batch 789 batch loss 1.50782537 epoch total loss 1.38233411\n",
      "Trained batch 790 batch loss 1.46234775 epoch total loss 1.38243544\n",
      "Trained batch 791 batch loss 1.33198845 epoch total loss 1.38237178\n",
      "Trained batch 792 batch loss 1.36316419 epoch total loss 1.38234746\n",
      "Trained batch 793 batch loss 1.39908183 epoch total loss 1.38236856\n",
      "Trained batch 794 batch loss 1.40555549 epoch total loss 1.38239765\n",
      "Trained batch 795 batch loss 1.44189835 epoch total loss 1.38247252\n",
      "Trained batch 796 batch loss 1.45254421 epoch total loss 1.38256049\n",
      "Trained batch 797 batch loss 1.51279247 epoch total loss 1.38272393\n",
      "Trained batch 798 batch loss 1.44309211 epoch total loss 1.38279963\n",
      "Trained batch 799 batch loss 1.43007326 epoch total loss 1.38285875\n",
      "Trained batch 800 batch loss 1.34214687 epoch total loss 1.38280797\n",
      "Trained batch 801 batch loss 1.34538078 epoch total loss 1.38276112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 802 batch loss 1.26265264 epoch total loss 1.38261139\n",
      "Trained batch 803 batch loss 1.36020291 epoch total loss 1.38258362\n",
      "Trained batch 804 batch loss 1.3284117 epoch total loss 1.38251615\n",
      "Trained batch 805 batch loss 1.34662771 epoch total loss 1.38247156\n",
      "Trained batch 806 batch loss 1.36938202 epoch total loss 1.38245535\n",
      "Trained batch 807 batch loss 1.42292523 epoch total loss 1.38250554\n",
      "Trained batch 808 batch loss 1.38645196 epoch total loss 1.38251054\n",
      "Trained batch 809 batch loss 1.37285304 epoch total loss 1.3824985\n",
      "Trained batch 810 batch loss 1.39798415 epoch total loss 1.38251758\n",
      "Trained batch 811 batch loss 1.3061111 epoch total loss 1.3824234\n",
      "Trained batch 812 batch loss 1.26987529 epoch total loss 1.38228488\n",
      "Trained batch 813 batch loss 1.38773537 epoch total loss 1.38229144\n",
      "Trained batch 814 batch loss 1.44919789 epoch total loss 1.38237369\n",
      "Trained batch 815 batch loss 1.34903812 epoch total loss 1.3823328\n",
      "Trained batch 816 batch loss 1.3953619 epoch total loss 1.38234878\n",
      "Trained batch 817 batch loss 1.38334095 epoch total loss 1.38235\n",
      "Trained batch 818 batch loss 1.36607981 epoch total loss 1.38233006\n",
      "Trained batch 819 batch loss 1.39941096 epoch total loss 1.38235092\n",
      "Trained batch 820 batch loss 1.20846581 epoch total loss 1.38213885\n",
      "Trained batch 821 batch loss 1.29612839 epoch total loss 1.38203418\n",
      "Trained batch 822 batch loss 1.43451595 epoch total loss 1.38209808\n",
      "Trained batch 823 batch loss 1.3921206 epoch total loss 1.38211024\n",
      "Trained batch 824 batch loss 1.39314628 epoch total loss 1.38212359\n",
      "Trained batch 825 batch loss 1.29464388 epoch total loss 1.38201761\n",
      "Trained batch 826 batch loss 1.26860929 epoch total loss 1.38188028\n",
      "Trained batch 827 batch loss 1.33859015 epoch total loss 1.38182795\n",
      "Trained batch 828 batch loss 1.30848372 epoch total loss 1.38173938\n",
      "Trained batch 829 batch loss 1.21723139 epoch total loss 1.38154101\n",
      "Trained batch 830 batch loss 1.29278576 epoch total loss 1.38143408\n",
      "Trained batch 831 batch loss 1.22517407 epoch total loss 1.38124621\n",
      "Trained batch 832 batch loss 1.28488171 epoch total loss 1.38113034\n",
      "Trained batch 833 batch loss 1.30828905 epoch total loss 1.38104296\n",
      "Trained batch 834 batch loss 1.24019217 epoch total loss 1.38087416\n",
      "Trained batch 835 batch loss 1.17435431 epoch total loss 1.3806268\n",
      "Trained batch 836 batch loss 1.17881298 epoch total loss 1.3803854\n",
      "Trained batch 837 batch loss 1.25205398 epoch total loss 1.3802321\n",
      "Trained batch 838 batch loss 1.22848582 epoch total loss 1.38005102\n",
      "Trained batch 839 batch loss 1.30367351 epoch total loss 1.37996006\n",
      "Trained batch 840 batch loss 1.31229568 epoch total loss 1.37987947\n",
      "Trained batch 841 batch loss 1.31877124 epoch total loss 1.37980676\n",
      "Trained batch 842 batch loss 1.38146043 epoch total loss 1.37980878\n",
      "Trained batch 843 batch loss 1.51490271 epoch total loss 1.379969\n",
      "Trained batch 844 batch loss 1.39231944 epoch total loss 1.37998366\n",
      "Trained batch 845 batch loss 1.38093388 epoch total loss 1.37998486\n",
      "Trained batch 846 batch loss 1.2674427 epoch total loss 1.37985182\n",
      "Trained batch 847 batch loss 1.39528942 epoch total loss 1.37986994\n",
      "Trained batch 848 batch loss 1.52120233 epoch total loss 1.38003671\n",
      "Trained batch 849 batch loss 1.35909867 epoch total loss 1.38001204\n",
      "Trained batch 850 batch loss 1.43110406 epoch total loss 1.38007224\n",
      "Trained batch 851 batch loss 1.43407428 epoch total loss 1.38013566\n",
      "Trained batch 852 batch loss 1.40213227 epoch total loss 1.38016152\n",
      "Trained batch 853 batch loss 1.27440393 epoch total loss 1.38003755\n",
      "Trained batch 854 batch loss 1.3508029 epoch total loss 1.38000333\n",
      "Trained batch 855 batch loss 1.40601254 epoch total loss 1.38003373\n",
      "Trained batch 856 batch loss 1.34725 epoch total loss 1.37999547\n",
      "Trained batch 857 batch loss 1.30943406 epoch total loss 1.37991321\n",
      "Trained batch 858 batch loss 1.39310634 epoch total loss 1.37992847\n",
      "Trained batch 859 batch loss 1.32471085 epoch total loss 1.37986422\n",
      "Trained batch 860 batch loss 1.27987814 epoch total loss 1.37974799\n",
      "Trained batch 861 batch loss 1.21135569 epoch total loss 1.37955236\n",
      "Trained batch 862 batch loss 1.18542683 epoch total loss 1.37932718\n",
      "Trained batch 863 batch loss 1.20269346 epoch total loss 1.37912238\n",
      "Trained batch 864 batch loss 1.48380172 epoch total loss 1.37924349\n",
      "Trained batch 865 batch loss 1.37857389 epoch total loss 1.37924266\n",
      "Trained batch 866 batch loss 1.57352054 epoch total loss 1.37946701\n",
      "Trained batch 867 batch loss 1.48419142 epoch total loss 1.37958765\n",
      "Trained batch 868 batch loss 1.38020468 epoch total loss 1.37958848\n",
      "Trained batch 869 batch loss 1.36170244 epoch total loss 1.37956786\n",
      "Trained batch 870 batch loss 1.29120171 epoch total loss 1.37946641\n",
      "Trained batch 871 batch loss 1.31736779 epoch total loss 1.37939513\n",
      "Trained batch 872 batch loss 1.34092522 epoch total loss 1.37935102\n",
      "Trained batch 873 batch loss 1.28025889 epoch total loss 1.37923753\n",
      "Trained batch 874 batch loss 1.22480083 epoch total loss 1.37906086\n",
      "Trained batch 875 batch loss 1.29404056 epoch total loss 1.37896371\n",
      "Trained batch 876 batch loss 1.32450747 epoch total loss 1.37890148\n",
      "Trained batch 877 batch loss 1.2096808 epoch total loss 1.3787086\n",
      "Trained batch 878 batch loss 1.3504653 epoch total loss 1.37867641\n",
      "Trained batch 879 batch loss 1.29547536 epoch total loss 1.37858188\n",
      "Trained batch 880 batch loss 1.23711276 epoch total loss 1.37842107\n",
      "Trained batch 881 batch loss 1.45255899 epoch total loss 1.37850511\n",
      "Trained batch 882 batch loss 1.41106176 epoch total loss 1.37854195\n",
      "Trained batch 883 batch loss 1.39855778 epoch total loss 1.3785646\n",
      "Trained batch 884 batch loss 1.49063778 epoch total loss 1.37869143\n",
      "Trained batch 885 batch loss 1.46644974 epoch total loss 1.3787905\n",
      "Trained batch 886 batch loss 1.45516658 epoch total loss 1.37887681\n",
      "Trained batch 887 batch loss 1.36508751 epoch total loss 1.37886119\n",
      "Trained batch 888 batch loss 1.30922687 epoch total loss 1.37878275\n",
      "Trained batch 889 batch loss 1.37564421 epoch total loss 1.37877929\n",
      "Trained batch 890 batch loss 1.36658764 epoch total loss 1.37876558\n",
      "Trained batch 891 batch loss 1.36049366 epoch total loss 1.37874496\n",
      "Trained batch 892 batch loss 1.34206057 epoch total loss 1.37870383\n",
      "Trained batch 893 batch loss 1.24824405 epoch total loss 1.3785578\n",
      "Trained batch 894 batch loss 1.25163007 epoch total loss 1.37841582\n",
      "Trained batch 895 batch loss 1.22358358 epoch total loss 1.37824285\n",
      "Trained batch 896 batch loss 1.3301568 epoch total loss 1.37818921\n",
      "Trained batch 897 batch loss 1.41174579 epoch total loss 1.37822664\n",
      "Trained batch 898 batch loss 1.55667448 epoch total loss 1.37842536\n",
      "Trained batch 899 batch loss 1.54684746 epoch total loss 1.37861264\n",
      "Trained batch 900 batch loss 1.47864556 epoch total loss 1.37872386\n",
      "Trained batch 901 batch loss 1.44591141 epoch total loss 1.37879837\n",
      "Trained batch 902 batch loss 1.34630322 epoch total loss 1.37876236\n",
      "Trained batch 903 batch loss 1.28523719 epoch total loss 1.37865889\n",
      "Trained batch 904 batch loss 1.25163567 epoch total loss 1.37851834\n",
      "Trained batch 905 batch loss 1.34468722 epoch total loss 1.37848091\n",
      "Trained batch 906 batch loss 1.39554977 epoch total loss 1.37849975\n",
      "Trained batch 907 batch loss 1.34770775 epoch total loss 1.37846577\n",
      "Trained batch 908 batch loss 1.32783055 epoch total loss 1.37841\n",
      "Trained batch 909 batch loss 1.34260499 epoch total loss 1.37837064\n",
      "Trained batch 910 batch loss 1.39006042 epoch total loss 1.37838352\n",
      "Trained batch 911 batch loss 1.26993871 epoch total loss 1.37826443\n",
      "Trained batch 912 batch loss 1.28044152 epoch total loss 1.37815714\n",
      "Trained batch 913 batch loss 1.28191614 epoch total loss 1.37805164\n",
      "Trained batch 914 batch loss 1.20698285 epoch total loss 1.37786448\n",
      "Trained batch 915 batch loss 1.34126782 epoch total loss 1.37782454\n",
      "Trained batch 916 batch loss 1.38276958 epoch total loss 1.37783\n",
      "Trained batch 917 batch loss 1.2654748 epoch total loss 1.37770748\n",
      "Trained batch 918 batch loss 1.32325315 epoch total loss 1.37764823\n",
      "Trained batch 919 batch loss 1.38977981 epoch total loss 1.37766135\n",
      "Trained batch 920 batch loss 1.27963686 epoch total loss 1.37755489\n",
      "Trained batch 921 batch loss 1.5088222 epoch total loss 1.37769735\n",
      "Trained batch 922 batch loss 1.40393913 epoch total loss 1.37772584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 923 batch loss 1.30178547 epoch total loss 1.37764347\n",
      "Trained batch 924 batch loss 1.25310934 epoch total loss 1.37750864\n",
      "Trained batch 925 batch loss 1.25467753 epoch total loss 1.37737584\n",
      "Trained batch 926 batch loss 1.41925442 epoch total loss 1.37742114\n",
      "Trained batch 927 batch loss 1.31570125 epoch total loss 1.3773545\n",
      "Trained batch 928 batch loss 1.43228698 epoch total loss 1.37741363\n",
      "Trained batch 929 batch loss 1.406039 epoch total loss 1.37744439\n",
      "Trained batch 930 batch loss 1.29700828 epoch total loss 1.37735796\n",
      "Trained batch 931 batch loss 1.26474953 epoch total loss 1.37723696\n",
      "Trained batch 932 batch loss 1.36994445 epoch total loss 1.37722921\n",
      "Trained batch 933 batch loss 1.33042574 epoch total loss 1.37717903\n",
      "Trained batch 934 batch loss 1.33038867 epoch total loss 1.37712908\n",
      "Trained batch 935 batch loss 1.33899081 epoch total loss 1.37708831\n",
      "Trained batch 936 batch loss 1.42896342 epoch total loss 1.37714362\n",
      "Trained batch 937 batch loss 1.47767353 epoch total loss 1.37725091\n",
      "Trained batch 938 batch loss 1.54477513 epoch total loss 1.3774296\n",
      "Trained batch 939 batch loss 1.58217573 epoch total loss 1.37764764\n",
      "Trained batch 940 batch loss 1.47549164 epoch total loss 1.37775159\n",
      "Trained batch 941 batch loss 1.2444694 epoch total loss 1.37761009\n",
      "Trained batch 942 batch loss 1.31723833 epoch total loss 1.37754595\n",
      "Trained batch 943 batch loss 1.33247459 epoch total loss 1.37749827\n",
      "Trained batch 944 batch loss 1.21321249 epoch total loss 1.37732422\n",
      "Trained batch 945 batch loss 1.32008219 epoch total loss 1.37726367\n",
      "Trained batch 946 batch loss 1.2988447 epoch total loss 1.3771807\n",
      "Trained batch 947 batch loss 1.28376961 epoch total loss 1.37708211\n",
      "Trained batch 948 batch loss 1.25045168 epoch total loss 1.3769486\n",
      "Trained batch 949 batch loss 1.23414409 epoch total loss 1.37679815\n",
      "Trained batch 950 batch loss 1.20796227 epoch total loss 1.37662041\n",
      "Trained batch 951 batch loss 1.34061611 epoch total loss 1.3765825\n",
      "Trained batch 952 batch loss 1.35956311 epoch total loss 1.37656474\n",
      "Trained batch 953 batch loss 1.13225031 epoch total loss 1.37630832\n",
      "Trained batch 954 batch loss 1.27361572 epoch total loss 1.37620056\n",
      "Trained batch 955 batch loss 1.21912777 epoch total loss 1.37603617\n",
      "Trained batch 956 batch loss 1.38729048 epoch total loss 1.37604797\n",
      "Trained batch 957 batch loss 1.46779692 epoch total loss 1.37614381\n",
      "Trained batch 958 batch loss 1.42585576 epoch total loss 1.37619567\n",
      "Trained batch 959 batch loss 1.32980084 epoch total loss 1.37614739\n",
      "Trained batch 960 batch loss 1.3844837 epoch total loss 1.37615609\n",
      "Trained batch 961 batch loss 1.43454182 epoch total loss 1.37621689\n",
      "Trained batch 962 batch loss 1.35472846 epoch total loss 1.3761946\n",
      "Trained batch 963 batch loss 1.39411652 epoch total loss 1.37621319\n",
      "Trained batch 964 batch loss 1.33399296 epoch total loss 1.37616944\n",
      "Trained batch 965 batch loss 1.31991792 epoch total loss 1.37611115\n",
      "Trained batch 966 batch loss 1.53355968 epoch total loss 1.37627411\n",
      "Trained batch 967 batch loss 1.37840736 epoch total loss 1.37627637\n",
      "Trained batch 968 batch loss 1.40629792 epoch total loss 1.37630737\n",
      "Trained batch 969 batch loss 1.32197666 epoch total loss 1.37625134\n",
      "Trained batch 970 batch loss 1.31251836 epoch total loss 1.37618554\n",
      "Trained batch 971 batch loss 1.17968524 epoch total loss 1.37598324\n",
      "Trained batch 972 batch loss 1.22800767 epoch total loss 1.37583101\n",
      "Trained batch 973 batch loss 1.3008275 epoch total loss 1.37575388\n",
      "Trained batch 974 batch loss 1.34701085 epoch total loss 1.37572443\n",
      "Trained batch 975 batch loss 1.46157622 epoch total loss 1.37581241\n",
      "Trained batch 976 batch loss 1.55709779 epoch total loss 1.37599826\n",
      "Trained batch 977 batch loss 1.53481865 epoch total loss 1.37616074\n",
      "Trained batch 978 batch loss 1.59825385 epoch total loss 1.37638783\n",
      "Trained batch 979 batch loss 1.31810498 epoch total loss 1.37632835\n",
      "Trained batch 980 batch loss 1.33573651 epoch total loss 1.37628686\n",
      "Trained batch 981 batch loss 1.25621939 epoch total loss 1.37616444\n",
      "Trained batch 982 batch loss 1.38064 epoch total loss 1.37616897\n",
      "Trained batch 983 batch loss 1.32136643 epoch total loss 1.3761133\n",
      "Trained batch 984 batch loss 1.34479702 epoch total loss 1.37608147\n",
      "Trained batch 985 batch loss 1.31294322 epoch total loss 1.37601745\n",
      "Trained batch 986 batch loss 1.4993459 epoch total loss 1.37614262\n",
      "Trained batch 987 batch loss 1.44043374 epoch total loss 1.37620771\n",
      "Trained batch 988 batch loss 1.46438491 epoch total loss 1.37629688\n",
      "Trained batch 989 batch loss 1.30525494 epoch total loss 1.37622511\n",
      "Trained batch 990 batch loss 1.34990692 epoch total loss 1.37619853\n",
      "Trained batch 991 batch loss 1.3036356 epoch total loss 1.37612522\n",
      "Trained batch 992 batch loss 1.42336464 epoch total loss 1.37617278\n",
      "Trained batch 993 batch loss 1.2995764 epoch total loss 1.37609565\n",
      "Trained batch 994 batch loss 1.31580329 epoch total loss 1.37603498\n",
      "Trained batch 995 batch loss 1.25268888 epoch total loss 1.375911\n",
      "Trained batch 996 batch loss 1.32598495 epoch total loss 1.37586081\n",
      "Trained batch 997 batch loss 1.31810665 epoch total loss 1.37580299\n",
      "Trained batch 998 batch loss 1.27509165 epoch total loss 1.37570214\n",
      "Trained batch 999 batch loss 1.19454527 epoch total loss 1.37552083\n",
      "Trained batch 1000 batch loss 1.4018333 epoch total loss 1.37554717\n",
      "Trained batch 1001 batch loss 1.36980629 epoch total loss 1.37554133\n",
      "Trained batch 1002 batch loss 1.44233644 epoch total loss 1.37560809\n",
      "Trained batch 1003 batch loss 1.4628644 epoch total loss 1.37569511\n",
      "Trained batch 1004 batch loss 1.49384403 epoch total loss 1.37581277\n",
      "Trained batch 1005 batch loss 1.38417602 epoch total loss 1.37582111\n",
      "Trained batch 1006 batch loss 1.38649285 epoch total loss 1.37583172\n",
      "Trained batch 1007 batch loss 1.21936619 epoch total loss 1.37567627\n",
      "Trained batch 1008 batch loss 1.36832428 epoch total loss 1.375669\n",
      "Trained batch 1009 batch loss 1.36365414 epoch total loss 1.37565708\n",
      "Trained batch 1010 batch loss 1.35419583 epoch total loss 1.37563586\n",
      "Trained batch 1011 batch loss 1.45102215 epoch total loss 1.37571049\n",
      "Trained batch 1012 batch loss 1.5037117 epoch total loss 1.37583685\n",
      "Trained batch 1013 batch loss 1.52558196 epoch total loss 1.37598479\n",
      "Trained batch 1014 batch loss 1.48076844 epoch total loss 1.37608802\n",
      "Trained batch 1015 batch loss 1.49551117 epoch total loss 1.37620568\n",
      "Trained batch 1016 batch loss 1.41434562 epoch total loss 1.37624311\n",
      "Trained batch 1017 batch loss 1.47603357 epoch total loss 1.37634134\n",
      "Trained batch 1018 batch loss 1.44812989 epoch total loss 1.3764118\n",
      "Trained batch 1019 batch loss 1.45652354 epoch total loss 1.37649047\n",
      "Trained batch 1020 batch loss 1.39732409 epoch total loss 1.37651098\n",
      "Trained batch 1021 batch loss 1.45047879 epoch total loss 1.37658334\n",
      "Trained batch 1022 batch loss 1.37575471 epoch total loss 1.3765825\n",
      "Trained batch 1023 batch loss 1.39258265 epoch total loss 1.37659812\n",
      "Trained batch 1024 batch loss 1.44295478 epoch total loss 1.37666297\n",
      "Trained batch 1025 batch loss 1.41489744 epoch total loss 1.37670028\n",
      "Trained batch 1026 batch loss 1.35278487 epoch total loss 1.37667704\n",
      "Trained batch 1027 batch loss 1.30095172 epoch total loss 1.37660325\n",
      "Trained batch 1028 batch loss 1.26618922 epoch total loss 1.37649584\n",
      "Trained batch 1029 batch loss 1.27384698 epoch total loss 1.37639606\n",
      "Trained batch 1030 batch loss 1.42148113 epoch total loss 1.37643981\n",
      "Trained batch 1031 batch loss 1.46069372 epoch total loss 1.37652159\n",
      "Trained batch 1032 batch loss 1.47438693 epoch total loss 1.37661636\n",
      "Trained batch 1033 batch loss 1.34035993 epoch total loss 1.37658119\n",
      "Trained batch 1034 batch loss 1.35382462 epoch total loss 1.37655926\n",
      "Trained batch 1035 batch loss 1.47632265 epoch total loss 1.3766557\n",
      "Trained batch 1036 batch loss 1.35078025 epoch total loss 1.37663078\n",
      "Trained batch 1037 batch loss 1.31626236 epoch total loss 1.37657261\n",
      "Trained batch 1038 batch loss 1.32256043 epoch total loss 1.37652051\n",
      "Trained batch 1039 batch loss 1.28764796 epoch total loss 1.37643492\n",
      "Trained batch 1040 batch loss 1.31059575 epoch total loss 1.3763715\n",
      "Trained batch 1041 batch loss 1.41498566 epoch total loss 1.3764087\n",
      "Trained batch 1042 batch loss 1.37439537 epoch total loss 1.37640679\n",
      "Trained batch 1043 batch loss 1.31794071 epoch total loss 1.37635076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1044 batch loss 1.27289963 epoch total loss 1.3762517\n",
      "Trained batch 1045 batch loss 1.17669368 epoch total loss 1.37606072\n",
      "Trained batch 1046 batch loss 1.1050446 epoch total loss 1.37580168\n",
      "Trained batch 1047 batch loss 1.5286063 epoch total loss 1.37594748\n",
      "Trained batch 1048 batch loss 1.53812253 epoch total loss 1.37610221\n",
      "Trained batch 1049 batch loss 1.43000162 epoch total loss 1.37615371\n",
      "Trained batch 1050 batch loss 1.25939298 epoch total loss 1.37604249\n",
      "Trained batch 1051 batch loss 1.33904397 epoch total loss 1.3760072\n",
      "Trained batch 1052 batch loss 1.27876687 epoch total loss 1.37591481\n",
      "Trained batch 1053 batch loss 1.28683162 epoch total loss 1.37583029\n",
      "Trained batch 1054 batch loss 1.25430739 epoch total loss 1.3757149\n",
      "Trained batch 1055 batch loss 1.32067871 epoch total loss 1.3756628\n",
      "Trained batch 1056 batch loss 1.30256832 epoch total loss 1.37559354\n",
      "Trained batch 1057 batch loss 1.34917164 epoch total loss 1.37556851\n",
      "Trained batch 1058 batch loss 1.37762296 epoch total loss 1.37557042\n",
      "Trained batch 1059 batch loss 1.36578035 epoch total loss 1.37556112\n",
      "Trained batch 1060 batch loss 1.29751599 epoch total loss 1.37548745\n",
      "Trained batch 1061 batch loss 1.20239437 epoch total loss 1.37532437\n",
      "Trained batch 1062 batch loss 1.26749623 epoch total loss 1.3752228\n",
      "Trained batch 1063 batch loss 1.40238249 epoch total loss 1.37524831\n",
      "Trained batch 1064 batch loss 1.37431085 epoch total loss 1.37524736\n",
      "Trained batch 1065 batch loss 1.38998759 epoch total loss 1.37526119\n",
      "Trained batch 1066 batch loss 1.31901073 epoch total loss 1.37520838\n",
      "Trained batch 1067 batch loss 1.32857347 epoch total loss 1.37516475\n",
      "Trained batch 1068 batch loss 1.3018837 epoch total loss 1.37509608\n",
      "Trained batch 1069 batch loss 1.24288142 epoch total loss 1.37497246\n",
      "Trained batch 1070 batch loss 1.29289699 epoch total loss 1.37489569\n",
      "Trained batch 1071 batch loss 1.35122728 epoch total loss 1.37487364\n",
      "Trained batch 1072 batch loss 1.30679727 epoch total loss 1.3748101\n",
      "Trained batch 1073 batch loss 1.37329721 epoch total loss 1.37480867\n",
      "Trained batch 1074 batch loss 1.30132473 epoch total loss 1.37474012\n",
      "Trained batch 1075 batch loss 1.36578703 epoch total loss 1.3747319\n",
      "Trained batch 1076 batch loss 1.20018697 epoch total loss 1.37456965\n",
      "Trained batch 1077 batch loss 1.34940481 epoch total loss 1.37454629\n",
      "Trained batch 1078 batch loss 1.30236983 epoch total loss 1.37447929\n",
      "Trained batch 1079 batch loss 1.32588077 epoch total loss 1.37443435\n",
      "Trained batch 1080 batch loss 1.3698225 epoch total loss 1.37443006\n",
      "Trained batch 1081 batch loss 1.32675421 epoch total loss 1.37438607\n",
      "Trained batch 1082 batch loss 1.23646462 epoch total loss 1.37425852\n",
      "Trained batch 1083 batch loss 1.28312361 epoch total loss 1.37417436\n",
      "Trained batch 1084 batch loss 1.37665451 epoch total loss 1.37417674\n",
      "Trained batch 1085 batch loss 1.31437504 epoch total loss 1.37412155\n",
      "Trained batch 1086 batch loss 1.32096922 epoch total loss 1.37407255\n",
      "Trained batch 1087 batch loss 1.38223565 epoch total loss 1.37408006\n",
      "Trained batch 1088 batch loss 1.31014764 epoch total loss 1.37402129\n",
      "Trained batch 1089 batch loss 1.30197573 epoch total loss 1.37395513\n",
      "Trained batch 1090 batch loss 1.28377032 epoch total loss 1.37387252\n",
      "Trained batch 1091 batch loss 1.28329873 epoch total loss 1.37378943\n",
      "Trained batch 1092 batch loss 1.20792 epoch total loss 1.37363756\n",
      "Trained batch 1093 batch loss 1.3902514 epoch total loss 1.37365282\n",
      "Trained batch 1094 batch loss 1.34309399 epoch total loss 1.37362492\n",
      "Trained batch 1095 batch loss 1.23128307 epoch total loss 1.37349486\n",
      "Trained batch 1096 batch loss 1.35928237 epoch total loss 1.37348187\n",
      "Trained batch 1097 batch loss 1.28889346 epoch total loss 1.37340486\n",
      "Trained batch 1098 batch loss 1.22694767 epoch total loss 1.37327147\n",
      "Trained batch 1099 batch loss 1.35405219 epoch total loss 1.37325394\n",
      "Trained batch 1100 batch loss 1.24318242 epoch total loss 1.37313569\n",
      "Trained batch 1101 batch loss 1.31484354 epoch total loss 1.37308264\n",
      "Trained batch 1102 batch loss 1.23467791 epoch total loss 1.37295699\n",
      "Trained batch 1103 batch loss 1.30747151 epoch total loss 1.37289774\n",
      "Trained batch 1104 batch loss 1.19843817 epoch total loss 1.37273967\n",
      "Trained batch 1105 batch loss 1.37559021 epoch total loss 1.3727423\n",
      "Trained batch 1106 batch loss 1.37127173 epoch total loss 1.37274098\n",
      "Trained batch 1107 batch loss 1.37426329 epoch total loss 1.3727423\n",
      "Trained batch 1108 batch loss 1.35205758 epoch total loss 1.37272358\n",
      "Trained batch 1109 batch loss 1.28244805 epoch total loss 1.37264228\n",
      "Trained batch 1110 batch loss 1.28278756 epoch total loss 1.37256134\n",
      "Trained batch 1111 batch loss 1.16636276 epoch total loss 1.37237573\n",
      "Trained batch 1112 batch loss 1.28468156 epoch total loss 1.37229693\n",
      "Trained batch 1113 batch loss 1.26748073 epoch total loss 1.37220263\n",
      "Trained batch 1114 batch loss 1.23226035 epoch total loss 1.37207711\n",
      "Trained batch 1115 batch loss 1.30283654 epoch total loss 1.372015\n",
      "Trained batch 1116 batch loss 1.23511505 epoch total loss 1.37189233\n",
      "Trained batch 1117 batch loss 1.30188811 epoch total loss 1.37182963\n",
      "Trained batch 1118 batch loss 1.16131318 epoch total loss 1.37164128\n",
      "Trained batch 1119 batch loss 1.25626993 epoch total loss 1.37153816\n",
      "Trained batch 1120 batch loss 1.43791187 epoch total loss 1.37159741\n",
      "Trained batch 1121 batch loss 1.37912965 epoch total loss 1.37160408\n",
      "Trained batch 1122 batch loss 1.42224753 epoch total loss 1.37164927\n",
      "Trained batch 1123 batch loss 1.40831292 epoch total loss 1.37168193\n",
      "Trained batch 1124 batch loss 1.31018019 epoch total loss 1.37162721\n",
      "Trained batch 1125 batch loss 1.29003441 epoch total loss 1.37155473\n",
      "Trained batch 1126 batch loss 1.35947967 epoch total loss 1.371544\n",
      "Trained batch 1127 batch loss 1.36100781 epoch total loss 1.37153459\n",
      "Trained batch 1128 batch loss 1.2668221 epoch total loss 1.37144172\n",
      "Trained batch 1129 batch loss 1.25276244 epoch total loss 1.3713367\n",
      "Trained batch 1130 batch loss 1.30205679 epoch total loss 1.37127531\n",
      "Trained batch 1131 batch loss 1.34084833 epoch total loss 1.37124836\n",
      "Trained batch 1132 batch loss 1.36809015 epoch total loss 1.37124562\n",
      "Trained batch 1133 batch loss 1.31096137 epoch total loss 1.37119234\n",
      "Trained batch 1134 batch loss 1.20379293 epoch total loss 1.37104464\n",
      "Trained batch 1135 batch loss 1.3027792 epoch total loss 1.37098444\n",
      "Trained batch 1136 batch loss 1.40404987 epoch total loss 1.37101364\n",
      "Trained batch 1137 batch loss 1.41388869 epoch total loss 1.37105131\n",
      "Trained batch 1138 batch loss 1.37338018 epoch total loss 1.37105346\n",
      "Trained batch 1139 batch loss 1.40024471 epoch total loss 1.37107909\n",
      "Trained batch 1140 batch loss 1.21361279 epoch total loss 1.37094092\n",
      "Trained batch 1141 batch loss 1.24060154 epoch total loss 1.37082672\n",
      "Trained batch 1142 batch loss 1.34053087 epoch total loss 1.37080026\n",
      "Trained batch 1143 batch loss 1.46343827 epoch total loss 1.3708812\n",
      "Trained batch 1144 batch loss 1.3932749 epoch total loss 1.37090087\n",
      "Trained batch 1145 batch loss 1.42480838 epoch total loss 1.37094796\n",
      "Trained batch 1146 batch loss 1.45228124 epoch total loss 1.37101889\n",
      "Trained batch 1147 batch loss 1.43697882 epoch total loss 1.37107635\n",
      "Trained batch 1148 batch loss 1.3405081 epoch total loss 1.37104976\n",
      "Trained batch 1149 batch loss 1.31408465 epoch total loss 1.37100017\n",
      "Trained batch 1150 batch loss 1.492275 epoch total loss 1.37110567\n",
      "Trained batch 1151 batch loss 1.39462733 epoch total loss 1.37112606\n",
      "Trained batch 1152 batch loss 1.38890016 epoch total loss 1.37114155\n",
      "Trained batch 1153 batch loss 1.35450172 epoch total loss 1.37112713\n",
      "Trained batch 1154 batch loss 1.38557458 epoch total loss 1.37113965\n",
      "Trained batch 1155 batch loss 1.36611986 epoch total loss 1.37113523\n",
      "Trained batch 1156 batch loss 1.34455955 epoch total loss 1.37111235\n",
      "Trained batch 1157 batch loss 1.31245625 epoch total loss 1.37106168\n",
      "Trained batch 1158 batch loss 1.22582078 epoch total loss 1.37093627\n",
      "Trained batch 1159 batch loss 1.15705812 epoch total loss 1.37075174\n",
      "Trained batch 1160 batch loss 1.22215152 epoch total loss 1.37062371\n",
      "Trained batch 1161 batch loss 1.21083701 epoch total loss 1.37048602\n",
      "Trained batch 1162 batch loss 1.1786654 epoch total loss 1.37032104\n",
      "Trained batch 1163 batch loss 1.09280312 epoch total loss 1.37008238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1164 batch loss 1.06282306 epoch total loss 1.36981845\n",
      "Trained batch 1165 batch loss 1.01332664 epoch total loss 1.36951244\n",
      "Trained batch 1166 batch loss 1.21461225 epoch total loss 1.36937952\n",
      "Trained batch 1167 batch loss 1.32259524 epoch total loss 1.36933947\n",
      "Trained batch 1168 batch loss 1.33017027 epoch total loss 1.36930597\n",
      "Trained batch 1169 batch loss 1.27381325 epoch total loss 1.36922431\n",
      "Trained batch 1170 batch loss 1.29569972 epoch total loss 1.36916137\n",
      "Trained batch 1171 batch loss 1.37360537 epoch total loss 1.36916518\n",
      "Trained batch 1172 batch loss 1.36758256 epoch total loss 1.36916387\n",
      "Trained batch 1173 batch loss 1.28777552 epoch total loss 1.36909437\n",
      "Trained batch 1174 batch loss 1.27333856 epoch total loss 1.36901283\n",
      "Trained batch 1175 batch loss 1.15095842 epoch total loss 1.36882734\n",
      "Trained batch 1176 batch loss 1.18690014 epoch total loss 1.36867261\n",
      "Trained batch 1177 batch loss 1.24175966 epoch total loss 1.36856472\n",
      "Trained batch 1178 batch loss 1.27381098 epoch total loss 1.36848426\n",
      "Trained batch 1179 batch loss 1.37762499 epoch total loss 1.36849213\n",
      "Trained batch 1180 batch loss 1.14758241 epoch total loss 1.36830485\n",
      "Trained batch 1181 batch loss 1.42074096 epoch total loss 1.36834931\n",
      "Trained batch 1182 batch loss 1.36460936 epoch total loss 1.3683461\n",
      "Trained batch 1183 batch loss 1.40728378 epoch total loss 1.368379\n",
      "Trained batch 1184 batch loss 1.37554026 epoch total loss 1.36838496\n",
      "Trained batch 1185 batch loss 1.31986344 epoch total loss 1.36834407\n",
      "Trained batch 1186 batch loss 1.17937255 epoch total loss 1.36818469\n",
      "Trained batch 1187 batch loss 1.32741797 epoch total loss 1.36815023\n",
      "Trained batch 1188 batch loss 1.35131276 epoch total loss 1.36813617\n",
      "Trained batch 1189 batch loss 1.41941857 epoch total loss 1.3681792\n",
      "Trained batch 1190 batch loss 1.48267019 epoch total loss 1.36827552\n",
      "Trained batch 1191 batch loss 1.35561728 epoch total loss 1.36826479\n",
      "Trained batch 1192 batch loss 1.24198413 epoch total loss 1.36815882\n",
      "Trained batch 1193 batch loss 1.23828197 epoch total loss 1.36805\n",
      "Trained batch 1194 batch loss 1.30781341 epoch total loss 1.36799955\n",
      "Trained batch 1195 batch loss 1.34732068 epoch total loss 1.36798227\n",
      "Trained batch 1196 batch loss 1.29548919 epoch total loss 1.36792171\n",
      "Trained batch 1197 batch loss 1.15934587 epoch total loss 1.36774743\n",
      "Trained batch 1198 batch loss 1.31756639 epoch total loss 1.36770558\n",
      "Trained batch 1199 batch loss 1.28729141 epoch total loss 1.36763847\n",
      "Trained batch 1200 batch loss 1.35571992 epoch total loss 1.36762846\n",
      "Trained batch 1201 batch loss 1.36614227 epoch total loss 1.36762714\n",
      "Trained batch 1202 batch loss 1.28038192 epoch total loss 1.36755466\n",
      "Trained batch 1203 batch loss 1.24230075 epoch total loss 1.36745048\n",
      "Trained batch 1204 batch loss 1.35133636 epoch total loss 1.36743712\n",
      "Trained batch 1205 batch loss 1.39436758 epoch total loss 1.36745954\n",
      "Trained batch 1206 batch loss 1.37727368 epoch total loss 1.36746764\n",
      "Trained batch 1207 batch loss 1.33062088 epoch total loss 1.36743712\n",
      "Trained batch 1208 batch loss 1.40147352 epoch total loss 1.36746526\n",
      "Trained batch 1209 batch loss 1.25429869 epoch total loss 1.36737168\n",
      "Trained batch 1210 batch loss 1.22434235 epoch total loss 1.36725342\n",
      "Trained batch 1211 batch loss 1.2774806 epoch total loss 1.36717939\n",
      "Trained batch 1212 batch loss 1.3372606 epoch total loss 1.36715472\n",
      "Trained batch 1213 batch loss 1.22761273 epoch total loss 1.36703968\n",
      "Trained batch 1214 batch loss 1.29721785 epoch total loss 1.36698222\n",
      "Trained batch 1215 batch loss 1.2144556 epoch total loss 1.36685669\n",
      "Trained batch 1216 batch loss 1.34204388 epoch total loss 1.36683619\n",
      "Trained batch 1217 batch loss 1.50371802 epoch total loss 1.36694872\n",
      "Trained batch 1218 batch loss 1.53441668 epoch total loss 1.36708617\n",
      "Trained batch 1219 batch loss 1.64280701 epoch total loss 1.36731243\n",
      "Trained batch 1220 batch loss 1.51013637 epoch total loss 1.36742949\n",
      "Trained batch 1221 batch loss 1.4942162 epoch total loss 1.36753333\n",
      "Trained batch 1222 batch loss 1.36084604 epoch total loss 1.36752784\n",
      "Trained batch 1223 batch loss 1.39521861 epoch total loss 1.36755049\n",
      "Trained batch 1224 batch loss 1.15714443 epoch total loss 1.36737859\n",
      "Trained batch 1225 batch loss 1.30448365 epoch total loss 1.36732721\n",
      "Trained batch 1226 batch loss 1.36472785 epoch total loss 1.36732507\n",
      "Trained batch 1227 batch loss 1.46683598 epoch total loss 1.36740613\n",
      "Trained batch 1228 batch loss 1.30852175 epoch total loss 1.36735821\n",
      "Trained batch 1229 batch loss 1.27978015 epoch total loss 1.36728692\n",
      "Trained batch 1230 batch loss 1.29792285 epoch total loss 1.36723053\n",
      "Trained batch 1231 batch loss 1.1817019 epoch total loss 1.36707985\n",
      "Trained batch 1232 batch loss 1.27036452 epoch total loss 1.36700141\n",
      "Trained batch 1233 batch loss 1.27490461 epoch total loss 1.36692667\n",
      "Trained batch 1234 batch loss 1.30253983 epoch total loss 1.36687446\n",
      "Trained batch 1235 batch loss 1.3052572 epoch total loss 1.36682463\n",
      "Trained batch 1236 batch loss 1.28491855 epoch total loss 1.36675835\n",
      "Trained batch 1237 batch loss 1.38787556 epoch total loss 1.36677539\n",
      "Trained batch 1238 batch loss 1.25930572 epoch total loss 1.36668861\n",
      "Trained batch 1239 batch loss 1.37163138 epoch total loss 1.36669254\n",
      "Trained batch 1240 batch loss 1.41766417 epoch total loss 1.36673367\n",
      "Trained batch 1241 batch loss 1.28054547 epoch total loss 1.36666417\n",
      "Trained batch 1242 batch loss 1.28443801 epoch total loss 1.36659801\n",
      "Trained batch 1243 batch loss 1.15275419 epoch total loss 1.36642587\n",
      "Trained batch 1244 batch loss 1.25076306 epoch total loss 1.36633289\n",
      "Trained batch 1245 batch loss 1.2476269 epoch total loss 1.36623764\n",
      "Trained batch 1246 batch loss 1.3338151 epoch total loss 1.36621165\n",
      "Trained batch 1247 batch loss 1.40541196 epoch total loss 1.366243\n",
      "Trained batch 1248 batch loss 1.28864229 epoch total loss 1.3661809\n",
      "Trained batch 1249 batch loss 1.25871229 epoch total loss 1.36609483\n",
      "Trained batch 1250 batch loss 1.19866288 epoch total loss 1.36596084\n",
      "Trained batch 1251 batch loss 1.40363598 epoch total loss 1.365991\n",
      "Trained batch 1252 batch loss 1.48601389 epoch total loss 1.36608684\n",
      "Trained batch 1253 batch loss 1.30329585 epoch total loss 1.36603677\n",
      "Trained batch 1254 batch loss 1.33617592 epoch total loss 1.36601293\n",
      "Trained batch 1255 batch loss 1.13095796 epoch total loss 1.36582565\n",
      "Trained batch 1256 batch loss 1.08492 epoch total loss 1.36560202\n",
      "Trained batch 1257 batch loss 1.12673926 epoch total loss 1.365412\n",
      "Trained batch 1258 batch loss 1.14719 epoch total loss 1.36523855\n",
      "Trained batch 1259 batch loss 1.24644446 epoch total loss 1.36514425\n",
      "Trained batch 1260 batch loss 1.26243234 epoch total loss 1.36506271\n",
      "Trained batch 1261 batch loss 1.26135015 epoch total loss 1.36498046\n",
      "Trained batch 1262 batch loss 1.3622241 epoch total loss 1.36497819\n",
      "Trained batch 1263 batch loss 1.4463253 epoch total loss 1.36504257\n",
      "Trained batch 1264 batch loss 1.43268192 epoch total loss 1.36509621\n",
      "Trained batch 1265 batch loss 1.37463796 epoch total loss 1.36510372\n",
      "Trained batch 1266 batch loss 1.34286594 epoch total loss 1.3650862\n",
      "Trained batch 1267 batch loss 1.33446932 epoch total loss 1.365062\n",
      "Trained batch 1268 batch loss 1.33491278 epoch total loss 1.36503828\n",
      "Trained batch 1269 batch loss 1.400581 epoch total loss 1.36506629\n",
      "Trained batch 1270 batch loss 1.39280319 epoch total loss 1.36508822\n",
      "Trained batch 1271 batch loss 1.58092427 epoch total loss 1.36525798\n",
      "Trained batch 1272 batch loss 1.42733204 epoch total loss 1.36530685\n",
      "Trained batch 1273 batch loss 1.44476271 epoch total loss 1.3653692\n",
      "Trained batch 1274 batch loss 1.39818895 epoch total loss 1.36539495\n",
      "Trained batch 1275 batch loss 1.40323448 epoch total loss 1.36542463\n",
      "Trained batch 1276 batch loss 1.51519632 epoch total loss 1.36554193\n",
      "Trained batch 1277 batch loss 1.51340938 epoch total loss 1.36565781\n",
      "Trained batch 1278 batch loss 1.36299729 epoch total loss 1.36565566\n",
      "Trained batch 1279 batch loss 1.24815452 epoch total loss 1.36556387\n",
      "Trained batch 1280 batch loss 1.49844122 epoch total loss 1.36566758\n",
      "Trained batch 1281 batch loss 1.43158913 epoch total loss 1.36571908\n",
      "Trained batch 1282 batch loss 1.40874982 epoch total loss 1.3657527\n",
      "Trained batch 1283 batch loss 1.46997678 epoch total loss 1.36583388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1284 batch loss 1.50139451 epoch total loss 1.36593938\n",
      "Trained batch 1285 batch loss 1.3739171 epoch total loss 1.36594558\n",
      "Trained batch 1286 batch loss 1.31893945 epoch total loss 1.3659091\n",
      "Trained batch 1287 batch loss 1.3335104 epoch total loss 1.36588395\n",
      "Trained batch 1288 batch loss 1.4429543 epoch total loss 1.36594379\n",
      "Trained batch 1289 batch loss 1.26877522 epoch total loss 1.36586845\n",
      "Trained batch 1290 batch loss 1.30256963 epoch total loss 1.36581933\n",
      "Trained batch 1291 batch loss 1.36105537 epoch total loss 1.36581564\n",
      "Trained batch 1292 batch loss 1.46825337 epoch total loss 1.36589503\n",
      "Trained batch 1293 batch loss 1.4281143 epoch total loss 1.36594307\n",
      "Trained batch 1294 batch loss 1.40670204 epoch total loss 1.36597466\n",
      "Trained batch 1295 batch loss 1.3502512 epoch total loss 1.36596251\n",
      "Trained batch 1296 batch loss 1.2047596 epoch total loss 1.36583805\n",
      "Trained batch 1297 batch loss 1.41079319 epoch total loss 1.36587262\n",
      "Trained batch 1298 batch loss 1.24194896 epoch total loss 1.36577725\n",
      "Trained batch 1299 batch loss 1.31018448 epoch total loss 1.36573446\n",
      "Trained batch 1300 batch loss 1.28293848 epoch total loss 1.36567068\n",
      "Trained batch 1301 batch loss 1.28365791 epoch total loss 1.36560774\n",
      "Trained batch 1302 batch loss 1.30091453 epoch total loss 1.36555803\n",
      "Trained batch 1303 batch loss 1.19254422 epoch total loss 1.36542523\n",
      "Trained batch 1304 batch loss 1.24808717 epoch total loss 1.36533523\n",
      "Trained batch 1305 batch loss 1.24072099 epoch total loss 1.36523974\n",
      "Trained batch 1306 batch loss 1.44586515 epoch total loss 1.36530149\n",
      "Trained batch 1307 batch loss 1.28493547 epoch total loss 1.36524\n",
      "Trained batch 1308 batch loss 1.35713685 epoch total loss 1.36523378\n",
      "Trained batch 1309 batch loss 1.48241687 epoch total loss 1.36532331\n",
      "Trained batch 1310 batch loss 1.23567271 epoch total loss 1.36522436\n",
      "Trained batch 1311 batch loss 1.43302798 epoch total loss 1.3652761\n",
      "Trained batch 1312 batch loss 1.37965655 epoch total loss 1.36528707\n",
      "Trained batch 1313 batch loss 1.27343178 epoch total loss 1.36521709\n",
      "Trained batch 1314 batch loss 1.3438549 epoch total loss 1.36520088\n",
      "Trained batch 1315 batch loss 1.40229833 epoch total loss 1.36522913\n",
      "Trained batch 1316 batch loss 1.43648219 epoch total loss 1.36528325\n",
      "Trained batch 1317 batch loss 1.28995323 epoch total loss 1.36522603\n",
      "Trained batch 1318 batch loss 1.35456169 epoch total loss 1.36521792\n",
      "Trained batch 1319 batch loss 1.39315176 epoch total loss 1.36523914\n",
      "Trained batch 1320 batch loss 1.56051588 epoch total loss 1.3653872\n",
      "Trained batch 1321 batch loss 1.39080262 epoch total loss 1.36540639\n",
      "Trained batch 1322 batch loss 1.37250972 epoch total loss 1.36541176\n",
      "Trained batch 1323 batch loss 1.35494041 epoch total loss 1.36540389\n",
      "Trained batch 1324 batch loss 1.26345348 epoch total loss 1.36532688\n",
      "Trained batch 1325 batch loss 1.38140011 epoch total loss 1.36533892\n",
      "Trained batch 1326 batch loss 1.52893758 epoch total loss 1.3654623\n",
      "Trained batch 1327 batch loss 1.47000504 epoch total loss 1.3655411\n",
      "Trained batch 1328 batch loss 1.41576076 epoch total loss 1.36557889\n",
      "Trained batch 1329 batch loss 1.3142674 epoch total loss 1.36554027\n",
      "Trained batch 1330 batch loss 1.38217914 epoch total loss 1.36555278\n",
      "Trained batch 1331 batch loss 1.19630349 epoch total loss 1.36542559\n",
      "Trained batch 1332 batch loss 1.32139993 epoch total loss 1.36539257\n",
      "Trained batch 1333 batch loss 1.33236074 epoch total loss 1.36536777\n",
      "Trained batch 1334 batch loss 1.26102567 epoch total loss 1.36528957\n",
      "Trained batch 1335 batch loss 1.24667859 epoch total loss 1.36520076\n",
      "Trained batch 1336 batch loss 1.20973682 epoch total loss 1.36508429\n",
      "Trained batch 1337 batch loss 1.22867286 epoch total loss 1.36498225\n",
      "Trained batch 1338 batch loss 1.3431356 epoch total loss 1.36496592\n",
      "Trained batch 1339 batch loss 1.34789741 epoch total loss 1.36495328\n",
      "Trained batch 1340 batch loss 1.57764792 epoch total loss 1.36511195\n",
      "Trained batch 1341 batch loss 1.62539446 epoch total loss 1.36530602\n",
      "Trained batch 1342 batch loss 1.36768985 epoch total loss 1.36530781\n",
      "Trained batch 1343 batch loss 1.45690084 epoch total loss 1.365376\n",
      "Trained batch 1344 batch loss 1.50457144 epoch total loss 1.36547947\n",
      "Trained batch 1345 batch loss 1.41983819 epoch total loss 1.36551988\n",
      "Trained batch 1346 batch loss 1.37183547 epoch total loss 1.36552453\n",
      "Trained batch 1347 batch loss 1.31274962 epoch total loss 1.36548543\n",
      "Trained batch 1348 batch loss 1.33913469 epoch total loss 1.36546588\n",
      "Trained batch 1349 batch loss 1.2697866 epoch total loss 1.36539495\n",
      "Trained batch 1350 batch loss 1.31282544 epoch total loss 1.36535597\n",
      "Trained batch 1351 batch loss 1.23281217 epoch total loss 1.36525786\n",
      "Trained batch 1352 batch loss 1.30428457 epoch total loss 1.3652128\n",
      "Trained batch 1353 batch loss 1.31030154 epoch total loss 1.36517227\n",
      "Trained batch 1354 batch loss 1.33319414 epoch total loss 1.36514866\n",
      "Trained batch 1355 batch loss 1.36217773 epoch total loss 1.3651464\n",
      "Trained batch 1356 batch loss 1.25441945 epoch total loss 1.36506474\n",
      "Trained batch 1357 batch loss 1.20597959 epoch total loss 1.36494756\n",
      "Trained batch 1358 batch loss 1.36490262 epoch total loss 1.36494744\n",
      "Trained batch 1359 batch loss 1.35465515 epoch total loss 1.36493981\n",
      "Trained batch 1360 batch loss 1.35208035 epoch total loss 1.36493039\n",
      "Trained batch 1361 batch loss 1.36041176 epoch total loss 1.36492705\n",
      "Trained batch 1362 batch loss 1.33829236 epoch total loss 1.36490738\n",
      "Trained batch 1363 batch loss 1.24026537 epoch total loss 1.36481595\n",
      "Trained batch 1364 batch loss 1.19751692 epoch total loss 1.36469328\n",
      "Trained batch 1365 batch loss 1.30821085 epoch total loss 1.36465192\n",
      "Trained batch 1366 batch loss 1.21160436 epoch total loss 1.36453986\n",
      "Trained batch 1367 batch loss 1.31959426 epoch total loss 1.36450696\n",
      "Trained batch 1368 batch loss 1.34718204 epoch total loss 1.36449432\n",
      "Trained batch 1369 batch loss 1.41333354 epoch total loss 1.36453\n",
      "Trained batch 1370 batch loss 1.39245355 epoch total loss 1.36455035\n",
      "Trained batch 1371 batch loss 1.47115946 epoch total loss 1.36462808\n",
      "Trained batch 1372 batch loss 1.33376575 epoch total loss 1.36460567\n",
      "Trained batch 1373 batch loss 1.37471271 epoch total loss 1.36461306\n",
      "Trained batch 1374 batch loss 1.33558011 epoch total loss 1.36459184\n",
      "Trained batch 1375 batch loss 1.31010962 epoch total loss 1.36455226\n",
      "Trained batch 1376 batch loss 1.43940353 epoch total loss 1.36460662\n",
      "Trained batch 1377 batch loss 1.4572655 epoch total loss 1.36467397\n",
      "Trained batch 1378 batch loss 1.39442396 epoch total loss 1.36469555\n",
      "Trained batch 1379 batch loss 1.2911998 epoch total loss 1.36464226\n",
      "Trained batch 1380 batch loss 1.29453731 epoch total loss 1.36459148\n",
      "Trained batch 1381 batch loss 1.3809346 epoch total loss 1.36460328\n",
      "Trained batch 1382 batch loss 1.38225055 epoch total loss 1.36461604\n",
      "Trained batch 1383 batch loss 1.35279262 epoch total loss 1.36460757\n",
      "Trained batch 1384 batch loss 1.46808302 epoch total loss 1.36468232\n",
      "Trained batch 1385 batch loss 1.42605114 epoch total loss 1.36472666\n",
      "Trained batch 1386 batch loss 1.22476244 epoch total loss 1.36462557\n",
      "Trained batch 1387 batch loss 1.25624514 epoch total loss 1.36454749\n",
      "Trained batch 1388 batch loss 1.18970382 epoch total loss 1.36442149\n",
      "Epoch 2 train loss 1.3644214868545532\n",
      "Validated batch 1 batch loss 1.35625505\n",
      "Validated batch 2 batch loss 1.29116738\n",
      "Validated batch 3 batch loss 1.29866457\n",
      "Validated batch 4 batch loss 1.25024509\n",
      "Validated batch 5 batch loss 1.3829838\n",
      "Validated batch 6 batch loss 1.43830025\n",
      "Validated batch 7 batch loss 1.25252807\n",
      "Validated batch 8 batch loss 1.35624588\n",
      "Validated batch 9 batch loss 1.32886863\n",
      "Validated batch 10 batch loss 1.31546581\n",
      "Validated batch 11 batch loss 1.33641839\n",
      "Validated batch 12 batch loss 1.17754602\n",
      "Validated batch 13 batch loss 1.47140312\n",
      "Validated batch 14 batch loss 1.23822594\n",
      "Validated batch 15 batch loss 1.35958695\n",
      "Validated batch 16 batch loss 1.36521304\n",
      "Validated batch 17 batch loss 1.382002\n",
      "Validated batch 18 batch loss 1.16253328\n",
      "Validated batch 19 batch loss 1.31845903\n",
      "Validated batch 20 batch loss 1.25246155\n",
      "Validated batch 21 batch loss 1.31787968\n",
      "Validated batch 22 batch loss 1.32229447\n",
      "Validated batch 23 batch loss 1.32675505\n",
      "Validated batch 24 batch loss 1.36612034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 25 batch loss 1.33047152\n",
      "Validated batch 26 batch loss 1.25005651\n",
      "Validated batch 27 batch loss 1.21643567\n",
      "Validated batch 28 batch loss 1.25571036\n",
      "Validated batch 29 batch loss 1.35683489\n",
      "Validated batch 30 batch loss 1.25057197\n",
      "Validated batch 31 batch loss 1.26735\n",
      "Validated batch 32 batch loss 1.33514607\n",
      "Validated batch 33 batch loss 1.35452116\n",
      "Validated batch 34 batch loss 1.21123219\n",
      "Validated batch 35 batch loss 1.20210505\n",
      "Validated batch 36 batch loss 1.33951235\n",
      "Validated batch 37 batch loss 1.36104774\n",
      "Validated batch 38 batch loss 1.45167601\n",
      "Validated batch 39 batch loss 1.39937854\n",
      "Validated batch 40 batch loss 1.27821207\n",
      "Validated batch 41 batch loss 1.44847381\n",
      "Validated batch 42 batch loss 1.29304457\n",
      "Validated batch 43 batch loss 1.32366109\n",
      "Validated batch 44 batch loss 1.3431412\n",
      "Validated batch 45 batch loss 1.0927825\n",
      "Validated batch 46 batch loss 1.32158279\n",
      "Validated batch 47 batch loss 1.39685202\n",
      "Validated batch 48 batch loss 1.3181622\n",
      "Validated batch 49 batch loss 1.30468905\n",
      "Validated batch 50 batch loss 1.28799915\n",
      "Validated batch 51 batch loss 1.24353552\n",
      "Validated batch 52 batch loss 1.32574213\n",
      "Validated batch 53 batch loss 1.37422228\n",
      "Validated batch 54 batch loss 1.20464087\n",
      "Validated batch 55 batch loss 1.32339704\n",
      "Validated batch 56 batch loss 1.29877305\n",
      "Validated batch 57 batch loss 1.32935619\n",
      "Validated batch 58 batch loss 1.34887874\n",
      "Validated batch 59 batch loss 1.31937528\n",
      "Validated batch 60 batch loss 1.24550831\n",
      "Validated batch 61 batch loss 1.2653209\n",
      "Validated batch 62 batch loss 1.31070209\n",
      "Validated batch 63 batch loss 1.32396603\n",
      "Validated batch 64 batch loss 1.34948969\n",
      "Validated batch 65 batch loss 1.40176725\n",
      "Validated batch 66 batch loss 1.59235215\n",
      "Validated batch 67 batch loss 1.39816785\n",
      "Validated batch 68 batch loss 1.33479893\n",
      "Validated batch 69 batch loss 1.19177842\n",
      "Validated batch 70 batch loss 1.23729026\n",
      "Validated batch 71 batch loss 1.22209942\n",
      "Validated batch 72 batch loss 1.2799201\n",
      "Validated batch 73 batch loss 1.2242949\n",
      "Validated batch 74 batch loss 1.27261949\n",
      "Validated batch 75 batch loss 1.3384943\n",
      "Validated batch 76 batch loss 1.28918266\n",
      "Validated batch 77 batch loss 1.42005396\n",
      "Validated batch 78 batch loss 1.35653043\n",
      "Validated batch 79 batch loss 1.34328759\n",
      "Validated batch 80 batch loss 1.28264427\n",
      "Validated batch 81 batch loss 1.41164422\n",
      "Validated batch 82 batch loss 1.35554624\n",
      "Validated batch 83 batch loss 1.39267302\n",
      "Validated batch 84 batch loss 1.38328385\n",
      "Validated batch 85 batch loss 1.38230586\n",
      "Validated batch 86 batch loss 1.3660512\n",
      "Validated batch 87 batch loss 1.2115736\n",
      "Validated batch 88 batch loss 1.27022099\n",
      "Validated batch 89 batch loss 1.36096621\n",
      "Validated batch 90 batch loss 1.37418795\n",
      "Validated batch 91 batch loss 1.28136075\n",
      "Validated batch 92 batch loss 1.29218042\n",
      "Validated batch 93 batch loss 1.21916854\n",
      "Validated batch 94 batch loss 1.31530964\n",
      "Validated batch 95 batch loss 1.39191747\n",
      "Validated batch 96 batch loss 1.25606537\n",
      "Validated batch 97 batch loss 1.3684926\n",
      "Validated batch 98 batch loss 1.43255985\n",
      "Validated batch 99 batch loss 1.19830537\n",
      "Validated batch 100 batch loss 1.33920932\n",
      "Validated batch 101 batch loss 1.29212368\n",
      "Validated batch 102 batch loss 1.36726749\n",
      "Validated batch 103 batch loss 1.3644737\n",
      "Validated batch 104 batch loss 1.22977257\n",
      "Validated batch 105 batch loss 1.11207068\n",
      "Validated batch 106 batch loss 1.30678272\n",
      "Validated batch 107 batch loss 1.30260527\n",
      "Validated batch 108 batch loss 1.30598474\n",
      "Validated batch 109 batch loss 1.30398476\n",
      "Validated batch 110 batch loss 1.22033286\n",
      "Validated batch 111 batch loss 1.33652186\n",
      "Validated batch 112 batch loss 1.40116227\n",
      "Validated batch 113 batch loss 1.34493184\n",
      "Validated batch 114 batch loss 1.34082878\n",
      "Validated batch 115 batch loss 1.19224834\n",
      "Validated batch 116 batch loss 1.28903401\n",
      "Validated batch 117 batch loss 1.18964362\n",
      "Validated batch 118 batch loss 1.29367828\n",
      "Validated batch 119 batch loss 1.2317946\n",
      "Validated batch 120 batch loss 1.24318671\n",
      "Validated batch 121 batch loss 1.29458117\n",
      "Validated batch 122 batch loss 1.29263234\n",
      "Validated batch 123 batch loss 1.30310881\n",
      "Validated batch 124 batch loss 1.35218906\n",
      "Validated batch 125 batch loss 1.23878407\n",
      "Validated batch 126 batch loss 1.39908576\n",
      "Validated batch 127 batch loss 1.35708499\n",
      "Validated batch 128 batch loss 1.20858467\n",
      "Validated batch 129 batch loss 1.29824007\n",
      "Validated batch 130 batch loss 1.29184604\n",
      "Validated batch 131 batch loss 1.29663742\n",
      "Validated batch 132 batch loss 1.42487502\n",
      "Validated batch 133 batch loss 1.32290983\n",
      "Validated batch 134 batch loss 1.2922132\n",
      "Validated batch 135 batch loss 1.30166674\n",
      "Validated batch 136 batch loss 1.28464222\n",
      "Validated batch 137 batch loss 1.29399681\n",
      "Validated batch 138 batch loss 1.27923608\n",
      "Validated batch 139 batch loss 1.29740989\n",
      "Validated batch 140 batch loss 1.35896742\n",
      "Validated batch 141 batch loss 1.29864621\n",
      "Validated batch 142 batch loss 1.1510371\n",
      "Validated batch 143 batch loss 1.24260116\n",
      "Validated batch 144 batch loss 1.4045012\n",
      "Validated batch 145 batch loss 1.19082665\n",
      "Validated batch 146 batch loss 1.16584146\n",
      "Validated batch 147 batch loss 1.27807701\n",
      "Validated batch 148 batch loss 1.30062366\n",
      "Validated batch 149 batch loss 1.19871557\n",
      "Validated batch 150 batch loss 1.3327558\n",
      "Validated batch 151 batch loss 1.21358955\n",
      "Validated batch 152 batch loss 1.31556869\n",
      "Validated batch 153 batch loss 1.37558722\n",
      "Validated batch 154 batch loss 1.41265321\n",
      "Validated batch 155 batch loss 1.24715269\n",
      "Validated batch 156 batch loss 1.41514373\n",
      "Validated batch 157 batch loss 1.17187035\n",
      "Validated batch 158 batch loss 1.19363379\n",
      "Validated batch 159 batch loss 1.27310133\n",
      "Validated batch 160 batch loss 1.26338565\n",
      "Validated batch 161 batch loss 1.37817276\n",
      "Validated batch 162 batch loss 1.32246923\n",
      "Validated batch 163 batch loss 1.35443091\n",
      "Validated batch 164 batch loss 1.32374167\n",
      "Validated batch 165 batch loss 1.25139332\n",
      "Validated batch 166 batch loss 1.34347343\n",
      "Validated batch 167 batch loss 1.33420944\n",
      "Validated batch 168 batch loss 1.37869155\n",
      "Validated batch 169 batch loss 1.4166137\n",
      "Validated batch 170 batch loss 1.355129\n",
      "Validated batch 171 batch loss 1.25590515\n",
      "Validated batch 172 batch loss 1.3340925\n",
      "Validated batch 173 batch loss 1.35341454\n",
      "Validated batch 174 batch loss 1.2654649\n",
      "Validated batch 175 batch loss 1.38629055\n",
      "Validated batch 176 batch loss 1.42969322\n",
      "Validated batch 177 batch loss 1.31600022\n",
      "Validated batch 178 batch loss 1.40273249\n",
      "Validated batch 179 batch loss 1.29719508\n",
      "Validated batch 180 batch loss 1.21141708\n",
      "Validated batch 181 batch loss 1.2829659\n",
      "Validated batch 182 batch loss 1.20402861\n",
      "Validated batch 183 batch loss 1.4125191\n",
      "Validated batch 184 batch loss 1.24511921\n",
      "Validated batch 185 batch loss 1.31473792\n",
      "Epoch 2 val loss 1.309090495109558\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-2-loss-1.3091.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.27200854 epoch total loss 1.27200854\n",
      "Trained batch 2 batch loss 1.32142293 epoch total loss 1.29671574\n",
      "Trained batch 3 batch loss 1.40062141 epoch total loss 1.33135092\n",
      "Trained batch 4 batch loss 1.53489769 epoch total loss 1.38223767\n",
      "Trained batch 5 batch loss 1.50028598 epoch total loss 1.40584731\n",
      "Trained batch 6 batch loss 1.40079927 epoch total loss 1.40500605\n",
      "Trained batch 7 batch loss 1.37066722 epoch total loss 1.40010059\n",
      "Trained batch 8 batch loss 1.29790163 epoch total loss 1.38732576\n",
      "Trained batch 9 batch loss 1.22797883 epoch total loss 1.36962056\n",
      "Trained batch 10 batch loss 1.21745014 epoch total loss 1.3544035\n",
      "Trained batch 11 batch loss 1.33564949 epoch total loss 1.35269856\n",
      "Trained batch 12 batch loss 1.3625263 epoch total loss 1.35351753\n",
      "Trained batch 13 batch loss 1.39357972 epoch total loss 1.35659921\n",
      "Trained batch 14 batch loss 1.2528671 epoch total loss 1.34918976\n",
      "Trained batch 15 batch loss 1.20350409 epoch total loss 1.33947742\n",
      "Trained batch 16 batch loss 1.24231791 epoch total loss 1.3334049\n",
      "Trained batch 17 batch loss 1.08779275 epoch total loss 1.31895721\n",
      "Trained batch 18 batch loss 1.23881006 epoch total loss 1.3145045\n",
      "Trained batch 19 batch loss 1.24735928 epoch total loss 1.31097054\n",
      "Trained batch 20 batch loss 1.17719448 epoch total loss 1.30428171\n",
      "Trained batch 21 batch loss 1.3215065 epoch total loss 1.30510199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 22 batch loss 1.26292062 epoch total loss 1.30318463\n",
      "Trained batch 23 batch loss 1.34710205 epoch total loss 1.305094\n",
      "Trained batch 24 batch loss 1.38000011 epoch total loss 1.30821514\n",
      "Trained batch 25 batch loss 1.43497252 epoch total loss 1.31328535\n",
      "Trained batch 26 batch loss 1.36431575 epoch total loss 1.31524801\n",
      "Trained batch 27 batch loss 1.38065636 epoch total loss 1.31767058\n",
      "Trained batch 28 batch loss 1.34510696 epoch total loss 1.31865048\n",
      "Trained batch 29 batch loss 1.3718369 epoch total loss 1.32048452\n",
      "Trained batch 30 batch loss 1.29275298 epoch total loss 1.31956017\n",
      "Trained batch 31 batch loss 1.3477596 epoch total loss 1.32046974\n",
      "Trained batch 32 batch loss 1.31345129 epoch total loss 1.32025039\n",
      "Trained batch 33 batch loss 1.35240841 epoch total loss 1.32122493\n",
      "Trained batch 34 batch loss 1.28134775 epoch total loss 1.32005215\n",
      "Trained batch 35 batch loss 1.23946595 epoch total loss 1.31774962\n",
      "Trained batch 36 batch loss 1.23436522 epoch total loss 1.31543338\n",
      "Trained batch 37 batch loss 1.30639911 epoch total loss 1.31518924\n",
      "Trained batch 38 batch loss 1.21883 epoch total loss 1.31265354\n",
      "Trained batch 39 batch loss 1.28479886 epoch total loss 1.31193924\n",
      "Trained batch 40 batch loss 1.35078597 epoch total loss 1.31291032\n",
      "Trained batch 41 batch loss 1.3012892 epoch total loss 1.31262696\n",
      "Trained batch 42 batch loss 1.36306655 epoch total loss 1.31382787\n",
      "Trained batch 43 batch loss 1.38772726 epoch total loss 1.31554639\n",
      "Trained batch 44 batch loss 1.31779945 epoch total loss 1.31559765\n",
      "Trained batch 45 batch loss 1.37416744 epoch total loss 1.31689918\n",
      "Trained batch 46 batch loss 1.41974616 epoch total loss 1.31913495\n",
      "Trained batch 47 batch loss 1.24058414 epoch total loss 1.31746376\n",
      "Trained batch 48 batch loss 1.2079767 epoch total loss 1.31518281\n",
      "Trained batch 49 batch loss 1.29666591 epoch total loss 1.31480491\n",
      "Trained batch 50 batch loss 1.3193773 epoch total loss 1.31489623\n",
      "Trained batch 51 batch loss 1.37244105 epoch total loss 1.31602466\n",
      "Trained batch 52 batch loss 1.2754854 epoch total loss 1.31524491\n",
      "Trained batch 53 batch loss 1.33996475 epoch total loss 1.31571138\n",
      "Trained batch 54 batch loss 1.38692904 epoch total loss 1.31703031\n",
      "Trained batch 55 batch loss 1.41599226 epoch total loss 1.31882966\n",
      "Trained batch 56 batch loss 1.39460802 epoch total loss 1.3201828\n",
      "Trained batch 57 batch loss 1.44132864 epoch total loss 1.32230818\n",
      "Trained batch 58 batch loss 1.31405413 epoch total loss 1.32216597\n",
      "Trained batch 59 batch loss 1.27415645 epoch total loss 1.32135212\n",
      "Trained batch 60 batch loss 1.32001173 epoch total loss 1.32132983\n",
      "Trained batch 61 batch loss 1.25769281 epoch total loss 1.32028663\n",
      "Trained batch 62 batch loss 1.23362541 epoch total loss 1.3188889\n",
      "Trained batch 63 batch loss 1.19745743 epoch total loss 1.31696141\n",
      "Trained batch 64 batch loss 1.22944212 epoch total loss 1.31559384\n",
      "Trained batch 65 batch loss 1.28998911 epoch total loss 1.31519985\n",
      "Trained batch 66 batch loss 1.19944024 epoch total loss 1.31344593\n",
      "Trained batch 67 batch loss 1.22411251 epoch total loss 1.31211257\n",
      "Trained batch 68 batch loss 1.17157578 epoch total loss 1.31004596\n",
      "Trained batch 69 batch loss 1.28051841 epoch total loss 1.309618\n",
      "Trained batch 70 batch loss 1.27655101 epoch total loss 1.30914557\n",
      "Trained batch 71 batch loss 1.28351212 epoch total loss 1.3087846\n",
      "Trained batch 72 batch loss 1.3276341 epoch total loss 1.30904639\n",
      "Trained batch 73 batch loss 1.40727854 epoch total loss 1.31039214\n",
      "Trained batch 74 batch loss 1.33431292 epoch total loss 1.31071532\n",
      "Trained batch 75 batch loss 1.31802642 epoch total loss 1.31081283\n",
      "Trained batch 76 batch loss 1.30273962 epoch total loss 1.31070662\n",
      "Trained batch 77 batch loss 1.2231555 epoch total loss 1.30956948\n",
      "Trained batch 78 batch loss 1.34675622 epoch total loss 1.31004632\n",
      "Trained batch 79 batch loss 1.33291888 epoch total loss 1.31033576\n",
      "Trained batch 80 batch loss 1.35430479 epoch total loss 1.31088531\n",
      "Trained batch 81 batch loss 1.33230877 epoch total loss 1.31114984\n",
      "Trained batch 82 batch loss 1.32766008 epoch total loss 1.31135118\n",
      "Trained batch 83 batch loss 1.35908806 epoch total loss 1.31192625\n",
      "Trained batch 84 batch loss 1.50565457 epoch total loss 1.31423247\n",
      "Trained batch 85 batch loss 1.34144282 epoch total loss 1.31455266\n",
      "Trained batch 86 batch loss 1.26657212 epoch total loss 1.31399477\n",
      "Trained batch 87 batch loss 1.29636467 epoch total loss 1.31379211\n",
      "Trained batch 88 batch loss 1.24694502 epoch total loss 1.31303251\n",
      "Trained batch 89 batch loss 1.23548222 epoch total loss 1.31216109\n",
      "Trained batch 90 batch loss 1.27785754 epoch total loss 1.31178\n",
      "Trained batch 91 batch loss 1.24896705 epoch total loss 1.31108975\n",
      "Trained batch 92 batch loss 1.25884783 epoch total loss 1.31052196\n",
      "Trained batch 93 batch loss 1.14277446 epoch total loss 1.3087182\n",
      "Trained batch 94 batch loss 1.2569263 epoch total loss 1.30816722\n",
      "Trained batch 95 batch loss 1.18590271 epoch total loss 1.30688024\n",
      "Trained batch 96 batch loss 1.2662816 epoch total loss 1.3064574\n",
      "Trained batch 97 batch loss 1.36241961 epoch total loss 1.30703425\n",
      "Trained batch 98 batch loss 1.2515682 epoch total loss 1.30646825\n",
      "Trained batch 99 batch loss 1.14464951 epoch total loss 1.30483377\n",
      "Trained batch 100 batch loss 1.23935246 epoch total loss 1.30417895\n",
      "Trained batch 101 batch loss 1.32263815 epoch total loss 1.30436158\n",
      "Trained batch 102 batch loss 1.236233 epoch total loss 1.30369377\n",
      "Trained batch 103 batch loss 1.32868147 epoch total loss 1.30393624\n",
      "Trained batch 104 batch loss 1.26430726 epoch total loss 1.30355525\n",
      "Trained batch 105 batch loss 1.20150101 epoch total loss 1.30258334\n",
      "Trained batch 106 batch loss 1.17662024 epoch total loss 1.30139506\n",
      "Trained batch 107 batch loss 1.26874 epoch total loss 1.30108988\n",
      "Trained batch 108 batch loss 1.15479136 epoch total loss 1.29973519\n",
      "Trained batch 109 batch loss 1.19364524 epoch total loss 1.29876196\n",
      "Trained batch 110 batch loss 1.18997788 epoch total loss 1.29777288\n",
      "Trained batch 111 batch loss 1.26032543 epoch total loss 1.29743564\n",
      "Trained batch 112 batch loss 1.32905054 epoch total loss 1.29771793\n",
      "Trained batch 113 batch loss 1.2385546 epoch total loss 1.29719436\n",
      "Trained batch 114 batch loss 1.38491082 epoch total loss 1.29796386\n",
      "Trained batch 115 batch loss 1.39036715 epoch total loss 1.29876733\n",
      "Trained batch 116 batch loss 1.42151618 epoch total loss 1.29982543\n",
      "Trained batch 117 batch loss 1.48817503 epoch total loss 1.30143523\n",
      "Trained batch 118 batch loss 1.43500853 epoch total loss 1.30256736\n",
      "Trained batch 119 batch loss 1.39956903 epoch total loss 1.3033824\n",
      "Trained batch 120 batch loss 1.21034491 epoch total loss 1.30260706\n",
      "Trained batch 121 batch loss 1.26816332 epoch total loss 1.30232239\n",
      "Trained batch 122 batch loss 1.37419319 epoch total loss 1.30291152\n",
      "Trained batch 123 batch loss 1.31893635 epoch total loss 1.30304182\n",
      "Trained batch 124 batch loss 1.2619257 epoch total loss 1.30271029\n",
      "Trained batch 125 batch loss 1.2725035 epoch total loss 1.30246866\n",
      "Trained batch 126 batch loss 1.20732236 epoch total loss 1.30171347\n",
      "Trained batch 127 batch loss 1.25347638 epoch total loss 1.30133367\n",
      "Trained batch 128 batch loss 1.24510312 epoch total loss 1.30089438\n",
      "Trained batch 129 batch loss 1.25748444 epoch total loss 1.30055797\n",
      "Trained batch 130 batch loss 1.31560421 epoch total loss 1.3006736\n",
      "Trained batch 131 batch loss 1.33935714 epoch total loss 1.30096889\n",
      "Trained batch 132 batch loss 1.35350645 epoch total loss 1.30136681\n",
      "Trained batch 133 batch loss 1.49305 epoch total loss 1.30280817\n",
      "Trained batch 134 batch loss 1.28097498 epoch total loss 1.30264521\n",
      "Trained batch 135 batch loss 1.36389029 epoch total loss 1.30309892\n",
      "Trained batch 136 batch loss 1.47125769 epoch total loss 1.30433536\n",
      "Trained batch 137 batch loss 1.27049983 epoch total loss 1.30408823\n",
      "Trained batch 138 batch loss 1.2556783 epoch total loss 1.30373752\n",
      "Trained batch 139 batch loss 1.21187973 epoch total loss 1.30307662\n",
      "Trained batch 140 batch loss 1.43403876 epoch total loss 1.30401206\n",
      "Trained batch 141 batch loss 1.40994036 epoch total loss 1.30476332\n",
      "Trained batch 142 batch loss 1.35630441 epoch total loss 1.30512631\n",
      "Trained batch 143 batch loss 1.28807139 epoch total loss 1.3050071\n",
      "Trained batch 144 batch loss 1.32919133 epoch total loss 1.30517507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 145 batch loss 1.31743264 epoch total loss 1.30525959\n",
      "Trained batch 146 batch loss 1.32276011 epoch total loss 1.30537939\n",
      "Trained batch 147 batch loss 1.40301263 epoch total loss 1.30604351\n",
      "Trained batch 148 batch loss 1.40306044 epoch total loss 1.30669904\n",
      "Trained batch 149 batch loss 1.38625145 epoch total loss 1.30723298\n",
      "Trained batch 150 batch loss 1.22258055 epoch total loss 1.30666864\n",
      "Trained batch 151 batch loss 1.22924852 epoch total loss 1.30615592\n",
      "Trained batch 152 batch loss 1.06064403 epoch total loss 1.30454063\n",
      "Trained batch 153 batch loss 1.32546329 epoch total loss 1.30467749\n",
      "Trained batch 154 batch loss 1.47020078 epoch total loss 1.30575228\n",
      "Trained batch 155 batch loss 1.28261149 epoch total loss 1.30560291\n",
      "Trained batch 156 batch loss 1.32744932 epoch total loss 1.30574298\n",
      "Trained batch 157 batch loss 1.32464862 epoch total loss 1.30586338\n",
      "Trained batch 158 batch loss 1.39990544 epoch total loss 1.30645859\n",
      "Trained batch 159 batch loss 1.38254917 epoch total loss 1.30693722\n",
      "Trained batch 160 batch loss 1.32475257 epoch total loss 1.30704856\n",
      "Trained batch 161 batch loss 1.3737129 epoch total loss 1.30746257\n",
      "Trained batch 162 batch loss 1.35772884 epoch total loss 1.30777287\n",
      "Trained batch 163 batch loss 1.29609156 epoch total loss 1.30770123\n",
      "Trained batch 164 batch loss 1.39052033 epoch total loss 1.3082062\n",
      "Trained batch 165 batch loss 1.32777607 epoch total loss 1.30832481\n",
      "Trained batch 166 batch loss 1.42379129 epoch total loss 1.3090204\n",
      "Trained batch 167 batch loss 1.23492789 epoch total loss 1.3085767\n",
      "Trained batch 168 batch loss 1.30296767 epoch total loss 1.30854332\n",
      "Trained batch 169 batch loss 1.32474911 epoch total loss 1.30863929\n",
      "Trained batch 170 batch loss 1.30772591 epoch total loss 1.30863392\n",
      "Trained batch 171 batch loss 1.36856806 epoch total loss 1.30898428\n",
      "Trained batch 172 batch loss 1.37059736 epoch total loss 1.3093425\n",
      "Trained batch 173 batch loss 1.34451485 epoch total loss 1.30954576\n",
      "Trained batch 174 batch loss 1.18393123 epoch total loss 1.30882382\n",
      "Trained batch 175 batch loss 1.37674165 epoch total loss 1.30921197\n",
      "Trained batch 176 batch loss 1.34226346 epoch total loss 1.30939972\n",
      "Trained batch 177 batch loss 1.30641603 epoch total loss 1.30938292\n",
      "Trained batch 178 batch loss 1.41596806 epoch total loss 1.3099817\n",
      "Trained batch 179 batch loss 1.32545936 epoch total loss 1.31006813\n",
      "Trained batch 180 batch loss 1.25222063 epoch total loss 1.30974686\n",
      "Trained batch 181 batch loss 1.22410464 epoch total loss 1.3092736\n",
      "Trained batch 182 batch loss 1.30123615 epoch total loss 1.30922949\n",
      "Trained batch 183 batch loss 1.18384087 epoch total loss 1.30854428\n",
      "Trained batch 184 batch loss 1.30253553 epoch total loss 1.30851161\n",
      "Trained batch 185 batch loss 1.19202077 epoch total loss 1.30788195\n",
      "Trained batch 186 batch loss 1.36434925 epoch total loss 1.30818558\n",
      "Trained batch 187 batch loss 1.44503462 epoch total loss 1.3089174\n",
      "Trained batch 188 batch loss 1.56967354 epoch total loss 1.3103044\n",
      "Trained batch 189 batch loss 1.4408859 epoch total loss 1.31099522\n",
      "Trained batch 190 batch loss 1.43944335 epoch total loss 1.31167126\n",
      "Trained batch 191 batch loss 1.31622708 epoch total loss 1.3116951\n",
      "Trained batch 192 batch loss 1.13775194 epoch total loss 1.31078923\n",
      "Trained batch 193 batch loss 1.12048447 epoch total loss 1.30980313\n",
      "Trained batch 194 batch loss 1.0659616 epoch total loss 1.30854619\n",
      "Trained batch 195 batch loss 1.12926888 epoch total loss 1.30762684\n",
      "Trained batch 196 batch loss 1.26026404 epoch total loss 1.30738521\n",
      "Trained batch 197 batch loss 1.236673 epoch total loss 1.30702615\n",
      "Trained batch 198 batch loss 1.28210974 epoch total loss 1.30690038\n",
      "Trained batch 199 batch loss 1.29666805 epoch total loss 1.30684888\n",
      "Trained batch 200 batch loss 1.46632469 epoch total loss 1.30764627\n",
      "Trained batch 201 batch loss 1.39465594 epoch total loss 1.30807924\n",
      "Trained batch 202 batch loss 1.40873563 epoch total loss 1.30857742\n",
      "Trained batch 203 batch loss 1.27639055 epoch total loss 1.30841887\n",
      "Trained batch 204 batch loss 1.28248858 epoch total loss 1.30829191\n",
      "Trained batch 205 batch loss 1.26914394 epoch total loss 1.30810082\n",
      "Trained batch 206 batch loss 1.39089918 epoch total loss 1.30850279\n",
      "Trained batch 207 batch loss 1.36302757 epoch total loss 1.30876625\n",
      "Trained batch 208 batch loss 1.50489235 epoch total loss 1.30970907\n",
      "Trained batch 209 batch loss 1.45746124 epoch total loss 1.3104161\n",
      "Trained batch 210 batch loss 1.33341825 epoch total loss 1.31052554\n",
      "Trained batch 211 batch loss 1.4144882 epoch total loss 1.31101823\n",
      "Trained batch 212 batch loss 1.34684539 epoch total loss 1.31118715\n",
      "Trained batch 213 batch loss 1.44801593 epoch total loss 1.31182957\n",
      "Trained batch 214 batch loss 1.47592878 epoch total loss 1.31259644\n",
      "Trained batch 215 batch loss 1.34626913 epoch total loss 1.31275308\n",
      "Trained batch 216 batch loss 1.22545075 epoch total loss 1.31234896\n",
      "Trained batch 217 batch loss 1.16817021 epoch total loss 1.31168461\n",
      "Trained batch 218 batch loss 1.29013085 epoch total loss 1.31158578\n",
      "Trained batch 219 batch loss 1.42297029 epoch total loss 1.31209433\n",
      "Trained batch 220 batch loss 1.40002036 epoch total loss 1.31249404\n",
      "Trained batch 221 batch loss 1.4386301 epoch total loss 1.31306481\n",
      "Trained batch 222 batch loss 1.45159054 epoch total loss 1.31368876\n",
      "Trained batch 223 batch loss 1.42920554 epoch total loss 1.31420684\n",
      "Trained batch 224 batch loss 1.30686772 epoch total loss 1.31417394\n",
      "Trained batch 225 batch loss 1.33684456 epoch total loss 1.31427479\n",
      "Trained batch 226 batch loss 1.40760958 epoch total loss 1.31468785\n",
      "Trained batch 227 batch loss 1.30260408 epoch total loss 1.31463456\n",
      "Trained batch 228 batch loss 1.2211318 epoch total loss 1.31422448\n",
      "Trained batch 229 batch loss 1.35657847 epoch total loss 1.31440938\n",
      "Trained batch 230 batch loss 1.23795235 epoch total loss 1.31407702\n",
      "Trained batch 231 batch loss 1.22999835 epoch total loss 1.31371307\n",
      "Trained batch 232 batch loss 1.22760856 epoch total loss 1.31334186\n",
      "Trained batch 233 batch loss 1.13808656 epoch total loss 1.31258976\n",
      "Trained batch 234 batch loss 1.10351205 epoch total loss 1.31169629\n",
      "Trained batch 235 batch loss 1.31212139 epoch total loss 1.31169808\n",
      "Trained batch 236 batch loss 1.44511962 epoch total loss 1.31226349\n",
      "Trained batch 237 batch loss 1.46432734 epoch total loss 1.31290507\n",
      "Trained batch 238 batch loss 1.4154309 epoch total loss 1.3133359\n",
      "Trained batch 239 batch loss 1.29741979 epoch total loss 1.31326938\n",
      "Trained batch 240 batch loss 1.3625493 epoch total loss 1.31347466\n",
      "Trained batch 241 batch loss 1.27140391 epoch total loss 1.3133\n",
      "Trained batch 242 batch loss 1.19225812 epoch total loss 1.31279993\n",
      "Trained batch 243 batch loss 1.34866476 epoch total loss 1.31294751\n",
      "Trained batch 244 batch loss 1.41723633 epoch total loss 1.31337488\n",
      "Trained batch 245 batch loss 1.46156359 epoch total loss 1.31397974\n",
      "Trained batch 246 batch loss 1.54053223 epoch total loss 1.31490076\n",
      "Trained batch 247 batch loss 1.39072263 epoch total loss 1.31520772\n",
      "Trained batch 248 batch loss 1.33598852 epoch total loss 1.31529152\n",
      "Trained batch 249 batch loss 1.44922698 epoch total loss 1.3158294\n",
      "Trained batch 250 batch loss 1.32803261 epoch total loss 1.31587815\n",
      "Trained batch 251 batch loss 1.36666453 epoch total loss 1.31608057\n",
      "Trained batch 252 batch loss 1.41947424 epoch total loss 1.31649077\n",
      "Trained batch 253 batch loss 1.29970741 epoch total loss 1.31642449\n",
      "Trained batch 254 batch loss 1.30677199 epoch total loss 1.31638646\n",
      "Trained batch 255 batch loss 1.38737726 epoch total loss 1.31666481\n",
      "Trained batch 256 batch loss 1.27677667 epoch total loss 1.31650901\n",
      "Trained batch 257 batch loss 1.33179212 epoch total loss 1.31656849\n",
      "Trained batch 258 batch loss 1.2146244 epoch total loss 1.31617332\n",
      "Trained batch 259 batch loss 1.30565977 epoch total loss 1.31613278\n",
      "Trained batch 260 batch loss 1.31910956 epoch total loss 1.31614423\n",
      "Trained batch 261 batch loss 1.35155606 epoch total loss 1.31628\n",
      "Trained batch 262 batch loss 1.38300431 epoch total loss 1.31653464\n",
      "Trained batch 263 batch loss 1.26084065 epoch total loss 1.3163228\n",
      "Trained batch 264 batch loss 1.32918131 epoch total loss 1.31637156\n",
      "Trained batch 265 batch loss 1.39215648 epoch total loss 1.31665754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 266 batch loss 1.32311368 epoch total loss 1.31668186\n",
      "Trained batch 267 batch loss 1.31037021 epoch total loss 1.31665814\n",
      "Trained batch 268 batch loss 1.32760584 epoch total loss 1.31669903\n",
      "Trained batch 269 batch loss 1.35212934 epoch total loss 1.31683075\n",
      "Trained batch 270 batch loss 1.38330412 epoch total loss 1.31707692\n",
      "Trained batch 271 batch loss 1.30683923 epoch total loss 1.31703925\n",
      "Trained batch 272 batch loss 1.256428 epoch total loss 1.31681645\n",
      "Trained batch 273 batch loss 1.38101554 epoch total loss 1.31705165\n",
      "Trained batch 274 batch loss 1.22465587 epoch total loss 1.31671441\n",
      "Trained batch 275 batch loss 1.2701509 epoch total loss 1.31654513\n",
      "Trained batch 276 batch loss 1.20423698 epoch total loss 1.31613815\n",
      "Trained batch 277 batch loss 1.12320817 epoch total loss 1.31544161\n",
      "Trained batch 278 batch loss 1.1794219 epoch total loss 1.31495225\n",
      "Trained batch 279 batch loss 1.30870473 epoch total loss 1.31493\n",
      "Trained batch 280 batch loss 1.16760635 epoch total loss 1.31440377\n",
      "Trained batch 281 batch loss 1.16304529 epoch total loss 1.31386518\n",
      "Trained batch 282 batch loss 1.10045934 epoch total loss 1.31310844\n",
      "Trained batch 283 batch loss 1.01372254 epoch total loss 1.31205058\n",
      "Trained batch 284 batch loss 1.23490918 epoch total loss 1.31177902\n",
      "Trained batch 285 batch loss 1.40202272 epoch total loss 1.31209552\n",
      "Trained batch 286 batch loss 1.34747148 epoch total loss 1.31221926\n",
      "Trained batch 287 batch loss 1.39325368 epoch total loss 1.31250155\n",
      "Trained batch 288 batch loss 1.32765055 epoch total loss 1.31255412\n",
      "Trained batch 289 batch loss 1.45479202 epoch total loss 1.31304634\n",
      "Trained batch 290 batch loss 1.37848663 epoch total loss 1.313272\n",
      "Trained batch 291 batch loss 1.37470639 epoch total loss 1.31348312\n",
      "Trained batch 292 batch loss 1.35113454 epoch total loss 1.31361198\n",
      "Trained batch 293 batch loss 1.30734825 epoch total loss 1.31359065\n",
      "Trained batch 294 batch loss 1.35780573 epoch total loss 1.31374109\n",
      "Trained batch 295 batch loss 1.18982553 epoch total loss 1.31332099\n",
      "Trained batch 296 batch loss 1.22450328 epoch total loss 1.31302094\n",
      "Trained batch 297 batch loss 1.10868025 epoch total loss 1.31233287\n",
      "Trained batch 298 batch loss 1.24582028 epoch total loss 1.31210971\n",
      "Trained batch 299 batch loss 1.30781794 epoch total loss 1.3120954\n",
      "Trained batch 300 batch loss 1.23717582 epoch total loss 1.31184566\n",
      "Trained batch 301 batch loss 1.36322343 epoch total loss 1.31201637\n",
      "Trained batch 302 batch loss 1.47411299 epoch total loss 1.31255317\n",
      "Trained batch 303 batch loss 1.29645562 epoch total loss 1.3125\n",
      "Trained batch 304 batch loss 1.32650316 epoch total loss 1.31254613\n",
      "Trained batch 305 batch loss 1.30430079 epoch total loss 1.31251895\n",
      "Trained batch 306 batch loss 1.27100182 epoch total loss 1.31238329\n",
      "Trained batch 307 batch loss 1.37359416 epoch total loss 1.31258273\n",
      "Trained batch 308 batch loss 1.32350624 epoch total loss 1.31261826\n",
      "Trained batch 309 batch loss 1.3250761 epoch total loss 1.31265855\n",
      "Trained batch 310 batch loss 1.17217028 epoch total loss 1.31220531\n",
      "Trained batch 311 batch loss 1.21190643 epoch total loss 1.31188285\n",
      "Trained batch 312 batch loss 1.30567026 epoch total loss 1.31186295\n",
      "Trained batch 313 batch loss 1.13454103 epoch total loss 1.31129646\n",
      "Trained batch 314 batch loss 1.23742223 epoch total loss 1.31106114\n",
      "Trained batch 315 batch loss 1.16909218 epoch total loss 1.31061053\n",
      "Trained batch 316 batch loss 1.21751356 epoch total loss 1.31031585\n",
      "Trained batch 317 batch loss 1.22889853 epoch total loss 1.31005907\n",
      "Trained batch 318 batch loss 1.21114242 epoch total loss 1.30974805\n",
      "Trained batch 319 batch loss 1.30198336 epoch total loss 1.30972362\n",
      "Trained batch 320 batch loss 1.25666189 epoch total loss 1.3095578\n",
      "Trained batch 321 batch loss 1.2837379 epoch total loss 1.30947745\n",
      "Trained batch 322 batch loss 1.22889721 epoch total loss 1.30922723\n",
      "Trained batch 323 batch loss 1.3928659 epoch total loss 1.30948615\n",
      "Trained batch 324 batch loss 1.32066202 epoch total loss 1.3095206\n",
      "Trained batch 325 batch loss 1.30272269 epoch total loss 1.30949974\n",
      "Trained batch 326 batch loss 1.27372241 epoch total loss 1.30939\n",
      "Trained batch 327 batch loss 1.31802702 epoch total loss 1.30941629\n",
      "Trained batch 328 batch loss 1.31275415 epoch total loss 1.30942643\n",
      "Trained batch 329 batch loss 1.3889004 epoch total loss 1.30966794\n",
      "Trained batch 330 batch loss 1.47295642 epoch total loss 1.31016278\n",
      "Trained batch 331 batch loss 1.48089719 epoch total loss 1.3106786\n",
      "Trained batch 332 batch loss 1.57575524 epoch total loss 1.31147707\n",
      "Trained batch 333 batch loss 1.38487887 epoch total loss 1.31169748\n",
      "Trained batch 334 batch loss 1.12069559 epoch total loss 1.31112564\n",
      "Trained batch 335 batch loss 1.25399232 epoch total loss 1.31095505\n",
      "Trained batch 336 batch loss 1.2110312 epoch total loss 1.31065774\n",
      "Trained batch 337 batch loss 1.22237337 epoch total loss 1.31039572\n",
      "Trained batch 338 batch loss 1.21291685 epoch total loss 1.31010735\n",
      "Trained batch 339 batch loss 1.15676796 epoch total loss 1.30965507\n",
      "Trained batch 340 batch loss 1.27245462 epoch total loss 1.30954564\n",
      "Trained batch 341 batch loss 1.17315817 epoch total loss 1.30914569\n",
      "Trained batch 342 batch loss 1.14896965 epoch total loss 1.30867732\n",
      "Trained batch 343 batch loss 1.18406653 epoch total loss 1.30831397\n",
      "Trained batch 344 batch loss 1.33195651 epoch total loss 1.30838275\n",
      "Trained batch 345 batch loss 1.22961831 epoch total loss 1.30815434\n",
      "Trained batch 346 batch loss 1.11533082 epoch total loss 1.30759704\n",
      "Trained batch 347 batch loss 1.15325451 epoch total loss 1.30715227\n",
      "Trained batch 348 batch loss 1.46117914 epoch total loss 1.3075949\n",
      "Trained batch 349 batch loss 1.1807878 epoch total loss 1.30723155\n",
      "Trained batch 350 batch loss 1.21357656 epoch total loss 1.30696392\n",
      "Trained batch 351 batch loss 1.11327553 epoch total loss 1.3064121\n",
      "Trained batch 352 batch loss 1.11404252 epoch total loss 1.30586565\n",
      "Trained batch 353 batch loss 1.27948785 epoch total loss 1.3057909\n",
      "Trained batch 354 batch loss 1.34386432 epoch total loss 1.30589843\n",
      "Trained batch 355 batch loss 1.51372313 epoch total loss 1.30648386\n",
      "Trained batch 356 batch loss 1.32274485 epoch total loss 1.30652964\n",
      "Trained batch 357 batch loss 1.33971226 epoch total loss 1.30662262\n",
      "Trained batch 358 batch loss 1.15510285 epoch total loss 1.30619931\n",
      "Trained batch 359 batch loss 1.39619231 epoch total loss 1.30645\n",
      "Trained batch 360 batch loss 1.34952497 epoch total loss 1.30656958\n",
      "Trained batch 361 batch loss 1.3423965 epoch total loss 1.30666888\n",
      "Trained batch 362 batch loss 1.28785825 epoch total loss 1.3066169\n",
      "Trained batch 363 batch loss 1.51714277 epoch total loss 1.30719697\n",
      "Trained batch 364 batch loss 1.51624858 epoch total loss 1.30777121\n",
      "Trained batch 365 batch loss 1.38182902 epoch total loss 1.3079741\n",
      "Trained batch 366 batch loss 1.297575 epoch total loss 1.30794573\n",
      "Trained batch 367 batch loss 1.23779809 epoch total loss 1.30775452\n",
      "Trained batch 368 batch loss 1.23696709 epoch total loss 1.30756223\n",
      "Trained batch 369 batch loss 1.254691 epoch total loss 1.30741894\n",
      "Trained batch 370 batch loss 1.26585674 epoch total loss 1.30730665\n",
      "Trained batch 371 batch loss 1.33600008 epoch total loss 1.30738401\n",
      "Trained batch 372 batch loss 1.21468449 epoch total loss 1.30713487\n",
      "Trained batch 373 batch loss 1.28400886 epoch total loss 1.30707276\n",
      "Trained batch 374 batch loss 1.21001673 epoch total loss 1.30681324\n",
      "Trained batch 375 batch loss 1.37301731 epoch total loss 1.30698979\n",
      "Trained batch 376 batch loss 1.23222554 epoch total loss 1.30679107\n",
      "Trained batch 377 batch loss 1.23624468 epoch total loss 1.30660391\n",
      "Trained batch 378 batch loss 1.04375863 epoch total loss 1.30590856\n",
      "Trained batch 379 batch loss 0.98702687 epoch total loss 1.30506718\n",
      "Trained batch 380 batch loss 1.24293745 epoch total loss 1.30490375\n",
      "Trained batch 381 batch loss 1.4080404 epoch total loss 1.30517447\n",
      "Trained batch 382 batch loss 1.3195287 epoch total loss 1.30521202\n",
      "Trained batch 383 batch loss 1.36463618 epoch total loss 1.30536711\n",
      "Trained batch 384 batch loss 1.34773219 epoch total loss 1.30547738\n",
      "Trained batch 385 batch loss 1.22552931 epoch total loss 1.30526972\n",
      "Trained batch 386 batch loss 1.20342135 epoch total loss 1.30500591\n",
      "Trained batch 387 batch loss 1.18736351 epoch total loss 1.30470192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 388 batch loss 1.28035438 epoch total loss 1.30463922\n",
      "Trained batch 389 batch loss 1.35160804 epoch total loss 1.30475986\n",
      "Trained batch 390 batch loss 1.2480303 epoch total loss 1.30461442\n",
      "Trained batch 391 batch loss 1.24505162 epoch total loss 1.30446208\n",
      "Trained batch 392 batch loss 1.2038604 epoch total loss 1.30420542\n",
      "Trained batch 393 batch loss 1.16889548 epoch total loss 1.30386114\n",
      "Trained batch 394 batch loss 1.11979747 epoch total loss 1.30339396\n",
      "Trained batch 395 batch loss 1.179317 epoch total loss 1.30307984\n",
      "Trained batch 396 batch loss 1.23041427 epoch total loss 1.30289638\n",
      "Trained batch 397 batch loss 1.28082705 epoch total loss 1.30284071\n",
      "Trained batch 398 batch loss 1.35594165 epoch total loss 1.30297422\n",
      "Trained batch 399 batch loss 1.29180646 epoch total loss 1.30294621\n",
      "Trained batch 400 batch loss 1.41698742 epoch total loss 1.30323136\n",
      "Trained batch 401 batch loss 1.23628 epoch total loss 1.30306435\n",
      "Trained batch 402 batch loss 1.24850869 epoch total loss 1.30292869\n",
      "Trained batch 403 batch loss 1.25146854 epoch total loss 1.30280101\n",
      "Trained batch 404 batch loss 1.2692225 epoch total loss 1.30271792\n",
      "Trained batch 405 batch loss 1.17672694 epoch total loss 1.30240679\n",
      "Trained batch 406 batch loss 1.07762218 epoch total loss 1.30185318\n",
      "Trained batch 407 batch loss 1.0402683 epoch total loss 1.3012104\n",
      "Trained batch 408 batch loss 1.24065971 epoch total loss 1.30106199\n",
      "Trained batch 409 batch loss 1.29691851 epoch total loss 1.30105197\n",
      "Trained batch 410 batch loss 1.54963923 epoch total loss 1.30165827\n",
      "Trained batch 411 batch loss 1.44748819 epoch total loss 1.30201304\n",
      "Trained batch 412 batch loss 1.33469296 epoch total loss 1.30209243\n",
      "Trained batch 413 batch loss 1.4628104 epoch total loss 1.30248165\n",
      "Trained batch 414 batch loss 1.39685106 epoch total loss 1.30270958\n",
      "Trained batch 415 batch loss 1.34928739 epoch total loss 1.30282187\n",
      "Trained batch 416 batch loss 1.45352077 epoch total loss 1.30318403\n",
      "Trained batch 417 batch loss 1.39195323 epoch total loss 1.30339694\n",
      "Trained batch 418 batch loss 1.38263261 epoch total loss 1.30358648\n",
      "Trained batch 419 batch loss 1.26621366 epoch total loss 1.30349743\n",
      "Trained batch 420 batch loss 1.29622328 epoch total loss 1.30348\n",
      "Trained batch 421 batch loss 1.11231375 epoch total loss 1.30302596\n",
      "Trained batch 422 batch loss 1.15830827 epoch total loss 1.302683\n",
      "Trained batch 423 batch loss 1.15796244 epoch total loss 1.30234087\n",
      "Trained batch 424 batch loss 1.20957053 epoch total loss 1.30212212\n",
      "Trained batch 425 batch loss 1.11713564 epoch total loss 1.30168688\n",
      "Trained batch 426 batch loss 1.01041043 epoch total loss 1.30100322\n",
      "Trained batch 427 batch loss 0.988725126 epoch total loss 1.30027187\n",
      "Trained batch 428 batch loss 1.01217067 epoch total loss 1.29959857\n",
      "Trained batch 429 batch loss 1.16040635 epoch total loss 1.29927421\n",
      "Trained batch 430 batch loss 1.26681662 epoch total loss 1.29919875\n",
      "Trained batch 431 batch loss 1.24469471 epoch total loss 1.29907227\n",
      "Trained batch 432 batch loss 1.30553865 epoch total loss 1.29908729\n",
      "Trained batch 433 batch loss 1.25314558 epoch total loss 1.29898119\n",
      "Trained batch 434 batch loss 1.28286076 epoch total loss 1.298944\n",
      "Trained batch 435 batch loss 1.41627502 epoch total loss 1.29921365\n",
      "Trained batch 436 batch loss 1.3940233 epoch total loss 1.2994312\n",
      "Trained batch 437 batch loss 1.31549025 epoch total loss 1.29946792\n",
      "Trained batch 438 batch loss 1.37902462 epoch total loss 1.2996496\n",
      "Trained batch 439 batch loss 1.43672347 epoch total loss 1.29996181\n",
      "Trained batch 440 batch loss 1.28281784 epoch total loss 1.29992282\n",
      "Trained batch 441 batch loss 1.34808564 epoch total loss 1.30003214\n",
      "Trained batch 442 batch loss 1.54570198 epoch total loss 1.30058789\n",
      "Trained batch 443 batch loss 1.3893075 epoch total loss 1.30078816\n",
      "Trained batch 444 batch loss 1.33242214 epoch total loss 1.30085933\n",
      "Trained batch 445 batch loss 1.25941157 epoch total loss 1.30076611\n",
      "Trained batch 446 batch loss 1.22868419 epoch total loss 1.30060458\n",
      "Trained batch 447 batch loss 1.23307502 epoch total loss 1.30045354\n",
      "Trained batch 448 batch loss 1.32158184 epoch total loss 1.30050075\n",
      "Trained batch 449 batch loss 1.27066731 epoch total loss 1.30043435\n",
      "Trained batch 450 batch loss 1.39735782 epoch total loss 1.30064964\n",
      "Trained batch 451 batch loss 1.23951125 epoch total loss 1.3005141\n",
      "Trained batch 452 batch loss 1.26196027 epoch total loss 1.30042887\n",
      "Trained batch 453 batch loss 1.29107046 epoch total loss 1.30040812\n",
      "Trained batch 454 batch loss 1.23436284 epoch total loss 1.30026269\n",
      "Trained batch 455 batch loss 1.18536186 epoch total loss 1.3000102\n",
      "Trained batch 456 batch loss 1.25435853 epoch total loss 1.29991007\n",
      "Trained batch 457 batch loss 1.3558346 epoch total loss 1.30003238\n",
      "Trained batch 458 batch loss 1.34404659 epoch total loss 1.30012858\n",
      "Trained batch 459 batch loss 1.28872335 epoch total loss 1.30010366\n",
      "Trained batch 460 batch loss 1.52863872 epoch total loss 1.30060041\n",
      "Trained batch 461 batch loss 1.52143371 epoch total loss 1.30107939\n",
      "Trained batch 462 batch loss 1.43452299 epoch total loss 1.30136824\n",
      "Trained batch 463 batch loss 1.2157892 epoch total loss 1.30118334\n",
      "Trained batch 464 batch loss 1.14324188 epoch total loss 1.300843\n",
      "Trained batch 465 batch loss 1.1034503 epoch total loss 1.3004185\n",
      "Trained batch 466 batch loss 1.11307275 epoch total loss 1.30001652\n",
      "Trained batch 467 batch loss 1.24220037 epoch total loss 1.29989266\n",
      "Trained batch 468 batch loss 1.05596161 epoch total loss 1.29937148\n",
      "Trained batch 469 batch loss 1.01026654 epoch total loss 1.29875493\n",
      "Trained batch 470 batch loss 1.06404257 epoch total loss 1.29825556\n",
      "Trained batch 471 batch loss 1.11258769 epoch total loss 1.29786146\n",
      "Trained batch 472 batch loss 1.13933563 epoch total loss 1.29752553\n",
      "Trained batch 473 batch loss 1.2293067 epoch total loss 1.2973814\n",
      "Trained batch 474 batch loss 1.31469071 epoch total loss 1.29741788\n",
      "Trained batch 475 batch loss 1.28601515 epoch total loss 1.29739392\n",
      "Trained batch 476 batch loss 1.36216784 epoch total loss 1.29752994\n",
      "Trained batch 477 batch loss 1.34127438 epoch total loss 1.29762161\n",
      "Trained batch 478 batch loss 1.28280985 epoch total loss 1.29759073\n",
      "Trained batch 479 batch loss 1.29087412 epoch total loss 1.29757667\n",
      "Trained batch 480 batch loss 1.23905146 epoch total loss 1.29745483\n",
      "Trained batch 481 batch loss 1.14514351 epoch total loss 1.29713821\n",
      "Trained batch 482 batch loss 1.26799011 epoch total loss 1.29707778\n",
      "Trained batch 483 batch loss 1.24067426 epoch total loss 1.29696095\n",
      "Trained batch 484 batch loss 1.21030843 epoch total loss 1.2967819\n",
      "Trained batch 485 batch loss 1.1716876 epoch total loss 1.29652405\n",
      "Trained batch 486 batch loss 1.21996 epoch total loss 1.29636645\n",
      "Trained batch 487 batch loss 1.20144606 epoch total loss 1.29617155\n",
      "Trained batch 488 batch loss 1.28951013 epoch total loss 1.29615784\n",
      "Trained batch 489 batch loss 1.09213901 epoch total loss 1.29574072\n",
      "Trained batch 490 batch loss 1.11487198 epoch total loss 1.29537153\n",
      "Trained batch 491 batch loss 1.29633427 epoch total loss 1.29537344\n",
      "Trained batch 492 batch loss 1.23848045 epoch total loss 1.29525781\n",
      "Trained batch 493 batch loss 1.22139239 epoch total loss 1.29510796\n",
      "Trained batch 494 batch loss 1.26455736 epoch total loss 1.29504621\n",
      "Trained batch 495 batch loss 1.34574723 epoch total loss 1.29514861\n",
      "Trained batch 496 batch loss 1.3901726 epoch total loss 1.2953403\n",
      "Trained batch 497 batch loss 1.31425512 epoch total loss 1.29537833\n",
      "Trained batch 498 batch loss 1.30617881 epoch total loss 1.2954\n",
      "Trained batch 499 batch loss 1.31937742 epoch total loss 1.29544806\n",
      "Trained batch 500 batch loss 1.50753343 epoch total loss 1.29587221\n",
      "Trained batch 501 batch loss 1.48549223 epoch total loss 1.29625058\n",
      "Trained batch 502 batch loss 1.27429318 epoch total loss 1.29620695\n",
      "Trained batch 503 batch loss 1.25971985 epoch total loss 1.29613435\n",
      "Trained batch 504 batch loss 1.20044935 epoch total loss 1.29594445\n",
      "Trained batch 505 batch loss 1.19412422 epoch total loss 1.29574287\n",
      "Trained batch 506 batch loss 1.30422068 epoch total loss 1.29575956\n",
      "Trained batch 507 batch loss 1.29438496 epoch total loss 1.29575682\n",
      "Trained batch 508 batch loss 1.35298407 epoch total loss 1.29586947\n",
      "Trained batch 509 batch loss 1.45807171 epoch total loss 1.29618812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 510 batch loss 1.33555532 epoch total loss 1.29626536\n",
      "Trained batch 511 batch loss 1.43090248 epoch total loss 1.29652882\n",
      "Trained batch 512 batch loss 1.34676242 epoch total loss 1.29662693\n",
      "Trained batch 513 batch loss 1.27191818 epoch total loss 1.29657876\n",
      "Trained batch 514 batch loss 1.26309979 epoch total loss 1.29651368\n",
      "Trained batch 515 batch loss 1.35670471 epoch total loss 1.2966305\n",
      "Trained batch 516 batch loss 1.27080894 epoch total loss 1.29658043\n",
      "Trained batch 517 batch loss 1.22450364 epoch total loss 1.29644108\n",
      "Trained batch 518 batch loss 1.19197273 epoch total loss 1.29623926\n",
      "Trained batch 519 batch loss 1.22240567 epoch total loss 1.29609704\n",
      "Trained batch 520 batch loss 1.22836971 epoch total loss 1.29596686\n",
      "Trained batch 521 batch loss 1.21144283 epoch total loss 1.29580462\n",
      "Trained batch 522 batch loss 1.29423404 epoch total loss 1.29580164\n",
      "Trained batch 523 batch loss 1.5080514 epoch total loss 1.29620743\n",
      "Trained batch 524 batch loss 1.438977 epoch total loss 1.29648\n",
      "Trained batch 525 batch loss 1.44630849 epoch total loss 1.29676521\n",
      "Trained batch 526 batch loss 1.27253914 epoch total loss 1.29671919\n",
      "Trained batch 527 batch loss 1.258 epoch total loss 1.29664564\n",
      "Trained batch 528 batch loss 1.22125185 epoch total loss 1.29650295\n",
      "Trained batch 529 batch loss 1.25413013 epoch total loss 1.29642284\n",
      "Trained batch 530 batch loss 1.41049 epoch total loss 1.29663801\n",
      "Trained batch 531 batch loss 1.30115736 epoch total loss 1.29664648\n",
      "Trained batch 532 batch loss 1.16595781 epoch total loss 1.29640079\n",
      "Trained batch 533 batch loss 1.09388852 epoch total loss 1.29602087\n",
      "Trained batch 534 batch loss 1.15815282 epoch total loss 1.29576266\n",
      "Trained batch 535 batch loss 1.10484 epoch total loss 1.29540586\n",
      "Trained batch 536 batch loss 1.32237959 epoch total loss 1.29545617\n",
      "Trained batch 537 batch loss 1.51200819 epoch total loss 1.29585946\n",
      "Trained batch 538 batch loss 1.52226233 epoch total loss 1.29628026\n",
      "Trained batch 539 batch loss 1.47640193 epoch total loss 1.29661441\n",
      "Trained batch 540 batch loss 1.41199136 epoch total loss 1.29682815\n",
      "Trained batch 541 batch loss 1.49346483 epoch total loss 1.29719162\n",
      "Trained batch 542 batch loss 1.50171232 epoch total loss 1.29756892\n",
      "Trained batch 543 batch loss 1.31723571 epoch total loss 1.29760516\n",
      "Trained batch 544 batch loss 1.33142495 epoch total loss 1.29766738\n",
      "Trained batch 545 batch loss 1.40252101 epoch total loss 1.29785979\n",
      "Trained batch 546 batch loss 1.31169486 epoch total loss 1.29788506\n",
      "Trained batch 547 batch loss 1.18493748 epoch total loss 1.29767859\n",
      "Trained batch 548 batch loss 1.19149864 epoch total loss 1.29748487\n",
      "Trained batch 549 batch loss 1.18437815 epoch total loss 1.29727888\n",
      "Trained batch 550 batch loss 1.16106987 epoch total loss 1.29703128\n",
      "Trained batch 551 batch loss 1.26012278 epoch total loss 1.29696429\n",
      "Trained batch 552 batch loss 1.13733 epoch total loss 1.29667509\n",
      "Trained batch 553 batch loss 1.22765207 epoch total loss 1.29655027\n",
      "Trained batch 554 batch loss 1.12010717 epoch total loss 1.29623187\n",
      "Trained batch 555 batch loss 1.29171515 epoch total loss 1.29622364\n",
      "Trained batch 556 batch loss 1.35077941 epoch total loss 1.29632175\n",
      "Trained batch 557 batch loss 1.26239121 epoch total loss 1.29626083\n",
      "Trained batch 558 batch loss 1.1992439 epoch total loss 1.29608691\n",
      "Trained batch 559 batch loss 1.28723621 epoch total loss 1.29607105\n",
      "Trained batch 560 batch loss 1.25226593 epoch total loss 1.29599285\n",
      "Trained batch 561 batch loss 1.3135066 epoch total loss 1.29602396\n",
      "Trained batch 562 batch loss 1.31416285 epoch total loss 1.29605627\n",
      "Trained batch 563 batch loss 1.36403084 epoch total loss 1.29617691\n",
      "Trained batch 564 batch loss 1.43210697 epoch total loss 1.29641795\n",
      "Trained batch 565 batch loss 1.4283421 epoch total loss 1.29665148\n",
      "Trained batch 566 batch loss 1.32459903 epoch total loss 1.29670084\n",
      "Trained batch 567 batch loss 1.3782711 epoch total loss 1.29684472\n",
      "Trained batch 568 batch loss 1.19351852 epoch total loss 1.29666293\n",
      "Trained batch 569 batch loss 1.276914 epoch total loss 1.29662824\n",
      "Trained batch 570 batch loss 1.26115417 epoch total loss 1.29656601\n",
      "Trained batch 571 batch loss 1.17870951 epoch total loss 1.29635954\n",
      "Trained batch 572 batch loss 1.15838504 epoch total loss 1.29611838\n",
      "Trained batch 573 batch loss 1.08614147 epoch total loss 1.29575181\n",
      "Trained batch 574 batch loss 1.17421544 epoch total loss 1.29554009\n",
      "Trained batch 575 batch loss 1.2555598 epoch total loss 1.2954706\n",
      "Trained batch 576 batch loss 1.09400809 epoch total loss 1.29512072\n",
      "Trained batch 577 batch loss 1.16820669 epoch total loss 1.29490077\n",
      "Trained batch 578 batch loss 1.21775651 epoch total loss 1.29476738\n",
      "Trained batch 579 batch loss 1.31647921 epoch total loss 1.29480481\n",
      "Trained batch 580 batch loss 1.24087036 epoch total loss 1.29471183\n",
      "Trained batch 581 batch loss 1.08761072 epoch total loss 1.29435527\n",
      "Trained batch 582 batch loss 1.2342422 epoch total loss 1.29425204\n",
      "Trained batch 583 batch loss 1.38071263 epoch total loss 1.29440045\n",
      "Trained batch 584 batch loss 1.3268671 epoch total loss 1.29445601\n",
      "Trained batch 585 batch loss 1.31091094 epoch total loss 1.29448414\n",
      "Trained batch 586 batch loss 1.24658251 epoch total loss 1.29440236\n",
      "Trained batch 587 batch loss 1.39668059 epoch total loss 1.29457653\n",
      "Trained batch 588 batch loss 1.18690944 epoch total loss 1.29439342\n",
      "Trained batch 589 batch loss 1.14503896 epoch total loss 1.29413986\n",
      "Trained batch 590 batch loss 1.06291175 epoch total loss 1.2937479\n",
      "Trained batch 591 batch loss 1.20909965 epoch total loss 1.29360473\n",
      "Trained batch 592 batch loss 1.2163496 epoch total loss 1.2934742\n",
      "Trained batch 593 batch loss 1.24320173 epoch total loss 1.29338956\n",
      "Trained batch 594 batch loss 1.25823283 epoch total loss 1.29333031\n",
      "Trained batch 595 batch loss 1.20556617 epoch total loss 1.29318285\n",
      "Trained batch 596 batch loss 1.31024361 epoch total loss 1.29321146\n",
      "Trained batch 597 batch loss 1.36216545 epoch total loss 1.29332697\n",
      "Trained batch 598 batch loss 1.34348536 epoch total loss 1.2934109\n",
      "Trained batch 599 batch loss 1.33216953 epoch total loss 1.29347563\n",
      "Trained batch 600 batch loss 1.12066841 epoch total loss 1.29318762\n",
      "Trained batch 601 batch loss 1.1798346 epoch total loss 1.29299891\n",
      "Trained batch 602 batch loss 1.3049252 epoch total loss 1.2930187\n",
      "Trained batch 603 batch loss 1.33729815 epoch total loss 1.29309213\n",
      "Trained batch 604 batch loss 1.39983273 epoch total loss 1.29326892\n",
      "Trained batch 605 batch loss 1.28851295 epoch total loss 1.29326105\n",
      "Trained batch 606 batch loss 1.24446595 epoch total loss 1.29318047\n",
      "Trained batch 607 batch loss 1.24770939 epoch total loss 1.29310548\n",
      "Trained batch 608 batch loss 1.3027935 epoch total loss 1.29312146\n",
      "Trained batch 609 batch loss 1.28939617 epoch total loss 1.29311526\n",
      "Trained batch 610 batch loss 1.3118552 epoch total loss 1.2931459\n",
      "Trained batch 611 batch loss 1.35749912 epoch total loss 1.29325128\n",
      "Trained batch 612 batch loss 1.19745493 epoch total loss 1.29309475\n",
      "Trained batch 613 batch loss 1.23082149 epoch total loss 1.29299319\n",
      "Trained batch 614 batch loss 1.20975387 epoch total loss 1.29285765\n",
      "Trained batch 615 batch loss 1.29159379 epoch total loss 1.2928555\n",
      "Trained batch 616 batch loss 1.32771683 epoch total loss 1.29291213\n",
      "Trained batch 617 batch loss 1.28842783 epoch total loss 1.29290485\n",
      "Trained batch 618 batch loss 1.28345835 epoch total loss 1.2928896\n",
      "Trained batch 619 batch loss 1.27597797 epoch total loss 1.2928623\n",
      "Trained batch 620 batch loss 1.18836153 epoch total loss 1.29269373\n",
      "Trained batch 621 batch loss 1.26762629 epoch total loss 1.29265332\n",
      "Trained batch 622 batch loss 1.25877345 epoch total loss 1.29259896\n",
      "Trained batch 623 batch loss 1.25964785 epoch total loss 1.29254603\n",
      "Trained batch 624 batch loss 1.30734849 epoch total loss 1.29256976\n",
      "Trained batch 625 batch loss 1.30423057 epoch total loss 1.29258847\n",
      "Trained batch 626 batch loss 1.16969156 epoch total loss 1.29239213\n",
      "Trained batch 627 batch loss 1.17038035 epoch total loss 1.29219759\n",
      "Trained batch 628 batch loss 1.34528267 epoch total loss 1.2922821\n",
      "Trained batch 629 batch loss 1.3107394 epoch total loss 1.29231143\n",
      "Trained batch 630 batch loss 1.26319838 epoch total loss 1.29226518\n",
      "Trained batch 631 batch loss 1.33165455 epoch total loss 1.29232764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 632 batch loss 1.27793074 epoch total loss 1.29230487\n",
      "Trained batch 633 batch loss 1.21900845 epoch total loss 1.29218912\n",
      "Trained batch 634 batch loss 1.18658507 epoch total loss 1.29202247\n",
      "Trained batch 635 batch loss 1.24507 epoch total loss 1.29194856\n",
      "Trained batch 636 batch loss 1.15157104 epoch total loss 1.29172778\n",
      "Trained batch 637 batch loss 1.24473977 epoch total loss 1.29165411\n",
      "Trained batch 638 batch loss 1.30995369 epoch total loss 1.29168272\n",
      "Trained batch 639 batch loss 1.19800055 epoch total loss 1.29153609\n",
      "Trained batch 640 batch loss 1.25967431 epoch total loss 1.29148638\n",
      "Trained batch 641 batch loss 1.27314103 epoch total loss 1.29145777\n",
      "Trained batch 642 batch loss 1.15378118 epoch total loss 1.29124331\n",
      "Trained batch 643 batch loss 1.25551462 epoch total loss 1.29118776\n",
      "Trained batch 644 batch loss 1.19704652 epoch total loss 1.29104149\n",
      "Trained batch 645 batch loss 1.28143466 epoch total loss 1.29102659\n",
      "Trained batch 646 batch loss 1.17871916 epoch total loss 1.29085279\n",
      "Trained batch 647 batch loss 1.23249829 epoch total loss 1.29076254\n",
      "Trained batch 648 batch loss 1.2205317 epoch total loss 1.29065418\n",
      "Trained batch 649 batch loss 1.23423409 epoch total loss 1.29056716\n",
      "Trained batch 650 batch loss 1.35129464 epoch total loss 1.29066074\n",
      "Trained batch 651 batch loss 1.26064014 epoch total loss 1.29061449\n",
      "Trained batch 652 batch loss 1.26220644 epoch total loss 1.29057097\n",
      "Trained batch 653 batch loss 1.47252679 epoch total loss 1.29084957\n",
      "Trained batch 654 batch loss 1.37888551 epoch total loss 1.29098427\n",
      "Trained batch 655 batch loss 1.18852854 epoch total loss 1.29082787\n",
      "Trained batch 656 batch loss 1.04532814 epoch total loss 1.29045367\n",
      "Trained batch 657 batch loss 1.20869827 epoch total loss 1.29032922\n",
      "Trained batch 658 batch loss 1.22259462 epoch total loss 1.29022622\n",
      "Trained batch 659 batch loss 1.43404365 epoch total loss 1.29044449\n",
      "Trained batch 660 batch loss 1.26316583 epoch total loss 1.29040313\n",
      "Trained batch 661 batch loss 1.37765992 epoch total loss 1.29053521\n",
      "Trained batch 662 batch loss 1.25355494 epoch total loss 1.2904793\n",
      "Trained batch 663 batch loss 1.19924593 epoch total loss 1.29034162\n",
      "Trained batch 664 batch loss 1.2585144 epoch total loss 1.29029369\n",
      "Trained batch 665 batch loss 1.32449019 epoch total loss 1.29034507\n",
      "Trained batch 666 batch loss 1.41537666 epoch total loss 1.29053283\n",
      "Trained batch 667 batch loss 1.42467022 epoch total loss 1.29073393\n",
      "Trained batch 668 batch loss 1.24036634 epoch total loss 1.29065859\n",
      "Trained batch 669 batch loss 1.26052785 epoch total loss 1.29061341\n",
      "Trained batch 670 batch loss 1.29739213 epoch total loss 1.29062355\n",
      "Trained batch 671 batch loss 1.1742059 epoch total loss 1.29045\n",
      "Trained batch 672 batch loss 1.33039403 epoch total loss 1.29050946\n",
      "Trained batch 673 batch loss 1.20259845 epoch total loss 1.29037881\n",
      "Trained batch 674 batch loss 1.25427103 epoch total loss 1.29032516\n",
      "Trained batch 675 batch loss 1.31275892 epoch total loss 1.29035842\n",
      "Trained batch 676 batch loss 1.32327306 epoch total loss 1.29040718\n",
      "Trained batch 677 batch loss 1.38118517 epoch total loss 1.29054117\n",
      "Trained batch 678 batch loss 1.32191992 epoch total loss 1.29058743\n",
      "Trained batch 679 batch loss 1.31300116 epoch total loss 1.29062045\n",
      "Trained batch 680 batch loss 1.36896181 epoch total loss 1.29073572\n",
      "Trained batch 681 batch loss 1.35279572 epoch total loss 1.2908268\n",
      "Trained batch 682 batch loss 1.31400132 epoch total loss 1.29086077\n",
      "Trained batch 683 batch loss 1.40822387 epoch total loss 1.29103255\n",
      "Trained batch 684 batch loss 1.34475422 epoch total loss 1.29111111\n",
      "Trained batch 685 batch loss 1.39002228 epoch total loss 1.29125547\n",
      "Trained batch 686 batch loss 1.30975032 epoch total loss 1.29128242\n",
      "Trained batch 687 batch loss 1.31515098 epoch total loss 1.29131711\n",
      "Trained batch 688 batch loss 1.27042258 epoch total loss 1.29128683\n",
      "Trained batch 689 batch loss 1.29877484 epoch total loss 1.29129767\n",
      "Trained batch 690 batch loss 1.34146428 epoch total loss 1.29137039\n",
      "Trained batch 691 batch loss 1.32385468 epoch total loss 1.29141748\n",
      "Trained batch 692 batch loss 1.24683142 epoch total loss 1.29135299\n",
      "Trained batch 693 batch loss 1.16709292 epoch total loss 1.2911737\n",
      "Trained batch 694 batch loss 1.27571821 epoch total loss 1.2911514\n",
      "Trained batch 695 batch loss 1.22102141 epoch total loss 1.29105043\n",
      "Trained batch 696 batch loss 1.23826683 epoch total loss 1.29097462\n",
      "Trained batch 697 batch loss 1.15420556 epoch total loss 1.29077852\n",
      "Trained batch 698 batch loss 1.07543397 epoch total loss 1.29047\n",
      "Trained batch 699 batch loss 1.09803545 epoch total loss 1.29019463\n",
      "Trained batch 700 batch loss 1.35269046 epoch total loss 1.29028392\n",
      "Trained batch 701 batch loss 1.40722167 epoch total loss 1.29045069\n",
      "Trained batch 702 batch loss 1.38870955 epoch total loss 1.29059076\n",
      "Trained batch 703 batch loss 1.28437376 epoch total loss 1.29058182\n",
      "Trained batch 704 batch loss 1.27320743 epoch total loss 1.29055715\n",
      "Trained batch 705 batch loss 1.21510553 epoch total loss 1.2904501\n",
      "Trained batch 706 batch loss 1.24126363 epoch total loss 1.29038048\n",
      "Trained batch 707 batch loss 1.18409586 epoch total loss 1.29023015\n",
      "Trained batch 708 batch loss 1.30117869 epoch total loss 1.29024565\n",
      "Trained batch 709 batch loss 1.26130581 epoch total loss 1.29020476\n",
      "Trained batch 710 batch loss 1.27010798 epoch total loss 1.29017639\n",
      "Trained batch 711 batch loss 1.19916523 epoch total loss 1.29004836\n",
      "Trained batch 712 batch loss 1.30617607 epoch total loss 1.29007101\n",
      "Trained batch 713 batch loss 1.18999195 epoch total loss 1.2899307\n",
      "Trained batch 714 batch loss 1.24633 epoch total loss 1.28986967\n",
      "Trained batch 715 batch loss 1.20718837 epoch total loss 1.28975403\n",
      "Trained batch 716 batch loss 1.09851789 epoch total loss 1.28948689\n",
      "Trained batch 717 batch loss 1.06066799 epoch total loss 1.28916776\n",
      "Trained batch 718 batch loss 1.00914621 epoch total loss 1.28877783\n",
      "Trained batch 719 batch loss 1.27091563 epoch total loss 1.28875303\n",
      "Trained batch 720 batch loss 1.45557547 epoch total loss 1.28898466\n",
      "Trained batch 721 batch loss 1.64895988 epoch total loss 1.28948402\n",
      "Trained batch 722 batch loss 1.28443396 epoch total loss 1.28947699\n",
      "Trained batch 723 batch loss 1.29970932 epoch total loss 1.28949106\n",
      "Trained batch 724 batch loss 1.26781178 epoch total loss 1.28946114\n",
      "Trained batch 725 batch loss 1.30963802 epoch total loss 1.28948903\n",
      "Trained batch 726 batch loss 1.36906493 epoch total loss 1.28959858\n",
      "Trained batch 727 batch loss 1.30863523 epoch total loss 1.28962481\n",
      "Trained batch 728 batch loss 1.26349878 epoch total loss 1.28958893\n",
      "Trained batch 729 batch loss 1.35983062 epoch total loss 1.28968525\n",
      "Trained batch 730 batch loss 1.31535137 epoch total loss 1.28972042\n",
      "Trained batch 731 batch loss 1.24971414 epoch total loss 1.2896657\n",
      "Trained batch 732 batch loss 1.25708163 epoch total loss 1.28962111\n",
      "Trained batch 733 batch loss 1.22026443 epoch total loss 1.28952658\n",
      "Trained batch 734 batch loss 1.21464062 epoch total loss 1.28942454\n",
      "Trained batch 735 batch loss 1.37182677 epoch total loss 1.28953671\n",
      "Trained batch 736 batch loss 1.23554027 epoch total loss 1.28946328\n",
      "Trained batch 737 batch loss 1.09553432 epoch total loss 1.28920019\n",
      "Trained batch 738 batch loss 1.24950695 epoch total loss 1.2891463\n",
      "Trained batch 739 batch loss 1.30137241 epoch total loss 1.28916287\n",
      "Trained batch 740 batch loss 1.27854252 epoch total loss 1.28914857\n",
      "Trained batch 741 batch loss 1.40691936 epoch total loss 1.28930759\n",
      "Trained batch 742 batch loss 1.30021703 epoch total loss 1.28932226\n",
      "Trained batch 743 batch loss 1.31757426 epoch total loss 1.28936028\n",
      "Trained batch 744 batch loss 1.29592872 epoch total loss 1.28936911\n",
      "Trained batch 745 batch loss 1.34425759 epoch total loss 1.28944278\n",
      "Trained batch 746 batch loss 1.3084805 epoch total loss 1.28946817\n",
      "Trained batch 747 batch loss 1.28035438 epoch total loss 1.28945601\n",
      "Trained batch 748 batch loss 1.22882175 epoch total loss 1.28937495\n",
      "Trained batch 749 batch loss 1.3307097 epoch total loss 1.28943014\n",
      "Trained batch 750 batch loss 1.44235611 epoch total loss 1.28963399\n",
      "Trained batch 751 batch loss 1.21801 epoch total loss 1.28953862\n",
      "Trained batch 752 batch loss 1.34007 epoch total loss 1.28960586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 753 batch loss 1.08291876 epoch total loss 1.28933144\n",
      "Trained batch 754 batch loss 1.15015697 epoch total loss 1.2891469\n",
      "Trained batch 755 batch loss 1.22654724 epoch total loss 1.28906393\n",
      "Trained batch 756 batch loss 1.26592696 epoch total loss 1.28903341\n",
      "Trained batch 757 batch loss 1.29011321 epoch total loss 1.28903472\n",
      "Trained batch 758 batch loss 1.36186647 epoch total loss 1.28913081\n",
      "Trained batch 759 batch loss 1.20790279 epoch total loss 1.28902388\n",
      "Trained batch 760 batch loss 1.22827256 epoch total loss 1.28894389\n",
      "Trained batch 761 batch loss 1.27557731 epoch total loss 1.28892636\n",
      "Trained batch 762 batch loss 1.19777417 epoch total loss 1.28880668\n",
      "Trained batch 763 batch loss 1.20848203 epoch total loss 1.28870142\n",
      "Trained batch 764 batch loss 1.2011348 epoch total loss 1.28858674\n",
      "Trained batch 765 batch loss 1.12204599 epoch total loss 1.28836906\n",
      "Trained batch 766 batch loss 1.12616932 epoch total loss 1.28815734\n",
      "Trained batch 767 batch loss 1.22928429 epoch total loss 1.28808057\n",
      "Trained batch 768 batch loss 1.20155406 epoch total loss 1.28796792\n",
      "Trained batch 769 batch loss 1.15670574 epoch total loss 1.28779721\n",
      "Trained batch 770 batch loss 1.16025424 epoch total loss 1.28763163\n",
      "Trained batch 771 batch loss 1.17150331 epoch total loss 1.28748095\n",
      "Trained batch 772 batch loss 1.26717532 epoch total loss 1.28745461\n",
      "Trained batch 773 batch loss 1.42438114 epoch total loss 1.28763175\n",
      "Trained batch 774 batch loss 1.29740882 epoch total loss 1.28764439\n",
      "Trained batch 775 batch loss 1.35550475 epoch total loss 1.28773201\n",
      "Trained batch 776 batch loss 1.37807703 epoch total loss 1.28784835\n",
      "Trained batch 777 batch loss 1.46966672 epoch total loss 1.28808236\n",
      "Trained batch 778 batch loss 1.27964318 epoch total loss 1.28807163\n",
      "Trained batch 779 batch loss 1.34059775 epoch total loss 1.28813899\n",
      "Trained batch 780 batch loss 1.24559224 epoch total loss 1.28808451\n",
      "Trained batch 781 batch loss 1.23617578 epoch total loss 1.28801799\n",
      "Trained batch 782 batch loss 1.23665035 epoch total loss 1.2879523\n",
      "Trained batch 783 batch loss 1.23332262 epoch total loss 1.28788257\n",
      "Trained batch 784 batch loss 1.32993007 epoch total loss 1.28793621\n",
      "Trained batch 785 batch loss 1.25015473 epoch total loss 1.28788817\n",
      "Trained batch 786 batch loss 1.2462883 epoch total loss 1.28783524\n",
      "Trained batch 787 batch loss 1.27302504 epoch total loss 1.28781641\n",
      "Trained batch 788 batch loss 1.23488295 epoch total loss 1.28774917\n",
      "Trained batch 789 batch loss 1.17382681 epoch total loss 1.28760481\n",
      "Trained batch 790 batch loss 1.11426115 epoch total loss 1.28738534\n",
      "Trained batch 791 batch loss 1.14665651 epoch total loss 1.28720748\n",
      "Trained batch 792 batch loss 1.16305864 epoch total loss 1.28705072\n",
      "Trained batch 793 batch loss 1.19847035 epoch total loss 1.28693902\n",
      "Trained batch 794 batch loss 1.20423412 epoch total loss 1.28683484\n",
      "Trained batch 795 batch loss 1.26729465 epoch total loss 1.28681028\n",
      "Trained batch 796 batch loss 1.17803836 epoch total loss 1.28667355\n",
      "Trained batch 797 batch loss 1.19671309 epoch total loss 1.28656054\n",
      "Trained batch 798 batch loss 1.43192554 epoch total loss 1.28674269\n",
      "Trained batch 799 batch loss 1.3717829 epoch total loss 1.28684914\n",
      "Trained batch 800 batch loss 1.23888695 epoch total loss 1.2867893\n",
      "Trained batch 801 batch loss 1.38953424 epoch total loss 1.28691745\n",
      "Trained batch 802 batch loss 1.2512424 epoch total loss 1.28687298\n",
      "Trained batch 803 batch loss 1.19483197 epoch total loss 1.28675842\n",
      "Trained batch 804 batch loss 1.21058989 epoch total loss 1.28666365\n",
      "Trained batch 805 batch loss 1.22886145 epoch total loss 1.28659177\n",
      "Trained batch 806 batch loss 1.19477355 epoch total loss 1.28647792\n",
      "Trained batch 807 batch loss 1.36439908 epoch total loss 1.28657448\n",
      "Trained batch 808 batch loss 1.46276283 epoch total loss 1.28679252\n",
      "Trained batch 809 batch loss 1.22398913 epoch total loss 1.28671491\n",
      "Trained batch 810 batch loss 1.32040739 epoch total loss 1.28675663\n",
      "Trained batch 811 batch loss 1.1039573 epoch total loss 1.28653121\n",
      "Trained batch 812 batch loss 1.20822966 epoch total loss 1.28643489\n",
      "Trained batch 813 batch loss 1.15343571 epoch total loss 1.28627121\n",
      "Trained batch 814 batch loss 1.19844627 epoch total loss 1.28616345\n",
      "Trained batch 815 batch loss 1.33953166 epoch total loss 1.28622878\n",
      "Trained batch 816 batch loss 1.35342276 epoch total loss 1.28631115\n",
      "Trained batch 817 batch loss 1.28598428 epoch total loss 1.28631079\n",
      "Trained batch 818 batch loss 1.28083503 epoch total loss 1.28630412\n",
      "Trained batch 819 batch loss 1.36370862 epoch total loss 1.28639877\n",
      "Trained batch 820 batch loss 1.21280468 epoch total loss 1.28630888\n",
      "Trained batch 821 batch loss 1.23830652 epoch total loss 1.28625047\n",
      "Trained batch 822 batch loss 1.23812568 epoch total loss 1.28619194\n",
      "Trained batch 823 batch loss 1.3305397 epoch total loss 1.28624582\n",
      "Trained batch 824 batch loss 1.25517797 epoch total loss 1.28620803\n",
      "Trained batch 825 batch loss 1.29019618 epoch total loss 1.2862128\n",
      "Trained batch 826 batch loss 1.27103043 epoch total loss 1.28619444\n",
      "Trained batch 827 batch loss 1.25508451 epoch total loss 1.28615689\n",
      "Trained batch 828 batch loss 1.13351941 epoch total loss 1.2859726\n",
      "Trained batch 829 batch loss 1.1953094 epoch total loss 1.28586316\n",
      "Trained batch 830 batch loss 1.2922169 epoch total loss 1.28587091\n",
      "Trained batch 831 batch loss 1.30815506 epoch total loss 1.28589761\n",
      "Trained batch 832 batch loss 1.30499589 epoch total loss 1.28592062\n",
      "Trained batch 833 batch loss 1.28039432 epoch total loss 1.28591406\n",
      "Trained batch 834 batch loss 1.18057692 epoch total loss 1.2857877\n",
      "Trained batch 835 batch loss 1.2756784 epoch total loss 1.28577554\n",
      "Trained batch 836 batch loss 1.30946851 epoch total loss 1.28580379\n",
      "Trained batch 837 batch loss 1.32291794 epoch total loss 1.28584814\n",
      "Trained batch 838 batch loss 1.29600728 epoch total loss 1.2858603\n",
      "Trained batch 839 batch loss 1.32107306 epoch total loss 1.28590226\n",
      "Trained batch 840 batch loss 1.2601012 epoch total loss 1.28587151\n",
      "Trained batch 841 batch loss 1.26723766 epoch total loss 1.28584933\n",
      "Trained batch 842 batch loss 1.09471202 epoch total loss 1.28562236\n",
      "Trained batch 843 batch loss 1.26600945 epoch total loss 1.28559911\n",
      "Trained batch 844 batch loss 1.30037344 epoch total loss 1.28561664\n",
      "Trained batch 845 batch loss 1.22118711 epoch total loss 1.28554034\n",
      "Trained batch 846 batch loss 1.18778718 epoch total loss 1.28542483\n",
      "Trained batch 847 batch loss 1.16851437 epoch total loss 1.28528666\n",
      "Trained batch 848 batch loss 1.13809407 epoch total loss 1.2851131\n",
      "Trained batch 849 batch loss 1.24517167 epoch total loss 1.28506601\n",
      "Trained batch 850 batch loss 1.19629943 epoch total loss 1.28496146\n",
      "Trained batch 851 batch loss 1.15381598 epoch total loss 1.28480744\n",
      "Trained batch 852 batch loss 1.26414013 epoch total loss 1.28478312\n",
      "Trained batch 853 batch loss 1.35200155 epoch total loss 1.28486204\n",
      "Trained batch 854 batch loss 1.4490751 epoch total loss 1.28505433\n",
      "Trained batch 855 batch loss 1.30286884 epoch total loss 1.28507519\n",
      "Trained batch 856 batch loss 1.31669223 epoch total loss 1.28511202\n",
      "Trained batch 857 batch loss 1.28380966 epoch total loss 1.28511047\n",
      "Trained batch 858 batch loss 1.14077258 epoch total loss 1.28494227\n",
      "Trained batch 859 batch loss 1.15488064 epoch total loss 1.28479087\n",
      "Trained batch 860 batch loss 1.26855659 epoch total loss 1.28477204\n",
      "Trained batch 861 batch loss 1.35085177 epoch total loss 1.28484869\n",
      "Trained batch 862 batch loss 1.33873725 epoch total loss 1.28491127\n",
      "Trained batch 863 batch loss 1.27124941 epoch total loss 1.28489542\n",
      "Trained batch 864 batch loss 1.23659265 epoch total loss 1.28483951\n",
      "Trained batch 865 batch loss 1.19400048 epoch total loss 1.28473449\n",
      "Trained batch 866 batch loss 1.25761104 epoch total loss 1.28470302\n",
      "Trained batch 867 batch loss 1.12889707 epoch total loss 1.28452337\n",
      "Trained batch 868 batch loss 1.08727193 epoch total loss 1.28429615\n",
      "Trained batch 869 batch loss 1.08480263 epoch total loss 1.28406656\n",
      "Trained batch 870 batch loss 1.27071726 epoch total loss 1.2840513\n",
      "Trained batch 871 batch loss 1.40783918 epoch total loss 1.2841934\n",
      "Trained batch 872 batch loss 1.27812564 epoch total loss 1.28418636\n",
      "Trained batch 873 batch loss 1.36870575 epoch total loss 1.28428316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 874 batch loss 1.37909591 epoch total loss 1.28439176\n",
      "Trained batch 875 batch loss 1.23655486 epoch total loss 1.28433704\n",
      "Trained batch 876 batch loss 1.20289254 epoch total loss 1.28424406\n",
      "Trained batch 877 batch loss 1.23275542 epoch total loss 1.28418541\n",
      "Trained batch 878 batch loss 1.26663542 epoch total loss 1.28416538\n",
      "Trained batch 879 batch loss 1.33747435 epoch total loss 1.28422606\n",
      "Trained batch 880 batch loss 1.23496521 epoch total loss 1.28417015\n",
      "Trained batch 881 batch loss 1.38228488 epoch total loss 1.28428149\n",
      "Trained batch 882 batch loss 1.30297613 epoch total loss 1.28430271\n",
      "Trained batch 883 batch loss 1.3714993 epoch total loss 1.28440142\n",
      "Trained batch 884 batch loss 1.33778906 epoch total loss 1.28446186\n",
      "Trained batch 885 batch loss 1.46235907 epoch total loss 1.28466284\n",
      "Trained batch 886 batch loss 1.39805031 epoch total loss 1.28479087\n",
      "Trained batch 887 batch loss 1.4621886 epoch total loss 1.28499079\n",
      "Trained batch 888 batch loss 1.36275911 epoch total loss 1.28507841\n",
      "Trained batch 889 batch loss 1.40765262 epoch total loss 1.28521621\n",
      "Trained batch 890 batch loss 1.32076526 epoch total loss 1.28525627\n",
      "Trained batch 891 batch loss 1.23400569 epoch total loss 1.28519869\n",
      "Trained batch 892 batch loss 1.3160888 epoch total loss 1.28523326\n",
      "Trained batch 893 batch loss 1.20991147 epoch total loss 1.28514898\n",
      "Trained batch 894 batch loss 1.21671808 epoch total loss 1.28507245\n",
      "Trained batch 895 batch loss 1.28933 epoch total loss 1.2850771\n",
      "Trained batch 896 batch loss 1.09987664 epoch total loss 1.28487039\n",
      "Trained batch 897 batch loss 1.42274463 epoch total loss 1.28502417\n",
      "Trained batch 898 batch loss 1.2775315 epoch total loss 1.28501582\n",
      "Trained batch 899 batch loss 1.35896814 epoch total loss 1.28509808\n",
      "Trained batch 900 batch loss 1.28412521 epoch total loss 1.28509712\n",
      "Trained batch 901 batch loss 1.16686261 epoch total loss 1.28496587\n",
      "Trained batch 902 batch loss 1.20275307 epoch total loss 1.2848748\n",
      "Trained batch 903 batch loss 1.26846457 epoch total loss 1.28485656\n",
      "Trained batch 904 batch loss 1.12015462 epoch total loss 1.28467429\n",
      "Trained batch 905 batch loss 1.2807157 epoch total loss 1.28467\n",
      "Trained batch 906 batch loss 1.27231312 epoch total loss 1.28465641\n",
      "Trained batch 907 batch loss 1.27032161 epoch total loss 1.28464055\n",
      "Trained batch 908 batch loss 1.29034257 epoch total loss 1.28464675\n",
      "Trained batch 909 batch loss 1.20931518 epoch total loss 1.2845639\n",
      "Trained batch 910 batch loss 1.17976546 epoch total loss 1.28444874\n",
      "Trained batch 911 batch loss 1.2603271 epoch total loss 1.2844224\n",
      "Trained batch 912 batch loss 1.44774389 epoch total loss 1.28460145\n",
      "Trained batch 913 batch loss 1.16491616 epoch total loss 1.28447032\n",
      "Trained batch 914 batch loss 1.20150423 epoch total loss 1.2843796\n",
      "Trained batch 915 batch loss 1.14467335 epoch total loss 1.28422689\n",
      "Trained batch 916 batch loss 1.24973488 epoch total loss 1.28418922\n",
      "Trained batch 917 batch loss 1.43245339 epoch total loss 1.28435099\n",
      "Trained batch 918 batch loss 1.27178609 epoch total loss 1.28433728\n",
      "Trained batch 919 batch loss 1.37298298 epoch total loss 1.2844336\n",
      "Trained batch 920 batch loss 1.2751708 epoch total loss 1.28442359\n",
      "Trained batch 921 batch loss 1.30894923 epoch total loss 1.28445017\n",
      "Trained batch 922 batch loss 1.27365303 epoch total loss 1.28443849\n",
      "Trained batch 923 batch loss 1.2585237 epoch total loss 1.28441048\n",
      "Trained batch 924 batch loss 1.3022275 epoch total loss 1.28442979\n",
      "Trained batch 925 batch loss 1.24819052 epoch total loss 1.28439057\n",
      "Trained batch 926 batch loss 1.28842044 epoch total loss 1.28439498\n",
      "Trained batch 927 batch loss 1.28684175 epoch total loss 1.2843976\n",
      "Trained batch 928 batch loss 1.28084362 epoch total loss 1.28439379\n",
      "Trained batch 929 batch loss 1.27344847 epoch total loss 1.28438199\n",
      "Trained batch 930 batch loss 1.32293475 epoch total loss 1.28442347\n",
      "Trained batch 931 batch loss 1.35919189 epoch total loss 1.28450382\n",
      "Trained batch 932 batch loss 1.34021699 epoch total loss 1.28456354\n",
      "Trained batch 933 batch loss 1.13990784 epoch total loss 1.28440857\n",
      "Trained batch 934 batch loss 1.16882038 epoch total loss 1.28428471\n",
      "Trained batch 935 batch loss 1.28866148 epoch total loss 1.28428948\n",
      "Trained batch 936 batch loss 1.15892947 epoch total loss 1.28415561\n",
      "Trained batch 937 batch loss 1.31024814 epoch total loss 1.2841835\n",
      "Trained batch 938 batch loss 1.2945118 epoch total loss 1.28419447\n",
      "Trained batch 939 batch loss 1.26137471 epoch total loss 1.28417015\n",
      "Trained batch 940 batch loss 1.28385067 epoch total loss 1.28416979\n",
      "Trained batch 941 batch loss 1.35305703 epoch total loss 1.28424299\n",
      "Trained batch 942 batch loss 1.27089906 epoch total loss 1.2842288\n",
      "Trained batch 943 batch loss 1.33698905 epoch total loss 1.28428483\n",
      "Trained batch 944 batch loss 1.28583837 epoch total loss 1.2842865\n",
      "Trained batch 945 batch loss 1.20962632 epoch total loss 1.28420746\n",
      "Trained batch 946 batch loss 1.21465373 epoch total loss 1.28413391\n",
      "Trained batch 947 batch loss 1.2732631 epoch total loss 1.28412247\n",
      "Trained batch 948 batch loss 1.27137876 epoch total loss 1.284109\n",
      "Trained batch 949 batch loss 1.36138928 epoch total loss 1.28419054\n",
      "Trained batch 950 batch loss 1.40241981 epoch total loss 1.28431499\n",
      "Trained batch 951 batch loss 1.32952905 epoch total loss 1.28436255\n",
      "Trained batch 952 batch loss 1.25807357 epoch total loss 1.28433502\n",
      "Trained batch 953 batch loss 1.22532606 epoch total loss 1.28427303\n",
      "Trained batch 954 batch loss 1.25347817 epoch total loss 1.28424072\n",
      "Trained batch 955 batch loss 1.26860332 epoch total loss 1.28422427\n",
      "Trained batch 956 batch loss 1.28243601 epoch total loss 1.28422248\n",
      "Trained batch 957 batch loss 1.29228354 epoch total loss 1.28423083\n",
      "Trained batch 958 batch loss 1.24989593 epoch total loss 1.28419495\n",
      "Trained batch 959 batch loss 1.25193095 epoch total loss 1.28416133\n",
      "Trained batch 960 batch loss 1.19734454 epoch total loss 1.28407097\n",
      "Trained batch 961 batch loss 1.24373686 epoch total loss 1.28402901\n",
      "Trained batch 962 batch loss 1.2333169 epoch total loss 1.28397632\n",
      "Trained batch 963 batch loss 1.40021813 epoch total loss 1.28409708\n",
      "Trained batch 964 batch loss 1.28931856 epoch total loss 1.28410244\n",
      "Trained batch 965 batch loss 1.14931273 epoch total loss 1.28396273\n",
      "Trained batch 966 batch loss 1.26793754 epoch total loss 1.28394616\n",
      "Trained batch 967 batch loss 1.21758103 epoch total loss 1.28387749\n",
      "Trained batch 968 batch loss 1.30122471 epoch total loss 1.28389549\n",
      "Trained batch 969 batch loss 1.32981646 epoch total loss 1.28394282\n",
      "Trained batch 970 batch loss 1.25658035 epoch total loss 1.28391469\n",
      "Trained batch 971 batch loss 1.39940214 epoch total loss 1.28403366\n",
      "Trained batch 972 batch loss 1.40032935 epoch total loss 1.28415322\n",
      "Trained batch 973 batch loss 1.3294816 epoch total loss 1.28419971\n",
      "Trained batch 974 batch loss 1.3566879 epoch total loss 1.28427422\n",
      "Trained batch 975 batch loss 1.22174048 epoch total loss 1.28421\n",
      "Trained batch 976 batch loss 1.19355798 epoch total loss 1.2841171\n",
      "Trained batch 977 batch loss 1.23568368 epoch total loss 1.28406763\n",
      "Trained batch 978 batch loss 1.28840613 epoch total loss 1.28407204\n",
      "Trained batch 979 batch loss 1.34526181 epoch total loss 1.28413451\n",
      "Trained batch 980 batch loss 1.24797237 epoch total loss 1.28409755\n",
      "Trained batch 981 batch loss 1.27812648 epoch total loss 1.28409147\n",
      "Trained batch 982 batch loss 1.17393589 epoch total loss 1.2839793\n",
      "Trained batch 983 batch loss 1.2606163 epoch total loss 1.28395557\n",
      "Trained batch 984 batch loss 1.37278318 epoch total loss 1.28404582\n",
      "Trained batch 985 batch loss 1.2074703 epoch total loss 1.28396809\n",
      "Trained batch 986 batch loss 1.0787859 epoch total loss 1.28376\n",
      "Trained batch 987 batch loss 1.18270123 epoch total loss 1.28365767\n",
      "Trained batch 988 batch loss 1.24725223 epoch total loss 1.28362072\n",
      "Trained batch 989 batch loss 1.21906757 epoch total loss 1.28355551\n",
      "Trained batch 990 batch loss 1.1116631 epoch total loss 1.28338194\n",
      "Trained batch 991 batch loss 1.23435569 epoch total loss 1.28333247\n",
      "Trained batch 992 batch loss 1.31657171 epoch total loss 1.28336596\n",
      "Trained batch 993 batch loss 1.25217378 epoch total loss 1.28333449\n",
      "Trained batch 994 batch loss 1.31177258 epoch total loss 1.2833631\n",
      "Trained batch 995 batch loss 1.33518791 epoch total loss 1.2834152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 996 batch loss 1.4333781 epoch total loss 1.28356576\n",
      "Trained batch 997 batch loss 1.46868408 epoch total loss 1.28375137\n",
      "Trained batch 998 batch loss 1.46600127 epoch total loss 1.283934\n",
      "Trained batch 999 batch loss 1.29288793 epoch total loss 1.28394294\n",
      "Trained batch 1000 batch loss 1.29177046 epoch total loss 1.28395069\n",
      "Trained batch 1001 batch loss 1.36201286 epoch total loss 1.28402877\n",
      "Trained batch 1002 batch loss 1.37522805 epoch total loss 1.28411973\n",
      "Trained batch 1003 batch loss 1.4061209 epoch total loss 1.28424144\n",
      "Trained batch 1004 batch loss 1.38319767 epoch total loss 1.2843399\n",
      "Trained batch 1005 batch loss 1.29560983 epoch total loss 1.28435123\n",
      "Trained batch 1006 batch loss 1.29738069 epoch total loss 1.2843641\n",
      "Trained batch 1007 batch loss 1.23229718 epoch total loss 1.28431237\n",
      "Trained batch 1008 batch loss 1.27104902 epoch total loss 1.28429925\n",
      "Trained batch 1009 batch loss 1.2823956 epoch total loss 1.28429723\n",
      "Trained batch 1010 batch loss 1.34028673 epoch total loss 1.28435278\n",
      "Trained batch 1011 batch loss 1.3863225 epoch total loss 1.28445363\n",
      "Trained batch 1012 batch loss 1.41276908 epoch total loss 1.28458035\n",
      "Trained batch 1013 batch loss 1.48180223 epoch total loss 1.28477514\n",
      "Trained batch 1014 batch loss 1.44998872 epoch total loss 1.28493798\n",
      "Trained batch 1015 batch loss 1.43214941 epoch total loss 1.28508306\n",
      "Trained batch 1016 batch loss 1.43803096 epoch total loss 1.2852335\n",
      "Trained batch 1017 batch loss 1.40759778 epoch total loss 1.28535378\n",
      "Trained batch 1018 batch loss 1.36251068 epoch total loss 1.2854296\n",
      "Trained batch 1019 batch loss 1.33702826 epoch total loss 1.28548026\n",
      "Trained batch 1020 batch loss 1.40183485 epoch total loss 1.28559434\n",
      "Trained batch 1021 batch loss 1.27466071 epoch total loss 1.28558373\n",
      "Trained batch 1022 batch loss 1.21808696 epoch total loss 1.28551769\n",
      "Trained batch 1023 batch loss 1.26566291 epoch total loss 1.28549826\n",
      "Trained batch 1024 batch loss 1.3838253 epoch total loss 1.28559422\n",
      "Trained batch 1025 batch loss 1.17848814 epoch total loss 1.28548968\n",
      "Trained batch 1026 batch loss 1.30099 epoch total loss 1.28550482\n",
      "Trained batch 1027 batch loss 1.34528255 epoch total loss 1.28556311\n",
      "Trained batch 1028 batch loss 1.29577827 epoch total loss 1.28557301\n",
      "Trained batch 1029 batch loss 1.13163543 epoch total loss 1.2854234\n",
      "Trained batch 1030 batch loss 1.11482573 epoch total loss 1.28525782\n",
      "Trained batch 1031 batch loss 1.19122803 epoch total loss 1.28516662\n",
      "Trained batch 1032 batch loss 1.11107981 epoch total loss 1.28499794\n",
      "Trained batch 1033 batch loss 1.12716353 epoch total loss 1.28484523\n",
      "Trained batch 1034 batch loss 1.13185871 epoch total loss 1.28469729\n",
      "Trained batch 1035 batch loss 1.10229182 epoch total loss 1.28452098\n",
      "Trained batch 1036 batch loss 1.27628362 epoch total loss 1.284513\n",
      "Trained batch 1037 batch loss 1.26767623 epoch total loss 1.28449678\n",
      "Trained batch 1038 batch loss 1.28101957 epoch total loss 1.28449345\n",
      "Trained batch 1039 batch loss 1.30065894 epoch total loss 1.28450906\n",
      "Trained batch 1040 batch loss 1.33213377 epoch total loss 1.28455484\n",
      "Trained batch 1041 batch loss 1.2446537 epoch total loss 1.28451645\n",
      "Trained batch 1042 batch loss 1.30680597 epoch total loss 1.28453779\n",
      "Trained batch 1043 batch loss 1.18952656 epoch total loss 1.28444672\n",
      "Trained batch 1044 batch loss 1.26677394 epoch total loss 1.28442979\n",
      "Trained batch 1045 batch loss 1.32353258 epoch total loss 1.28446722\n",
      "Trained batch 1046 batch loss 1.3614341 epoch total loss 1.28454077\n",
      "Trained batch 1047 batch loss 1.28170681 epoch total loss 1.28453803\n",
      "Trained batch 1048 batch loss 1.14486945 epoch total loss 1.28440487\n",
      "Trained batch 1049 batch loss 1.23232961 epoch total loss 1.28435516\n",
      "Trained batch 1050 batch loss 1.16798019 epoch total loss 1.2842443\n",
      "Trained batch 1051 batch loss 1.24196339 epoch total loss 1.28420413\n",
      "Trained batch 1052 batch loss 1.32515025 epoch total loss 1.28424299\n",
      "Trained batch 1053 batch loss 1.33516836 epoch total loss 1.28429139\n",
      "Trained batch 1054 batch loss 1.31479895 epoch total loss 1.28432035\n",
      "Trained batch 1055 batch loss 1.31023073 epoch total loss 1.28434491\n",
      "Trained batch 1056 batch loss 1.24890041 epoch total loss 1.28431129\n",
      "Trained batch 1057 batch loss 1.29604292 epoch total loss 1.28432238\n",
      "Trained batch 1058 batch loss 1.21841788 epoch total loss 1.28426015\n",
      "Trained batch 1059 batch loss 1.20548117 epoch total loss 1.28418565\n",
      "Trained batch 1060 batch loss 1.29407072 epoch total loss 1.28419495\n",
      "Trained batch 1061 batch loss 1.25606596 epoch total loss 1.28416848\n",
      "Trained batch 1062 batch loss 1.23624444 epoch total loss 1.28412342\n",
      "Trained batch 1063 batch loss 1.23788917 epoch total loss 1.28407991\n",
      "Trained batch 1064 batch loss 1.28701532 epoch total loss 1.28408265\n",
      "Trained batch 1065 batch loss 1.1478641 epoch total loss 1.28395474\n",
      "Trained batch 1066 batch loss 1.11683083 epoch total loss 1.28379786\n",
      "Trained batch 1067 batch loss 1.33125758 epoch total loss 1.28384244\n",
      "Trained batch 1068 batch loss 1.32393801 epoch total loss 1.28388\n",
      "Trained batch 1069 batch loss 1.34718299 epoch total loss 1.28393924\n",
      "Trained batch 1070 batch loss 1.24257624 epoch total loss 1.2839005\n",
      "Trained batch 1071 batch loss 1.28157723 epoch total loss 1.28389835\n",
      "Trained batch 1072 batch loss 1.2903266 epoch total loss 1.28390431\n",
      "Trained batch 1073 batch loss 1.29012251 epoch total loss 1.28391016\n",
      "Trained batch 1074 batch loss 1.30151415 epoch total loss 1.28392661\n",
      "Trained batch 1075 batch loss 1.22603619 epoch total loss 1.28387272\n",
      "Trained batch 1076 batch loss 1.21865 epoch total loss 1.28381217\n",
      "Trained batch 1077 batch loss 1.3223592 epoch total loss 1.28384793\n",
      "Trained batch 1078 batch loss 1.08333635 epoch total loss 1.28366196\n",
      "Trained batch 1079 batch loss 1.26189888 epoch total loss 1.2836417\n",
      "Trained batch 1080 batch loss 1.24968553 epoch total loss 1.28361022\n",
      "Trained batch 1081 batch loss 1.23905683 epoch total loss 1.28356898\n",
      "Trained batch 1082 batch loss 1.22629571 epoch total loss 1.28351605\n",
      "Trained batch 1083 batch loss 1.20737553 epoch total loss 1.28344584\n",
      "Trained batch 1084 batch loss 1.18900406 epoch total loss 1.28335869\n",
      "Trained batch 1085 batch loss 1.2841835 epoch total loss 1.28335941\n",
      "Trained batch 1086 batch loss 1.28015447 epoch total loss 1.28335643\n",
      "Trained batch 1087 batch loss 1.24838829 epoch total loss 1.28332424\n",
      "Trained batch 1088 batch loss 1.30858612 epoch total loss 1.28334749\n",
      "Trained batch 1089 batch loss 1.32331312 epoch total loss 1.28338432\n",
      "Trained batch 1090 batch loss 1.25394726 epoch total loss 1.28335726\n",
      "Trained batch 1091 batch loss 1.29888499 epoch total loss 1.28337145\n",
      "Trained batch 1092 batch loss 1.24246013 epoch total loss 1.2833339\n",
      "Trained batch 1093 batch loss 1.31591046 epoch total loss 1.2833637\n",
      "Trained batch 1094 batch loss 1.45878708 epoch total loss 1.28352404\n",
      "Trained batch 1095 batch loss 1.32994342 epoch total loss 1.28356647\n",
      "Trained batch 1096 batch loss 1.3360455 epoch total loss 1.28361428\n",
      "Trained batch 1097 batch loss 1.30618978 epoch total loss 1.2836349\n",
      "Trained batch 1098 batch loss 1.23732567 epoch total loss 1.2835927\n",
      "Trained batch 1099 batch loss 1.16055918 epoch total loss 1.28348064\n",
      "Trained batch 1100 batch loss 1.31906569 epoch total loss 1.28351307\n",
      "Trained batch 1101 batch loss 1.22792101 epoch total loss 1.28346252\n",
      "Trained batch 1102 batch loss 1.23284292 epoch total loss 1.28341663\n",
      "Trained batch 1103 batch loss 1.32543159 epoch total loss 1.28345466\n",
      "Trained batch 1104 batch loss 1.29955268 epoch total loss 1.28346932\n",
      "Trained batch 1105 batch loss 1.34854805 epoch total loss 1.28352809\n",
      "Trained batch 1106 batch loss 1.26891088 epoch total loss 1.28351498\n",
      "Trained batch 1107 batch loss 1.27531576 epoch total loss 1.28350747\n",
      "Trained batch 1108 batch loss 1.27526367 epoch total loss 1.28350008\n",
      "Trained batch 1109 batch loss 1.22984016 epoch total loss 1.28345168\n",
      "Trained batch 1110 batch loss 1.27500129 epoch total loss 1.28344405\n",
      "Trained batch 1111 batch loss 1.31531262 epoch total loss 1.28347278\n",
      "Trained batch 1112 batch loss 1.30047297 epoch total loss 1.28348804\n",
      "Trained batch 1113 batch loss 1.33914781 epoch total loss 1.28353798\n",
      "Trained batch 1114 batch loss 1.21942031 epoch total loss 1.28348041\n",
      "Trained batch 1115 batch loss 1.17512393 epoch total loss 1.28338325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1116 batch loss 1.24913287 epoch total loss 1.28335249\n",
      "Trained batch 1117 batch loss 1.28461933 epoch total loss 1.28335369\n",
      "Trained batch 1118 batch loss 1.42922533 epoch total loss 1.28348422\n",
      "Trained batch 1119 batch loss 1.25049424 epoch total loss 1.28345466\n",
      "Trained batch 1120 batch loss 1.31828618 epoch total loss 1.28348577\n",
      "Trained batch 1121 batch loss 1.32497895 epoch total loss 1.28352273\n",
      "Trained batch 1122 batch loss 1.29443622 epoch total loss 1.2835325\n",
      "Trained batch 1123 batch loss 1.18226194 epoch total loss 1.28344226\n",
      "Trained batch 1124 batch loss 1.07953656 epoch total loss 1.28326094\n",
      "Trained batch 1125 batch loss 1.31159651 epoch total loss 1.28328609\n",
      "Trained batch 1126 batch loss 1.3285768 epoch total loss 1.28332639\n",
      "Trained batch 1127 batch loss 1.32753825 epoch total loss 1.28336561\n",
      "Trained batch 1128 batch loss 1.36813068 epoch total loss 1.28344083\n",
      "Trained batch 1129 batch loss 1.50742269 epoch total loss 1.28363919\n",
      "Trained batch 1130 batch loss 1.31738067 epoch total loss 1.28366899\n",
      "Trained batch 1131 batch loss 1.2877419 epoch total loss 1.28367257\n",
      "Trained batch 1132 batch loss 1.51240838 epoch total loss 1.28387475\n",
      "Trained batch 1133 batch loss 1.28858113 epoch total loss 1.28387892\n",
      "Trained batch 1134 batch loss 1.27263522 epoch total loss 1.28386891\n",
      "Trained batch 1135 batch loss 1.18970776 epoch total loss 1.28378594\n",
      "Trained batch 1136 batch loss 1.33027482 epoch total loss 1.28382695\n",
      "Trained batch 1137 batch loss 1.20066 epoch total loss 1.28375375\n",
      "Trained batch 1138 batch loss 1.33388698 epoch total loss 1.28379786\n",
      "Trained batch 1139 batch loss 1.2484839 epoch total loss 1.28376687\n",
      "Trained batch 1140 batch loss 1.36357808 epoch total loss 1.28383684\n",
      "Trained batch 1141 batch loss 1.32655573 epoch total loss 1.28387427\n",
      "Trained batch 1142 batch loss 1.32385635 epoch total loss 1.2839092\n",
      "Trained batch 1143 batch loss 1.35615575 epoch total loss 1.2839725\n",
      "Trained batch 1144 batch loss 1.22854769 epoch total loss 1.28392398\n",
      "Trained batch 1145 batch loss 1.21456969 epoch total loss 1.28386343\n",
      "Trained batch 1146 batch loss 1.16104794 epoch total loss 1.28375626\n",
      "Trained batch 1147 batch loss 1.16331351 epoch total loss 1.28365123\n",
      "Trained batch 1148 batch loss 1.30903614 epoch total loss 1.28367341\n",
      "Trained batch 1149 batch loss 1.25690794 epoch total loss 1.28365016\n",
      "Trained batch 1150 batch loss 1.35370183 epoch total loss 1.28371119\n",
      "Trained batch 1151 batch loss 1.3130455 epoch total loss 1.28373659\n",
      "Trained batch 1152 batch loss 1.19534767 epoch total loss 1.28365982\n",
      "Trained batch 1153 batch loss 1.15523183 epoch total loss 1.28354847\n",
      "Trained batch 1154 batch loss 1.27700341 epoch total loss 1.28354275\n",
      "Trained batch 1155 batch loss 1.33111775 epoch total loss 1.283584\n",
      "Trained batch 1156 batch loss 1.3709079 epoch total loss 1.28365946\n",
      "Trained batch 1157 batch loss 1.25313592 epoch total loss 1.28363311\n",
      "Trained batch 1158 batch loss 1.20522308 epoch total loss 1.2835654\n",
      "Trained batch 1159 batch loss 1.21159673 epoch total loss 1.28350329\n",
      "Trained batch 1160 batch loss 1.28393137 epoch total loss 1.28350365\n",
      "Trained batch 1161 batch loss 1.33953762 epoch total loss 1.28355193\n",
      "Trained batch 1162 batch loss 1.2627461 epoch total loss 1.28353393\n",
      "Trained batch 1163 batch loss 1.23731613 epoch total loss 1.28349423\n",
      "Trained batch 1164 batch loss 1.24140131 epoch total loss 1.28345811\n",
      "Trained batch 1165 batch loss 1.32145357 epoch total loss 1.28349066\n",
      "Trained batch 1166 batch loss 1.24707472 epoch total loss 1.28345942\n",
      "Trained batch 1167 batch loss 1.38131809 epoch total loss 1.28354335\n",
      "Trained batch 1168 batch loss 1.24620473 epoch total loss 1.28351128\n",
      "Trained batch 1169 batch loss 1.46746409 epoch total loss 1.28366864\n",
      "Trained batch 1170 batch loss 1.39328492 epoch total loss 1.28376234\n",
      "Trained batch 1171 batch loss 1.25091386 epoch total loss 1.2837342\n",
      "Trained batch 1172 batch loss 1.17717922 epoch total loss 1.28364325\n",
      "Trained batch 1173 batch loss 1.17532492 epoch total loss 1.28355086\n",
      "Trained batch 1174 batch loss 1.36366057 epoch total loss 1.28361917\n",
      "Trained batch 1175 batch loss 1.24237204 epoch total loss 1.28358412\n",
      "Trained batch 1176 batch loss 1.2805717 epoch total loss 1.2835815\n",
      "Trained batch 1177 batch loss 1.18393302 epoch total loss 1.28349686\n",
      "Trained batch 1178 batch loss 1.20905817 epoch total loss 1.28343368\n",
      "Trained batch 1179 batch loss 1.17961323 epoch total loss 1.28334558\n",
      "Trained batch 1180 batch loss 1.13300753 epoch total loss 1.28321826\n",
      "Trained batch 1181 batch loss 1.21327806 epoch total loss 1.28315902\n",
      "Trained batch 1182 batch loss 1.32344425 epoch total loss 1.28319311\n",
      "Trained batch 1183 batch loss 1.15576065 epoch total loss 1.28308535\n",
      "Trained batch 1184 batch loss 1.16919303 epoch total loss 1.28298914\n",
      "Trained batch 1185 batch loss 1.18170381 epoch total loss 1.28290379\n",
      "Trained batch 1186 batch loss 1.29329681 epoch total loss 1.28291261\n",
      "Trained batch 1187 batch loss 1.37446499 epoch total loss 1.28298974\n",
      "Trained batch 1188 batch loss 1.27367175 epoch total loss 1.28298187\n",
      "Trained batch 1189 batch loss 1.29480898 epoch total loss 1.28299189\n",
      "Trained batch 1190 batch loss 1.04861879 epoch total loss 1.28279483\n",
      "Trained batch 1191 batch loss 0.969407499 epoch total loss 1.28253174\n",
      "Trained batch 1192 batch loss 1.00643539 epoch total loss 1.28230011\n",
      "Trained batch 1193 batch loss 1.14778078 epoch total loss 1.28218734\n",
      "Trained batch 1194 batch loss 1.17468929 epoch total loss 1.28209734\n",
      "Trained batch 1195 batch loss 1.24524736 epoch total loss 1.28206646\n",
      "Trained batch 1196 batch loss 1.22120905 epoch total loss 1.28201556\n",
      "Trained batch 1197 batch loss 1.06753647 epoch total loss 1.28183639\n",
      "Trained batch 1198 batch loss 1.08804703 epoch total loss 1.28167462\n",
      "Trained batch 1199 batch loss 1.08815622 epoch total loss 1.28151321\n",
      "Trained batch 1200 batch loss 1.27834451 epoch total loss 1.28151047\n",
      "Trained batch 1201 batch loss 1.36704302 epoch total loss 1.28158176\n",
      "Trained batch 1202 batch loss 1.54808831 epoch total loss 1.28180349\n",
      "Trained batch 1203 batch loss 1.48801541 epoch total loss 1.28197491\n",
      "Trained batch 1204 batch loss 1.3284018 epoch total loss 1.28201342\n",
      "Trained batch 1205 batch loss 1.38002443 epoch total loss 1.28209472\n",
      "Trained batch 1206 batch loss 1.2310698 epoch total loss 1.28205252\n",
      "Trained batch 1207 batch loss 1.14893627 epoch total loss 1.28194213\n",
      "Trained batch 1208 batch loss 1.22365427 epoch total loss 1.28189385\n",
      "Trained batch 1209 batch loss 1.22845399 epoch total loss 1.28184962\n",
      "Trained batch 1210 batch loss 1.34013677 epoch total loss 1.28189778\n",
      "Trained batch 1211 batch loss 1.34925 epoch total loss 1.28195333\n",
      "Trained batch 1212 batch loss 1.29710138 epoch total loss 1.28196585\n",
      "Trained batch 1213 batch loss 1.2241317 epoch total loss 1.28191817\n",
      "Trained batch 1214 batch loss 1.35690236 epoch total loss 1.28198\n",
      "Trained batch 1215 batch loss 1.21498716 epoch total loss 1.28192484\n",
      "Trained batch 1216 batch loss 1.26573837 epoch total loss 1.28191149\n",
      "Trained batch 1217 batch loss 1.26897776 epoch total loss 1.28190088\n",
      "Trained batch 1218 batch loss 1.19888103 epoch total loss 1.2818327\n",
      "Trained batch 1219 batch loss 1.14661551 epoch total loss 1.28172171\n",
      "Trained batch 1220 batch loss 1.12895799 epoch total loss 1.28159654\n",
      "Trained batch 1221 batch loss 1.18931615 epoch total loss 1.28152096\n",
      "Trained batch 1222 batch loss 1.11054504 epoch total loss 1.28138101\n",
      "Trained batch 1223 batch loss 1.09509325 epoch total loss 1.28122878\n",
      "Trained batch 1224 batch loss 1.17261446 epoch total loss 1.28114\n",
      "Trained batch 1225 batch loss 1.2578578 epoch total loss 1.2811209\n",
      "Trained batch 1226 batch loss 1.1084311 epoch total loss 1.28098011\n",
      "Trained batch 1227 batch loss 1.30320275 epoch total loss 1.28099823\n",
      "Trained batch 1228 batch loss 1.21658087 epoch total loss 1.28094578\n",
      "Trained batch 1229 batch loss 1.26919496 epoch total loss 1.28093612\n",
      "Trained batch 1230 batch loss 1.29992628 epoch total loss 1.28095162\n",
      "Trained batch 1231 batch loss 1.36204064 epoch total loss 1.28101742\n",
      "Trained batch 1232 batch loss 1.33452451 epoch total loss 1.28106081\n",
      "Trained batch 1233 batch loss 1.27854609 epoch total loss 1.28105879\n",
      "Trained batch 1234 batch loss 1.23481345 epoch total loss 1.28102136\n",
      "Trained batch 1235 batch loss 1.23976994 epoch total loss 1.28098798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1236 batch loss 1.1776762 epoch total loss 1.28090441\n",
      "Trained batch 1237 batch loss 1.40914059 epoch total loss 1.28100812\n",
      "Trained batch 1238 batch loss 1.3957001 epoch total loss 1.28110087\n",
      "Trained batch 1239 batch loss 1.35691941 epoch total loss 1.28116202\n",
      "Trained batch 1240 batch loss 1.26603496 epoch total loss 1.28114974\n",
      "Trained batch 1241 batch loss 1.28572989 epoch total loss 1.28115356\n",
      "Trained batch 1242 batch loss 1.24480069 epoch total loss 1.28112423\n",
      "Trained batch 1243 batch loss 1.22133923 epoch total loss 1.28107607\n",
      "Trained batch 1244 batch loss 1.24090743 epoch total loss 1.28104389\n",
      "Trained batch 1245 batch loss 1.22081113 epoch total loss 1.28099549\n",
      "Trained batch 1246 batch loss 1.4191047 epoch total loss 1.28110623\n",
      "Trained batch 1247 batch loss 1.18100023 epoch total loss 1.28102601\n",
      "Trained batch 1248 batch loss 1.24944639 epoch total loss 1.28100073\n",
      "Trained batch 1249 batch loss 1.32750416 epoch total loss 1.28103793\n",
      "Trained batch 1250 batch loss 1.14107656 epoch total loss 1.28092599\n",
      "Trained batch 1251 batch loss 1.2137599 epoch total loss 1.28087223\n",
      "Trained batch 1252 batch loss 1.27392578 epoch total loss 1.28086674\n",
      "Trained batch 1253 batch loss 1.14336884 epoch total loss 1.28075695\n",
      "Trained batch 1254 batch loss 1.23100638 epoch total loss 1.28071725\n",
      "Trained batch 1255 batch loss 1.15787137 epoch total loss 1.28061926\n",
      "Trained batch 1256 batch loss 1.16212344 epoch total loss 1.28052497\n",
      "Trained batch 1257 batch loss 1.27525306 epoch total loss 1.2805208\n",
      "Trained batch 1258 batch loss 1.28058803 epoch total loss 1.28052092\n",
      "Trained batch 1259 batch loss 1.28841615 epoch total loss 1.28052723\n",
      "Trained batch 1260 batch loss 1.3390584 epoch total loss 1.28057373\n",
      "Trained batch 1261 batch loss 1.33709478 epoch total loss 1.28061843\n",
      "Trained batch 1262 batch loss 1.30039215 epoch total loss 1.28063416\n",
      "Trained batch 1263 batch loss 1.21459162 epoch total loss 1.28058183\n",
      "Trained batch 1264 batch loss 0.98556304 epoch total loss 1.28034842\n",
      "Trained batch 1265 batch loss 1.20404124 epoch total loss 1.28028822\n",
      "Trained batch 1266 batch loss 1.26524007 epoch total loss 1.2802763\n",
      "Trained batch 1267 batch loss 1.33030128 epoch total loss 1.28031588\n",
      "Trained batch 1268 batch loss 1.35702848 epoch total loss 1.28037632\n",
      "Trained batch 1269 batch loss 1.36366761 epoch total loss 1.280442\n",
      "Trained batch 1270 batch loss 1.41576076 epoch total loss 1.28054857\n",
      "Trained batch 1271 batch loss 1.32563376 epoch total loss 1.2805841\n",
      "Trained batch 1272 batch loss 1.33026838 epoch total loss 1.2806232\n",
      "Trained batch 1273 batch loss 1.23767149 epoch total loss 1.28058934\n",
      "Trained batch 1274 batch loss 1.2943058 epoch total loss 1.28060019\n",
      "Trained batch 1275 batch loss 1.39492834 epoch total loss 1.28068984\n",
      "Trained batch 1276 batch loss 1.37579656 epoch total loss 1.28076434\n",
      "Trained batch 1277 batch loss 1.4237318 epoch total loss 1.28087628\n",
      "Trained batch 1278 batch loss 1.21748829 epoch total loss 1.28082681\n",
      "Trained batch 1279 batch loss 1.32847285 epoch total loss 1.280864\n",
      "Trained batch 1280 batch loss 1.08625698 epoch total loss 1.28071201\n",
      "Trained batch 1281 batch loss 1.08436489 epoch total loss 1.28055871\n",
      "Trained batch 1282 batch loss 1.18232024 epoch total loss 1.28048217\n",
      "Trained batch 1283 batch loss 1.14666331 epoch total loss 1.28037786\n",
      "Trained batch 1284 batch loss 1.14680934 epoch total loss 1.2802738\n",
      "Trained batch 1285 batch loss 1.20707488 epoch total loss 1.28021681\n",
      "Trained batch 1286 batch loss 1.12362146 epoch total loss 1.2800951\n",
      "Trained batch 1287 batch loss 1.28865492 epoch total loss 1.28010178\n",
      "Trained batch 1288 batch loss 1.23918581 epoch total loss 1.28007\n",
      "Trained batch 1289 batch loss 1.1619122 epoch total loss 1.27997828\n",
      "Trained batch 1290 batch loss 1.28031886 epoch total loss 1.27997851\n",
      "Trained batch 1291 batch loss 1.10926628 epoch total loss 1.27984619\n",
      "Trained batch 1292 batch loss 1.17409444 epoch total loss 1.27976441\n",
      "Trained batch 1293 batch loss 1.11633682 epoch total loss 1.27963793\n",
      "Trained batch 1294 batch loss 1.25270176 epoch total loss 1.27961719\n",
      "Trained batch 1295 batch loss 1.30800259 epoch total loss 1.27963901\n",
      "Trained batch 1296 batch loss 1.36100268 epoch total loss 1.27970183\n",
      "Trained batch 1297 batch loss 1.45293617 epoch total loss 1.27983534\n",
      "Trained batch 1298 batch loss 1.38435471 epoch total loss 1.27991593\n",
      "Trained batch 1299 batch loss 1.21905613 epoch total loss 1.27986908\n",
      "Trained batch 1300 batch loss 1.19983125 epoch total loss 1.27980745\n",
      "Trained batch 1301 batch loss 1.24271321 epoch total loss 1.27977896\n",
      "Trained batch 1302 batch loss 1.20883751 epoch total loss 1.27972448\n",
      "Trained batch 1303 batch loss 1.21441531 epoch total loss 1.27967429\n",
      "Trained batch 1304 batch loss 1.28541231 epoch total loss 1.2796787\n",
      "Trained batch 1305 batch loss 1.28639221 epoch total loss 1.27968383\n",
      "Trained batch 1306 batch loss 1.17212141 epoch total loss 1.27960145\n",
      "Trained batch 1307 batch loss 1.20616865 epoch total loss 1.27954531\n",
      "Trained batch 1308 batch loss 1.17088366 epoch total loss 1.27946222\n",
      "Trained batch 1309 batch loss 1.24232316 epoch total loss 1.27943385\n",
      "Trained batch 1310 batch loss 1.22958684 epoch total loss 1.27939582\n",
      "Trained batch 1311 batch loss 1.25375915 epoch total loss 1.27937627\n",
      "Trained batch 1312 batch loss 1.34686351 epoch total loss 1.27942777\n",
      "Trained batch 1313 batch loss 1.26775026 epoch total loss 1.27941883\n",
      "Trained batch 1314 batch loss 1.29667068 epoch total loss 1.27943194\n",
      "Trained batch 1315 batch loss 1.31357491 epoch total loss 1.27945793\n",
      "Trained batch 1316 batch loss 1.18703139 epoch total loss 1.27938771\n",
      "Trained batch 1317 batch loss 1.28060234 epoch total loss 1.27938867\n",
      "Trained batch 1318 batch loss 1.33724773 epoch total loss 1.27943254\n",
      "Trained batch 1319 batch loss 1.2361331 epoch total loss 1.27939975\n",
      "Trained batch 1320 batch loss 1.43520916 epoch total loss 1.27951777\n",
      "Trained batch 1321 batch loss 1.11442423 epoch total loss 1.27939272\n",
      "Trained batch 1322 batch loss 1.24493682 epoch total loss 1.27936673\n",
      "Trained batch 1323 batch loss 1.28082669 epoch total loss 1.2793678\n",
      "Trained batch 1324 batch loss 1.20105577 epoch total loss 1.27930868\n",
      "Trained batch 1325 batch loss 1.1550231 epoch total loss 1.27921486\n",
      "Trained batch 1326 batch loss 1.32371402 epoch total loss 1.27924848\n",
      "Trained batch 1327 batch loss 1.19236827 epoch total loss 1.27918303\n",
      "Trained batch 1328 batch loss 1.32827199 epoch total loss 1.27922\n",
      "Trained batch 1329 batch loss 1.33066261 epoch total loss 1.27925861\n",
      "Trained batch 1330 batch loss 1.15919316 epoch total loss 1.27916837\n",
      "Trained batch 1331 batch loss 1.23099279 epoch total loss 1.27913213\n",
      "Trained batch 1332 batch loss 1.23997188 epoch total loss 1.2791028\n",
      "Trained batch 1333 batch loss 1.26184916 epoch total loss 1.27908981\n",
      "Trained batch 1334 batch loss 1.19333756 epoch total loss 1.27902555\n",
      "Trained batch 1335 batch loss 1.09640718 epoch total loss 1.27888882\n",
      "Trained batch 1336 batch loss 1.19168305 epoch total loss 1.27882349\n",
      "Trained batch 1337 batch loss 1.36838007 epoch total loss 1.27889049\n",
      "Trained batch 1338 batch loss 1.27294207 epoch total loss 1.27888608\n",
      "Trained batch 1339 batch loss 1.21530759 epoch total loss 1.27883863\n",
      "Trained batch 1340 batch loss 1.18211722 epoch total loss 1.27876639\n",
      "Trained batch 1341 batch loss 1.08614957 epoch total loss 1.27862275\n",
      "Trained batch 1342 batch loss 1.045735 epoch total loss 1.2784493\n",
      "Trained batch 1343 batch loss 1.08201051 epoch total loss 1.27830303\n",
      "Trained batch 1344 batch loss 1.14246058 epoch total loss 1.27820194\n",
      "Trained batch 1345 batch loss 1.24435258 epoch total loss 1.27817678\n",
      "Trained batch 1346 batch loss 1.36090565 epoch total loss 1.2782383\n",
      "Trained batch 1347 batch loss 1.20399892 epoch total loss 1.27818322\n",
      "Trained batch 1348 batch loss 0.998863637 epoch total loss 1.27797604\n",
      "Trained batch 1349 batch loss 1.10100532 epoch total loss 1.27784479\n",
      "Trained batch 1350 batch loss 1.12377715 epoch total loss 1.2777307\n",
      "Trained batch 1351 batch loss 1.18676472 epoch total loss 1.27766335\n",
      "Trained batch 1352 batch loss 1.26659584 epoch total loss 1.27765512\n",
      "Trained batch 1353 batch loss 1.23582828 epoch total loss 1.27762425\n",
      "Trained batch 1354 batch loss 1.28054762 epoch total loss 1.2776264\n",
      "Trained batch 1355 batch loss 1.20687318 epoch total loss 1.27757418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1356 batch loss 1.2919569 epoch total loss 1.27758479\n",
      "Trained batch 1357 batch loss 1.27095675 epoch total loss 1.27758\n",
      "Trained batch 1358 batch loss 1.45076656 epoch total loss 1.27770758\n",
      "Trained batch 1359 batch loss 1.47704875 epoch total loss 1.2778542\n",
      "Trained batch 1360 batch loss 1.6480304 epoch total loss 1.27812648\n",
      "Trained batch 1361 batch loss 1.33959246 epoch total loss 1.27817166\n",
      "Trained batch 1362 batch loss 1.09339929 epoch total loss 1.27803588\n",
      "Trained batch 1363 batch loss 1.27439 epoch total loss 1.27803326\n",
      "Trained batch 1364 batch loss 1.43746805 epoch total loss 1.2781502\n",
      "Trained batch 1365 batch loss 1.35426307 epoch total loss 1.27820599\n",
      "Trained batch 1366 batch loss 1.36812663 epoch total loss 1.27827179\n",
      "Trained batch 1367 batch loss 1.38297665 epoch total loss 1.27834833\n",
      "Trained batch 1368 batch loss 1.2217859 epoch total loss 1.27830696\n",
      "Trained batch 1369 batch loss 1.32582581 epoch total loss 1.27834177\n",
      "Trained batch 1370 batch loss 1.17868888 epoch total loss 1.27826893\n",
      "Trained batch 1371 batch loss 1.28493571 epoch total loss 1.27827382\n",
      "Trained batch 1372 batch loss 1.27903581 epoch total loss 1.27827442\n",
      "Trained batch 1373 batch loss 1.18954158 epoch total loss 1.27820981\n",
      "Trained batch 1374 batch loss 1.2583921 epoch total loss 1.27819538\n",
      "Trained batch 1375 batch loss 1.31522834 epoch total loss 1.27822232\n",
      "Trained batch 1376 batch loss 1.37942815 epoch total loss 1.27829587\n",
      "Trained batch 1377 batch loss 1.37403858 epoch total loss 1.27836537\n",
      "Trained batch 1378 batch loss 1.38935935 epoch total loss 1.27844596\n",
      "Trained batch 1379 batch loss 1.35715318 epoch total loss 1.27850306\n",
      "Trained batch 1380 batch loss 1.28113914 epoch total loss 1.27850497\n",
      "Trained batch 1381 batch loss 1.36705291 epoch total loss 1.2785691\n",
      "Trained batch 1382 batch loss 1.24599266 epoch total loss 1.2785455\n",
      "Trained batch 1383 batch loss 1.3694998 epoch total loss 1.27861118\n",
      "Trained batch 1384 batch loss 1.27960348 epoch total loss 1.27861202\n",
      "Trained batch 1385 batch loss 1.20263648 epoch total loss 1.27855718\n",
      "Trained batch 1386 batch loss 1.19398403 epoch total loss 1.27849615\n",
      "Trained batch 1387 batch loss 1.06834757 epoch total loss 1.27834463\n",
      "Trained batch 1388 batch loss 1.19605124 epoch total loss 1.27828526\n",
      "Epoch 3 train loss 1.278285264968872\n",
      "Validated batch 1 batch loss 1.16986394\n",
      "Validated batch 2 batch loss 1.22645926\n",
      "Validated batch 3 batch loss 1.29301262\n",
      "Validated batch 4 batch loss 1.19387734\n",
      "Validated batch 5 batch loss 1.33347011\n",
      "Validated batch 6 batch loss 1.36172533\n",
      "Validated batch 7 batch loss 1.11861265\n",
      "Validated batch 8 batch loss 1.26282883\n",
      "Validated batch 9 batch loss 1.20459914\n",
      "Validated batch 10 batch loss 1.34442329\n",
      "Validated batch 11 batch loss 1.25495887\n",
      "Validated batch 12 batch loss 1.08893466\n",
      "Validated batch 13 batch loss 1.17067385\n",
      "Validated batch 14 batch loss 1.20042217\n",
      "Validated batch 15 batch loss 1.19019818\n",
      "Validated batch 16 batch loss 1.22007489\n",
      "Validated batch 17 batch loss 1.19681323\n",
      "Validated batch 18 batch loss 1.16702104\n",
      "Validated batch 19 batch loss 1.26979935\n",
      "Validated batch 20 batch loss 1.30126131\n",
      "Validated batch 21 batch loss 1.32218885\n",
      "Validated batch 22 batch loss 1.25013149\n",
      "Validated batch 23 batch loss 1.17147255\n",
      "Validated batch 24 batch loss 1.20179129\n",
      "Validated batch 25 batch loss 1.20783973\n",
      "Validated batch 26 batch loss 1.20978105\n",
      "Validated batch 27 batch loss 1.17452693\n",
      "Validated batch 28 batch loss 1.21927202\n",
      "Validated batch 29 batch loss 1.30590189\n",
      "Validated batch 30 batch loss 1.2221328\n",
      "Validated batch 31 batch loss 1.08322191\n",
      "Validated batch 32 batch loss 1.18163645\n",
      "Validated batch 33 batch loss 1.21094561\n",
      "Validated batch 34 batch loss 1.17585\n",
      "Validated batch 35 batch loss 1.22037208\n",
      "Validated batch 36 batch loss 1.23250413\n",
      "Validated batch 37 batch loss 1.20276928\n",
      "Validated batch 38 batch loss 1.34143877\n",
      "Validated batch 39 batch loss 1.29546213\n",
      "Validated batch 40 batch loss 1.22523665\n",
      "Validated batch 41 batch loss 1.33268321\n",
      "Validated batch 42 batch loss 1.00973105\n",
      "Validated batch 43 batch loss 1.13704145\n",
      "Validated batch 44 batch loss 1.11632609\n",
      "Validated batch 45 batch loss 1.23139572\n",
      "Validated batch 46 batch loss 1.37557805\n",
      "Validated batch 47 batch loss 1.29488122\n",
      "Validated batch 48 batch loss 1.27577889\n",
      "Validated batch 49 batch loss 1.17006254\n",
      "Validated batch 50 batch loss 1.19197106\n",
      "Validated batch 51 batch loss 1.23373365\n",
      "Validated batch 52 batch loss 1.25212884\n",
      "Validated batch 53 batch loss 1.20295417\n",
      "Validated batch 54 batch loss 1.2386328\n",
      "Validated batch 55 batch loss 1.24592519\n",
      "Validated batch 56 batch loss 1.29851544\n",
      "Validated batch 57 batch loss 1.15891218\n",
      "Validated batch 58 batch loss 1.10395622\n",
      "Validated batch 59 batch loss 1.32706\n",
      "Validated batch 60 batch loss 1.28765798\n",
      "Validated batch 61 batch loss 1.37144792\n",
      "Validated batch 62 batch loss 1.36674023\n",
      "Validated batch 63 batch loss 1.21735895\n",
      "Validated batch 64 batch loss 1.43767476\n",
      "Validated batch 65 batch loss 1.20145702\n",
      "Validated batch 66 batch loss 1.33065522\n",
      "Validated batch 67 batch loss 1.27377248\n",
      "Validated batch 68 batch loss 1.05836964\n",
      "Validated batch 69 batch loss 1.2736789\n",
      "Validated batch 70 batch loss 1.33751488\n",
      "Validated batch 71 batch loss 1.23161268\n",
      "Validated batch 72 batch loss 1.2471385\n",
      "Validated batch 73 batch loss 1.19662428\n",
      "Validated batch 74 batch loss 1.2937417\n",
      "Validated batch 75 batch loss 1.43251514\n",
      "Validated batch 76 batch loss 1.20221877\n",
      "Validated batch 77 batch loss 1.3443768\n",
      "Validated batch 78 batch loss 1.22580552\n",
      "Validated batch 79 batch loss 1.2870214\n",
      "Validated batch 80 batch loss 1.24200845\n",
      "Validated batch 81 batch loss 1.14377427\n",
      "Validated batch 82 batch loss 1.4034034\n",
      "Validated batch 83 batch loss 1.26390171\n",
      "Validated batch 84 batch loss 1.30765009\n",
      "Validated batch 85 batch loss 1.24117863\n",
      "Validated batch 86 batch loss 1.31980908\n",
      "Validated batch 87 batch loss 1.18650186\n",
      "Validated batch 88 batch loss 1.26169717\n",
      "Validated batch 89 batch loss 1.21724033\n",
      "Validated batch 90 batch loss 1.22777963\n",
      "Validated batch 91 batch loss 1.28441858\n",
      "Validated batch 92 batch loss 1.23713768\n",
      "Validated batch 93 batch loss 1.29636931\n",
      "Validated batch 94 batch loss 1.2786262\n",
      "Validated batch 95 batch loss 1.21755862\n",
      "Validated batch 96 batch loss 1.19791877\n",
      "Validated batch 97 batch loss 1.27832568\n",
      "Validated batch 98 batch loss 1.27478099\n",
      "Validated batch 99 batch loss 1.34721565\n",
      "Validated batch 100 batch loss 1.31048346\n",
      "Validated batch 101 batch loss 1.27002025\n",
      "Validated batch 102 batch loss 1.20992088\n",
      "Validated batch 103 batch loss 1.24922597\n",
      "Validated batch 104 batch loss 1.25849009\n",
      "Validated batch 105 batch loss 1.25164318\n",
      "Validated batch 106 batch loss 1.35868382\n",
      "Validated batch 107 batch loss 1.30402362\n",
      "Validated batch 108 batch loss 1.31658602\n",
      "Validated batch 109 batch loss 1.38255477\n",
      "Validated batch 110 batch loss 1.13389635\n",
      "Validated batch 111 batch loss 1.24499631\n",
      "Validated batch 112 batch loss 1.15665913\n",
      "Validated batch 113 batch loss 1.19847584\n",
      "Validated batch 114 batch loss 1.36447847\n",
      "Validated batch 115 batch loss 1.15254784\n",
      "Validated batch 116 batch loss 1.26931727\n",
      "Validated batch 117 batch loss 1.10583889\n",
      "Validated batch 118 batch loss 1.26841545\n",
      "Validated batch 119 batch loss 1.1639123\n",
      "Validated batch 120 batch loss 1.18285716\n",
      "Validated batch 121 batch loss 1.22311568\n",
      "Validated batch 122 batch loss 1.21445763\n",
      "Validated batch 123 batch loss 1.25899601\n",
      "Validated batch 124 batch loss 1.2991643\n",
      "Validated batch 125 batch loss 1.1457603\n",
      "Validated batch 126 batch loss 1.31761849\n",
      "Validated batch 127 batch loss 1.28979754\n",
      "Validated batch 128 batch loss 1.13968849\n",
      "Validated batch 129 batch loss 1.24988437\n",
      "Validated batch 130 batch loss 1.23334527\n",
      "Validated batch 131 batch loss 1.23552454\n",
      "Validated batch 132 batch loss 1.37513232\n",
      "Validated batch 133 batch loss 1.28655756\n",
      "Validated batch 134 batch loss 1.23887444\n",
      "Validated batch 135 batch loss 1.26004076\n",
      "Validated batch 136 batch loss 1.2133193\n",
      "Validated batch 137 batch loss 1.17413831\n",
      "Validated batch 138 batch loss 1.19751906\n",
      "Validated batch 139 batch loss 1.31526601\n",
      "Validated batch 140 batch loss 1.24401546\n",
      "Validated batch 141 batch loss 1.23053\n",
      "Validated batch 142 batch loss 1.19932449\n",
      "Validated batch 143 batch loss 1.1483916\n",
      "Validated batch 144 batch loss 1.27504086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 145 batch loss 1.27213812\n",
      "Validated batch 146 batch loss 1.19996703\n",
      "Validated batch 147 batch loss 1.19620931\n",
      "Validated batch 148 batch loss 1.26680064\n",
      "Validated batch 149 batch loss 1.23605061\n",
      "Validated batch 150 batch loss 1.30867732\n",
      "Validated batch 151 batch loss 1.31138313\n",
      "Validated batch 152 batch loss 1.17837405\n",
      "Validated batch 153 batch loss 1.19649255\n",
      "Validated batch 154 batch loss 1.2575469\n",
      "Validated batch 155 batch loss 1.20528913\n",
      "Validated batch 156 batch loss 1.2993865\n",
      "Validated batch 157 batch loss 1.32114935\n",
      "Validated batch 158 batch loss 1.43767858\n",
      "Validated batch 159 batch loss 1.39069295\n",
      "Validated batch 160 batch loss 1.29567075\n",
      "Validated batch 161 batch loss 1.16069746\n",
      "Validated batch 162 batch loss 1.19761872\n",
      "Validated batch 163 batch loss 1.16227126\n",
      "Validated batch 164 batch loss 1.23909736\n",
      "Validated batch 165 batch loss 1.14888573\n",
      "Validated batch 166 batch loss 1.23392379\n",
      "Validated batch 167 batch loss 1.2787447\n",
      "Validated batch 168 batch loss 1.2577889\n",
      "Validated batch 169 batch loss 1.30824471\n",
      "Validated batch 170 batch loss 1.38884592\n",
      "Validated batch 171 batch loss 1.30735564\n",
      "Validated batch 172 batch loss 1.26282692\n",
      "Validated batch 173 batch loss 1.38914084\n",
      "Validated batch 174 batch loss 1.29316294\n",
      "Validated batch 175 batch loss 1.3255868\n",
      "Validated batch 176 batch loss 1.2999\n",
      "Validated batch 177 batch loss 1.40903878\n",
      "Validated batch 178 batch loss 1.30328894\n",
      "Validated batch 179 batch loss 1.18911052\n",
      "Validated batch 180 batch loss 1.19119573\n",
      "Validated batch 181 batch loss 1.268507\n",
      "Validated batch 182 batch loss 1.37395799\n",
      "Validated batch 183 batch loss 1.18412864\n",
      "Validated batch 184 batch loss 1.273067\n",
      "Validated batch 185 batch loss 1.20949793\n",
      "Epoch 3 val loss 1.2487211227416992\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-3-loss-1.2487.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.24777949 epoch total loss 1.24777949\n",
      "Trained batch 2 batch loss 1.386011 epoch total loss 1.31689525\n",
      "Trained batch 3 batch loss 1.35134256 epoch total loss 1.32837772\n",
      "Trained batch 4 batch loss 1.24146414 epoch total loss 1.30664933\n",
      "Trained batch 5 batch loss 1.27549219 epoch total loss 1.3004179\n",
      "Trained batch 6 batch loss 1.39013338 epoch total loss 1.31537044\n",
      "Trained batch 7 batch loss 1.17635453 epoch total loss 1.29551113\n",
      "Trained batch 8 batch loss 1.17598045 epoch total loss 1.28056979\n",
      "Trained batch 9 batch loss 1.28408957 epoch total loss 1.28096092\n",
      "Trained batch 10 batch loss 1.35807872 epoch total loss 1.28867269\n",
      "Trained batch 11 batch loss 1.35031557 epoch total loss 1.29427671\n",
      "Trained batch 12 batch loss 1.26099777 epoch total loss 1.29150343\n",
      "Trained batch 13 batch loss 1.19991732 epoch total loss 1.28445828\n",
      "Trained batch 14 batch loss 1.24664843 epoch total loss 1.28175759\n",
      "Trained batch 15 batch loss 1.21565604 epoch total loss 1.2773509\n",
      "Trained batch 16 batch loss 1.2753036 epoch total loss 1.27722287\n",
      "Trained batch 17 batch loss 1.37852132 epoch total loss 1.28318155\n",
      "Trained batch 18 batch loss 1.47567511 epoch total loss 1.29387569\n",
      "Trained batch 19 batch loss 1.34644842 epoch total loss 1.29664266\n",
      "Trained batch 20 batch loss 1.13897407 epoch total loss 1.28875923\n",
      "Trained batch 21 batch loss 1.10535789 epoch total loss 1.28002584\n",
      "Trained batch 22 batch loss 1.10140395 epoch total loss 1.27190673\n",
      "Trained batch 23 batch loss 1.24533367 epoch total loss 1.27075124\n",
      "Trained batch 24 batch loss 1.15097094 epoch total loss 1.26576042\n",
      "Trained batch 25 batch loss 1.08378947 epoch total loss 1.25848162\n",
      "Trained batch 26 batch loss 1.17897439 epoch total loss 1.25542367\n",
      "Trained batch 27 batch loss 1.25480723 epoch total loss 1.25540078\n",
      "Trained batch 28 batch loss 1.24338245 epoch total loss 1.2549715\n",
      "Trained batch 29 batch loss 1.25812709 epoch total loss 1.25508022\n",
      "Trained batch 30 batch loss 1.20723104 epoch total loss 1.2534852\n",
      "Trained batch 31 batch loss 1.30814672 epoch total loss 1.25524855\n",
      "Trained batch 32 batch loss 1.26552224 epoch total loss 1.25556958\n",
      "Trained batch 33 batch loss 1.23736775 epoch total loss 1.25501812\n",
      "Trained batch 34 batch loss 1.18141782 epoch total loss 1.25285339\n",
      "Trained batch 35 batch loss 1.22992313 epoch total loss 1.25219822\n",
      "Trained batch 36 batch loss 1.19058943 epoch total loss 1.25048685\n",
      "Trained batch 37 batch loss 1.03858161 epoch total loss 1.2447598\n",
      "Trained batch 38 batch loss 1.01076841 epoch total loss 1.23860204\n",
      "Trained batch 39 batch loss 1.16051626 epoch total loss 1.2365998\n",
      "Trained batch 40 batch loss 1.30389631 epoch total loss 1.23828232\n",
      "Trained batch 41 batch loss 1.36139417 epoch total loss 1.24128497\n",
      "Trained batch 42 batch loss 1.43554437 epoch total loss 1.24591017\n",
      "Trained batch 43 batch loss 1.40901983 epoch total loss 1.24970341\n",
      "Trained batch 44 batch loss 1.33481181 epoch total loss 1.2516377\n",
      "Trained batch 45 batch loss 1.29178309 epoch total loss 1.25252986\n",
      "Trained batch 46 batch loss 1.20319188 epoch total loss 1.25145733\n",
      "Trained batch 47 batch loss 1.40601611 epoch total loss 1.25474584\n",
      "Trained batch 48 batch loss 1.21005583 epoch total loss 1.25381482\n",
      "Trained batch 49 batch loss 1.19479525 epoch total loss 1.25261021\n",
      "Trained batch 50 batch loss 1.20300722 epoch total loss 1.25161815\n",
      "Trained batch 51 batch loss 1.28673518 epoch total loss 1.25230682\n",
      "Trained batch 52 batch loss 1.22359324 epoch total loss 1.25175464\n",
      "Trained batch 53 batch loss 1.22517264 epoch total loss 1.25125313\n",
      "Trained batch 54 batch loss 1.21300292 epoch total loss 1.25054479\n",
      "Trained batch 55 batch loss 1.25650799 epoch total loss 1.25065327\n",
      "Trained batch 56 batch loss 1.23411965 epoch total loss 1.25035799\n",
      "Trained batch 57 batch loss 1.21228361 epoch total loss 1.24969\n",
      "Trained batch 58 batch loss 1.18710923 epoch total loss 1.24861109\n",
      "Trained batch 59 batch loss 1.26239955 epoch total loss 1.24884474\n",
      "Trained batch 60 batch loss 1.32679152 epoch total loss 1.25014377\n",
      "Trained batch 61 batch loss 1.2717948 epoch total loss 1.25049877\n",
      "Trained batch 62 batch loss 1.14453602 epoch total loss 1.24878979\n",
      "Trained batch 63 batch loss 1.08477426 epoch total loss 1.24618638\n",
      "Trained batch 64 batch loss 1.29520059 epoch total loss 1.2469523\n",
      "Trained batch 65 batch loss 1.211676 epoch total loss 1.24640965\n",
      "Trained batch 66 batch loss 1.19626141 epoch total loss 1.2456497\n",
      "Trained batch 67 batch loss 1.13101816 epoch total loss 1.2439388\n",
      "Trained batch 68 batch loss 1.16635847 epoch total loss 1.24279797\n",
      "Trained batch 69 batch loss 1.19404733 epoch total loss 1.24209142\n",
      "Trained batch 70 batch loss 1.12453508 epoch total loss 1.240412\n",
      "Trained batch 71 batch loss 1.29030764 epoch total loss 1.24111474\n",
      "Trained batch 72 batch loss 1.3746506 epoch total loss 1.24296939\n",
      "Trained batch 73 batch loss 1.27020907 epoch total loss 1.24334252\n",
      "Trained batch 74 batch loss 1.27792311 epoch total loss 1.24380994\n",
      "Trained batch 75 batch loss 1.2019062 epoch total loss 1.24325109\n",
      "Trained batch 76 batch loss 1.21163702 epoch total loss 1.24283516\n",
      "Trained batch 77 batch loss 1.07629228 epoch total loss 1.24067235\n",
      "Trained batch 78 batch loss 1.21220326 epoch total loss 1.24030733\n",
      "Trained batch 79 batch loss 1.12647176 epoch total loss 1.23886645\n",
      "Trained batch 80 batch loss 1.31917393 epoch total loss 1.23987031\n",
      "Trained batch 81 batch loss 1.32026947 epoch total loss 1.24086285\n",
      "Trained batch 82 batch loss 1.27002394 epoch total loss 1.24121845\n",
      "Trained batch 83 batch loss 1.21889448 epoch total loss 1.24094951\n",
      "Trained batch 84 batch loss 1.18643332 epoch total loss 1.24030054\n",
      "Trained batch 85 batch loss 1.26498151 epoch total loss 1.24059093\n",
      "Trained batch 86 batch loss 1.06837773 epoch total loss 1.23858833\n",
      "Trained batch 87 batch loss 1.10518348 epoch total loss 1.23705506\n",
      "Trained batch 88 batch loss 0.994690597 epoch total loss 1.23430085\n",
      "Trained batch 89 batch loss 1.09674406 epoch total loss 1.2327553\n",
      "Trained batch 90 batch loss 1.11302376 epoch total loss 1.23142493\n",
      "Trained batch 91 batch loss 1.0550617 epoch total loss 1.22948682\n",
      "Trained batch 92 batch loss 1.21303988 epoch total loss 1.22930813\n",
      "Trained batch 93 batch loss 1.07713425 epoch total loss 1.22767174\n",
      "Trained batch 94 batch loss 1.1769551 epoch total loss 1.22713232\n",
      "Trained batch 95 batch loss 1.23361349 epoch total loss 1.22720051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 96 batch loss 1.13518381 epoch total loss 1.22624195\n",
      "Trained batch 97 batch loss 1.1487124 epoch total loss 1.22544277\n",
      "Trained batch 98 batch loss 1.10371423 epoch total loss 1.22420061\n",
      "Trained batch 99 batch loss 1.14879203 epoch total loss 1.22343886\n",
      "Trained batch 100 batch loss 1.21400464 epoch total loss 1.22334445\n",
      "Trained batch 101 batch loss 1.14725327 epoch total loss 1.22259116\n",
      "Trained batch 102 batch loss 1.22394276 epoch total loss 1.22260439\n",
      "Trained batch 103 batch loss 1.35060811 epoch total loss 1.22384715\n",
      "Trained batch 104 batch loss 1.48673534 epoch total loss 1.22637486\n",
      "Trained batch 105 batch loss 1.32154846 epoch total loss 1.22728121\n",
      "Trained batch 106 batch loss 1.21508384 epoch total loss 1.22716618\n",
      "Trained batch 107 batch loss 1.12345552 epoch total loss 1.226197\n",
      "Trained batch 108 batch loss 1.14893723 epoch total loss 1.22548163\n",
      "Trained batch 109 batch loss 1.18867302 epoch total loss 1.22514403\n",
      "Trained batch 110 batch loss 1.12209618 epoch total loss 1.22420728\n",
      "Trained batch 111 batch loss 1.18299747 epoch total loss 1.22383595\n",
      "Trained batch 112 batch loss 1.27960956 epoch total loss 1.22433388\n",
      "Trained batch 113 batch loss 1.24180281 epoch total loss 1.2244885\n",
      "Trained batch 114 batch loss 1.30894279 epoch total loss 1.22522938\n",
      "Trained batch 115 batch loss 1.36475503 epoch total loss 1.22644269\n",
      "Trained batch 116 batch loss 1.26377439 epoch total loss 1.22676456\n",
      "Trained batch 117 batch loss 1.4357059 epoch total loss 1.22855031\n",
      "Trained batch 118 batch loss 1.39447236 epoch total loss 1.22995639\n",
      "Trained batch 119 batch loss 1.34451926 epoch total loss 1.23091912\n",
      "Trained batch 120 batch loss 1.22159362 epoch total loss 1.23084128\n",
      "Trained batch 121 batch loss 1.2757442 epoch total loss 1.23121238\n",
      "Trained batch 122 batch loss 1.31594753 epoch total loss 1.23190701\n",
      "Trained batch 123 batch loss 1.20810676 epoch total loss 1.23171341\n",
      "Trained batch 124 batch loss 1.22385883 epoch total loss 1.23165011\n",
      "Trained batch 125 batch loss 1.14761782 epoch total loss 1.23097777\n",
      "Trained batch 126 batch loss 1.18586171 epoch total loss 1.23061979\n",
      "Trained batch 127 batch loss 1.18029749 epoch total loss 1.23022354\n",
      "Trained batch 128 batch loss 1.21362603 epoch total loss 1.23009384\n",
      "Trained batch 129 batch loss 1.16811538 epoch total loss 1.22961342\n",
      "Trained batch 130 batch loss 1.28464317 epoch total loss 1.23003674\n",
      "Trained batch 131 batch loss 1.52520597 epoch total loss 1.23228991\n",
      "Trained batch 132 batch loss 1.47924662 epoch total loss 1.23416078\n",
      "Trained batch 133 batch loss 1.22345436 epoch total loss 1.23408031\n",
      "Trained batch 134 batch loss 1.19730747 epoch total loss 1.23380589\n",
      "Trained batch 135 batch loss 1.17922401 epoch total loss 1.23340166\n",
      "Trained batch 136 batch loss 1.29525959 epoch total loss 1.23385644\n",
      "Trained batch 137 batch loss 1.27958512 epoch total loss 1.23419023\n",
      "Trained batch 138 batch loss 1.26813269 epoch total loss 1.23443615\n",
      "Trained batch 139 batch loss 1.33922672 epoch total loss 1.23519\n",
      "Trained batch 140 batch loss 1.38294196 epoch total loss 1.23624539\n",
      "Trained batch 141 batch loss 1.36973667 epoch total loss 1.23719215\n",
      "Trained batch 142 batch loss 1.42717147 epoch total loss 1.23853\n",
      "Trained batch 143 batch loss 1.25057018 epoch total loss 1.2386142\n",
      "Trained batch 144 batch loss 1.27633834 epoch total loss 1.2388761\n",
      "Trained batch 145 batch loss 1.16472852 epoch total loss 1.23836482\n",
      "Trained batch 146 batch loss 1.29406786 epoch total loss 1.23874629\n",
      "Trained batch 147 batch loss 1.27756083 epoch total loss 1.23901033\n",
      "Trained batch 148 batch loss 1.22841549 epoch total loss 1.23893869\n",
      "Trained batch 149 batch loss 1.14555514 epoch total loss 1.23831201\n",
      "Trained batch 150 batch loss 1.19820333 epoch total loss 1.2380445\n",
      "Trained batch 151 batch loss 1.17228127 epoch total loss 1.23760903\n",
      "Trained batch 152 batch loss 1.28803623 epoch total loss 1.23794079\n",
      "Trained batch 153 batch loss 1.32136893 epoch total loss 1.23848605\n",
      "Trained batch 154 batch loss 1.26450121 epoch total loss 1.23865497\n",
      "Trained batch 155 batch loss 1.29665017 epoch total loss 1.23902917\n",
      "Trained batch 156 batch loss 1.29894626 epoch total loss 1.23941326\n",
      "Trained batch 157 batch loss 1.44901609 epoch total loss 1.24074829\n",
      "Trained batch 158 batch loss 1.24391532 epoch total loss 1.24076831\n",
      "Trained batch 159 batch loss 1.21427596 epoch total loss 1.24060178\n",
      "Trained batch 160 batch loss 1.20212793 epoch total loss 1.24036133\n",
      "Trained batch 161 batch loss 1.07325554 epoch total loss 1.23932338\n",
      "Trained batch 162 batch loss 1.15379906 epoch total loss 1.2387954\n",
      "Trained batch 163 batch loss 1.10092366 epoch total loss 1.23794961\n",
      "Trained batch 164 batch loss 1.11784816 epoch total loss 1.23721719\n",
      "Trained batch 165 batch loss 0.973063588 epoch total loss 1.23561633\n",
      "Trained batch 166 batch loss 0.958638489 epoch total loss 1.23394775\n",
      "Trained batch 167 batch loss 0.945732594 epoch total loss 1.23222196\n",
      "Trained batch 168 batch loss 0.957296312 epoch total loss 1.23058546\n",
      "Trained batch 169 batch loss 1.18741977 epoch total loss 1.23033011\n",
      "Trained batch 170 batch loss 1.18917727 epoch total loss 1.230088\n",
      "Trained batch 171 batch loss 1.19649959 epoch total loss 1.22989154\n",
      "Trained batch 172 batch loss 1.16484928 epoch total loss 1.22951353\n",
      "Trained batch 173 batch loss 1.2107389 epoch total loss 1.22940493\n",
      "Trained batch 174 batch loss 1.31645179 epoch total loss 1.22990525\n",
      "Trained batch 175 batch loss 1.28012955 epoch total loss 1.2301923\n",
      "Trained batch 176 batch loss 1.2427851 epoch total loss 1.23026383\n",
      "Trained batch 177 batch loss 1.32381988 epoch total loss 1.2307924\n",
      "Trained batch 178 batch loss 1.34953475 epoch total loss 1.2314595\n",
      "Trained batch 179 batch loss 1.35717869 epoch total loss 1.23216176\n",
      "Trained batch 180 batch loss 1.26151752 epoch total loss 1.23232484\n",
      "Trained batch 181 batch loss 1.26791072 epoch total loss 1.23252153\n",
      "Trained batch 182 batch loss 1.27401423 epoch total loss 1.23274946\n",
      "Trained batch 183 batch loss 1.24788189 epoch total loss 1.23283219\n",
      "Trained batch 184 batch loss 1.24034679 epoch total loss 1.23287296\n",
      "Trained batch 185 batch loss 1.20532608 epoch total loss 1.23272407\n",
      "Trained batch 186 batch loss 1.24806499 epoch total loss 1.23280656\n",
      "Trained batch 187 batch loss 1.15981627 epoch total loss 1.23241627\n",
      "Trained batch 188 batch loss 1.15492642 epoch total loss 1.23200405\n",
      "Trained batch 189 batch loss 1.0688585 epoch total loss 1.23114085\n",
      "Trained batch 190 batch loss 1.19806921 epoch total loss 1.23096681\n",
      "Trained batch 191 batch loss 1.19699121 epoch total loss 1.23078895\n",
      "Trained batch 192 batch loss 1.32727778 epoch total loss 1.23129141\n",
      "Trained batch 193 batch loss 1.24706888 epoch total loss 1.23137319\n",
      "Trained batch 194 batch loss 1.30521512 epoch total loss 1.23175383\n",
      "Trained batch 195 batch loss 1.28235483 epoch total loss 1.23201334\n",
      "Trained batch 196 batch loss 1.2705071 epoch total loss 1.23220968\n",
      "Trained batch 197 batch loss 1.18649626 epoch total loss 1.2319777\n",
      "Trained batch 198 batch loss 1.36328769 epoch total loss 1.23264086\n",
      "Trained batch 199 batch loss 1.20190239 epoch total loss 1.23248637\n",
      "Trained batch 200 batch loss 1.11100447 epoch total loss 1.231879\n",
      "Trained batch 201 batch loss 1.22963595 epoch total loss 1.23186779\n",
      "Trained batch 202 batch loss 1.20352232 epoch total loss 1.23172748\n",
      "Trained batch 203 batch loss 1.15639818 epoch total loss 1.23135638\n",
      "Trained batch 204 batch loss 1.24270344 epoch total loss 1.23141205\n",
      "Trained batch 205 batch loss 1.22937655 epoch total loss 1.23140204\n",
      "Trained batch 206 batch loss 1.20634198 epoch total loss 1.23128045\n",
      "Trained batch 207 batch loss 1.16908717 epoch total loss 1.23097992\n",
      "Trained batch 208 batch loss 1.16025352 epoch total loss 1.23063993\n",
      "Trained batch 209 batch loss 1.1954 epoch total loss 1.23047125\n",
      "Trained batch 210 batch loss 1.219347 epoch total loss 1.23041832\n",
      "Trained batch 211 batch loss 1.21152794 epoch total loss 1.2303288\n",
      "Trained batch 212 batch loss 1.16601634 epoch total loss 1.23002541\n",
      "Trained batch 213 batch loss 1.16253233 epoch total loss 1.22970855\n",
      "Trained batch 214 batch loss 1.22921157 epoch total loss 1.22970629\n",
      "Trained batch 215 batch loss 1.15840137 epoch total loss 1.22937453\n",
      "Trained batch 216 batch loss 1.29338598 epoch total loss 1.22967088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 217 batch loss 1.2886374 epoch total loss 1.22994268\n",
      "Trained batch 218 batch loss 1.32884216 epoch total loss 1.23039627\n",
      "Trained batch 219 batch loss 1.21772408 epoch total loss 1.23033834\n",
      "Trained batch 220 batch loss 1.30399776 epoch total loss 1.23067307\n",
      "Trained batch 221 batch loss 1.20127571 epoch total loss 1.23054\n",
      "Trained batch 222 batch loss 1.31070542 epoch total loss 1.23090112\n",
      "Trained batch 223 batch loss 1.25428772 epoch total loss 1.23100603\n",
      "Trained batch 224 batch loss 1.19946682 epoch total loss 1.23086524\n",
      "Trained batch 225 batch loss 1.27944469 epoch total loss 1.23108113\n",
      "Trained batch 226 batch loss 1.32169104 epoch total loss 1.23148203\n",
      "Trained batch 227 batch loss 1.32634068 epoch total loss 1.2319\n",
      "Trained batch 228 batch loss 1.3825109 epoch total loss 1.23256052\n",
      "Trained batch 229 batch loss 1.29866719 epoch total loss 1.23284924\n",
      "Trained batch 230 batch loss 1.17624164 epoch total loss 1.23260307\n",
      "Trained batch 231 batch loss 1.16825843 epoch total loss 1.23232448\n",
      "Trained batch 232 batch loss 1.2579999 epoch total loss 1.23243511\n",
      "Trained batch 233 batch loss 1.29641771 epoch total loss 1.23270977\n",
      "Trained batch 234 batch loss 1.2171433 epoch total loss 1.23264325\n",
      "Trained batch 235 batch loss 1.2653681 epoch total loss 1.23278248\n",
      "Trained batch 236 batch loss 1.22909164 epoch total loss 1.23276687\n",
      "Trained batch 237 batch loss 1.23947513 epoch total loss 1.23279512\n",
      "Trained batch 238 batch loss 1.20422769 epoch total loss 1.23267508\n",
      "Trained batch 239 batch loss 1.27728784 epoch total loss 1.23286176\n",
      "Trained batch 240 batch loss 1.23813891 epoch total loss 1.23288369\n",
      "Trained batch 241 batch loss 1.24664235 epoch total loss 1.23294079\n",
      "Trained batch 242 batch loss 1.06826651 epoch total loss 1.23226035\n",
      "Trained batch 243 batch loss 1.12609053 epoch total loss 1.23182344\n",
      "Trained batch 244 batch loss 1.21763694 epoch total loss 1.23176539\n",
      "Trained batch 245 batch loss 1.13488746 epoch total loss 1.23137\n",
      "Trained batch 246 batch loss 1.30270123 epoch total loss 1.23165989\n",
      "Trained batch 247 batch loss 1.21054709 epoch total loss 1.23157442\n",
      "Trained batch 248 batch loss 1.24789011 epoch total loss 1.23164022\n",
      "Trained batch 249 batch loss 1.1211412 epoch total loss 1.23119652\n",
      "Trained batch 250 batch loss 1.21169615 epoch total loss 1.23111856\n",
      "Trained batch 251 batch loss 1.28300989 epoch total loss 1.23132527\n",
      "Trained batch 252 batch loss 1.2870003 epoch total loss 1.23154616\n",
      "Trained batch 253 batch loss 1.22984445 epoch total loss 1.23153949\n",
      "Trained batch 254 batch loss 1.31611884 epoch total loss 1.23187256\n",
      "Trained batch 255 batch loss 1.21428561 epoch total loss 1.23180366\n",
      "Trained batch 256 batch loss 1.08586121 epoch total loss 1.2312336\n",
      "Trained batch 257 batch loss 1.11260986 epoch total loss 1.23077202\n",
      "Trained batch 258 batch loss 1.34809792 epoch total loss 1.23122668\n",
      "Trained batch 259 batch loss 1.18034291 epoch total loss 1.23103023\n",
      "Trained batch 260 batch loss 1.19369316 epoch total loss 1.23088658\n",
      "Trained batch 261 batch loss 1.08618975 epoch total loss 1.23033214\n",
      "Trained batch 262 batch loss 1.04132652 epoch total loss 1.2296108\n",
      "Trained batch 263 batch loss 1.14791024 epoch total loss 1.22930014\n",
      "Trained batch 264 batch loss 1.15305865 epoch total loss 1.2290113\n",
      "Trained batch 265 batch loss 1.31540895 epoch total loss 1.22933733\n",
      "Trained batch 266 batch loss 1.21566892 epoch total loss 1.22928596\n",
      "Trained batch 267 batch loss 1.27993059 epoch total loss 1.22947562\n",
      "Trained batch 268 batch loss 1.35274267 epoch total loss 1.22993565\n",
      "Trained batch 269 batch loss 1.30537248 epoch total loss 1.23021603\n",
      "Trained batch 270 batch loss 1.28227973 epoch total loss 1.23040879\n",
      "Trained batch 271 batch loss 1.28283811 epoch total loss 1.23060226\n",
      "Trained batch 272 batch loss 1.1610117 epoch total loss 1.23034644\n",
      "Trained batch 273 batch loss 1.18914664 epoch total loss 1.23019552\n",
      "Trained batch 274 batch loss 1.33438683 epoch total loss 1.2305758\n",
      "Trained batch 275 batch loss 1.33273935 epoch total loss 1.23094726\n",
      "Trained batch 276 batch loss 1.25392461 epoch total loss 1.23103058\n",
      "Trained batch 277 batch loss 1.20601869 epoch total loss 1.23094034\n",
      "Trained batch 278 batch loss 1.37553346 epoch total loss 1.23146033\n",
      "Trained batch 279 batch loss 1.39774394 epoch total loss 1.23205638\n",
      "Trained batch 280 batch loss 1.30963397 epoch total loss 1.23233342\n",
      "Trained batch 281 batch loss 1.13339567 epoch total loss 1.23198128\n",
      "Trained batch 282 batch loss 1.34783816 epoch total loss 1.23239207\n",
      "Trained batch 283 batch loss 1.40887 epoch total loss 1.23301578\n",
      "Trained batch 284 batch loss 1.37895703 epoch total loss 1.23352969\n",
      "Trained batch 285 batch loss 1.19581938 epoch total loss 1.23339736\n",
      "Trained batch 286 batch loss 1.39587915 epoch total loss 1.23396552\n",
      "Trained batch 287 batch loss 1.3896606 epoch total loss 1.23450792\n",
      "Trained batch 288 batch loss 1.24540949 epoch total loss 1.23454583\n",
      "Trained batch 289 batch loss 1.26960862 epoch total loss 1.23466718\n",
      "Trained batch 290 batch loss 1.19900322 epoch total loss 1.23454416\n",
      "Trained batch 291 batch loss 1.15291476 epoch total loss 1.23426378\n",
      "Trained batch 292 batch loss 1.36794734 epoch total loss 1.23472154\n",
      "Trained batch 293 batch loss 1.28706086 epoch total loss 1.23490012\n",
      "Trained batch 294 batch loss 1.25076926 epoch total loss 1.23495412\n",
      "Trained batch 295 batch loss 1.32013118 epoch total loss 1.23524284\n",
      "Trained batch 296 batch loss 1.24935448 epoch total loss 1.23529053\n",
      "Trained batch 297 batch loss 1.36642718 epoch total loss 1.23573208\n",
      "Trained batch 298 batch loss 1.18291688 epoch total loss 1.23555481\n",
      "Trained batch 299 batch loss 1.11222148 epoch total loss 1.23514235\n",
      "Trained batch 300 batch loss 1.12650824 epoch total loss 1.23478019\n",
      "Trained batch 301 batch loss 1.21613133 epoch total loss 1.2347182\n",
      "Trained batch 302 batch loss 1.25823963 epoch total loss 1.23479605\n",
      "Trained batch 303 batch loss 1.20411217 epoch total loss 1.23469484\n",
      "Trained batch 304 batch loss 1.21852493 epoch total loss 1.23464167\n",
      "Trained batch 305 batch loss 1.15863407 epoch total loss 1.2343924\n",
      "Trained batch 306 batch loss 1.25681663 epoch total loss 1.2344656\n",
      "Trained batch 307 batch loss 1.1716162 epoch total loss 1.23426104\n",
      "Trained batch 308 batch loss 1.25773406 epoch total loss 1.23433709\n",
      "Trained batch 309 batch loss 1.22849441 epoch total loss 1.23431826\n",
      "Trained batch 310 batch loss 1.08905292 epoch total loss 1.23384964\n",
      "Trained batch 311 batch loss 1.20242691 epoch total loss 1.23374856\n",
      "Trained batch 312 batch loss 1.1765976 epoch total loss 1.23356545\n",
      "Trained batch 313 batch loss 1.29696333 epoch total loss 1.23376799\n",
      "Trained batch 314 batch loss 1.3127768 epoch total loss 1.23401952\n",
      "Trained batch 315 batch loss 1.29531491 epoch total loss 1.23421419\n",
      "Trained batch 316 batch loss 1.35238123 epoch total loss 1.23458815\n",
      "Trained batch 317 batch loss 1.30563414 epoch total loss 1.23481226\n",
      "Trained batch 318 batch loss 1.27219319 epoch total loss 1.2349298\n",
      "Trained batch 319 batch loss 1.31032658 epoch total loss 1.23516619\n",
      "Trained batch 320 batch loss 1.2205497 epoch total loss 1.23512053\n",
      "Trained batch 321 batch loss 1.18825817 epoch total loss 1.2349745\n",
      "Trained batch 322 batch loss 1.22933245 epoch total loss 1.23495698\n",
      "Trained batch 323 batch loss 1.11329806 epoch total loss 1.2345804\n",
      "Trained batch 324 batch loss 1.05898631 epoch total loss 1.23403847\n",
      "Trained batch 325 batch loss 1.1697681 epoch total loss 1.2338407\n",
      "Trained batch 326 batch loss 1.21445847 epoch total loss 1.23378122\n",
      "Trained batch 327 batch loss 1.30874562 epoch total loss 1.23401046\n",
      "Trained batch 328 batch loss 1.16466689 epoch total loss 1.2337991\n",
      "Trained batch 329 batch loss 1.0570426 epoch total loss 1.23326182\n",
      "Trained batch 330 batch loss 1.11069429 epoch total loss 1.23289037\n",
      "Trained batch 331 batch loss 1.12889564 epoch total loss 1.23257625\n",
      "Trained batch 332 batch loss 1.17132568 epoch total loss 1.23239172\n",
      "Trained batch 333 batch loss 1.16711617 epoch total loss 1.23219573\n",
      "Trained batch 334 batch loss 1.28077757 epoch total loss 1.23234117\n",
      "Trained batch 335 batch loss 1.28374982 epoch total loss 1.23249471\n",
      "Trained batch 336 batch loss 1.14240694 epoch total loss 1.23222649\n",
      "Trained batch 337 batch loss 1.16639745 epoch total loss 1.23203123\n",
      "Trained batch 338 batch loss 1.32236147 epoch total loss 1.23229849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 339 batch loss 1.44580305 epoch total loss 1.23292828\n",
      "Trained batch 340 batch loss 1.45950222 epoch total loss 1.23359466\n",
      "Trained batch 341 batch loss 1.43412602 epoch total loss 1.23418272\n",
      "Trained batch 342 batch loss 1.26194477 epoch total loss 1.23426378\n",
      "Trained batch 343 batch loss 1.17444706 epoch total loss 1.23408937\n",
      "Trained batch 344 batch loss 1.28669631 epoch total loss 1.23424232\n",
      "Trained batch 345 batch loss 1.26358771 epoch total loss 1.23432732\n",
      "Trained batch 346 batch loss 1.47314441 epoch total loss 1.23501754\n",
      "Trained batch 347 batch loss 1.19467843 epoch total loss 1.23490131\n",
      "Trained batch 348 batch loss 1.41126966 epoch total loss 1.23540807\n",
      "Trained batch 349 batch loss 1.09269667 epoch total loss 1.23499906\n",
      "Trained batch 350 batch loss 1.23346269 epoch total loss 1.23499465\n",
      "Trained batch 351 batch loss 1.34088719 epoch total loss 1.23529637\n",
      "Trained batch 352 batch loss 1.16731536 epoch total loss 1.23510325\n",
      "Trained batch 353 batch loss 1.17965472 epoch total loss 1.23494613\n",
      "Trained batch 354 batch loss 1.24540901 epoch total loss 1.23497581\n",
      "Trained batch 355 batch loss 1.44355571 epoch total loss 1.23556328\n",
      "Trained batch 356 batch loss 1.25068426 epoch total loss 1.23560572\n",
      "Trained batch 357 batch loss 1.10111499 epoch total loss 1.23522902\n",
      "Trained batch 358 batch loss 1.04743826 epoch total loss 1.23470438\n",
      "Trained batch 359 batch loss 1.13194323 epoch total loss 1.23441815\n",
      "Trained batch 360 batch loss 1.04845846 epoch total loss 1.23390162\n",
      "Trained batch 361 batch loss 1.03958619 epoch total loss 1.23336339\n",
      "Trained batch 362 batch loss 1.05632293 epoch total loss 1.23287427\n",
      "Trained batch 363 batch loss 1.13731587 epoch total loss 1.23261106\n",
      "Trained batch 364 batch loss 1.25383317 epoch total loss 1.23266947\n",
      "Trained batch 365 batch loss 1.1712389 epoch total loss 1.23250115\n",
      "Trained batch 366 batch loss 1.16574752 epoch total loss 1.23231876\n",
      "Trained batch 367 batch loss 1.29737902 epoch total loss 1.23249602\n",
      "Trained batch 368 batch loss 1.19952476 epoch total loss 1.2324065\n",
      "Trained batch 369 batch loss 1.28485894 epoch total loss 1.23254859\n",
      "Trained batch 370 batch loss 1.25280416 epoch total loss 1.23260331\n",
      "Trained batch 371 batch loss 1.16680694 epoch total loss 1.23242605\n",
      "Trained batch 372 batch loss 1.29744935 epoch total loss 1.23260081\n",
      "Trained batch 373 batch loss 1.32637453 epoch total loss 1.23285222\n",
      "Trained batch 374 batch loss 1.2986486 epoch total loss 1.23302817\n",
      "Trained batch 375 batch loss 1.17703295 epoch total loss 1.2328788\n",
      "Trained batch 376 batch loss 1.12627363 epoch total loss 1.23259532\n",
      "Trained batch 377 batch loss 1.14884567 epoch total loss 1.23237312\n",
      "Trained batch 378 batch loss 1.16655946 epoch total loss 1.23219907\n",
      "Trained batch 379 batch loss 1.24857736 epoch total loss 1.23224223\n",
      "Trained batch 380 batch loss 1.25502753 epoch total loss 1.23230219\n",
      "Trained batch 381 batch loss 1.27350307 epoch total loss 1.23241031\n",
      "Trained batch 382 batch loss 1.29175353 epoch total loss 1.23256564\n",
      "Trained batch 383 batch loss 1.30283046 epoch total loss 1.2327491\n",
      "Trained batch 384 batch loss 1.24363947 epoch total loss 1.23277748\n",
      "Trained batch 385 batch loss 1.33337355 epoch total loss 1.23303878\n",
      "Trained batch 386 batch loss 1.14419174 epoch total loss 1.23280859\n",
      "Trained batch 387 batch loss 1.14904809 epoch total loss 1.23259223\n",
      "Trained batch 388 batch loss 1.25173736 epoch total loss 1.23264158\n",
      "Trained batch 389 batch loss 1.0907383 epoch total loss 1.2322768\n",
      "Trained batch 390 batch loss 1.23850083 epoch total loss 1.23229265\n",
      "Trained batch 391 batch loss 1.29032707 epoch total loss 1.23244107\n",
      "Trained batch 392 batch loss 1.09794152 epoch total loss 1.23209798\n",
      "Trained batch 393 batch loss 1.29909277 epoch total loss 1.23226845\n",
      "Trained batch 394 batch loss 1.24279261 epoch total loss 1.23229516\n",
      "Trained batch 395 batch loss 1.1841234 epoch total loss 1.2321732\n",
      "Trained batch 396 batch loss 1.25578237 epoch total loss 1.23223281\n",
      "Trained batch 397 batch loss 1.13441324 epoch total loss 1.23198628\n",
      "Trained batch 398 batch loss 1.25206506 epoch total loss 1.23203683\n",
      "Trained batch 399 batch loss 1.2640574 epoch total loss 1.23211706\n",
      "Trained batch 400 batch loss 1.28699088 epoch total loss 1.23225427\n",
      "Trained batch 401 batch loss 1.25995398 epoch total loss 1.23232329\n",
      "Trained batch 402 batch loss 1.29321587 epoch total loss 1.2324748\n",
      "Trained batch 403 batch loss 1.08999431 epoch total loss 1.23212123\n",
      "Trained batch 404 batch loss 1.25607944 epoch total loss 1.23218048\n",
      "Trained batch 405 batch loss 1.19009697 epoch total loss 1.23207664\n",
      "Trained batch 406 batch loss 1.2993288 epoch total loss 1.23224223\n",
      "Trained batch 407 batch loss 1.35950124 epoch total loss 1.23255491\n",
      "Trained batch 408 batch loss 1.39809549 epoch total loss 1.2329607\n",
      "Trained batch 409 batch loss 1.31092525 epoch total loss 1.2331512\n",
      "Trained batch 410 batch loss 1.29435396 epoch total loss 1.23330045\n",
      "Trained batch 411 batch loss 1.16689956 epoch total loss 1.23313892\n",
      "Trained batch 412 batch loss 1.18142641 epoch total loss 1.23301339\n",
      "Trained batch 413 batch loss 1.14323437 epoch total loss 1.23279607\n",
      "Trained batch 414 batch loss 1.14351 epoch total loss 1.23258042\n",
      "Trained batch 415 batch loss 1.26816392 epoch total loss 1.23266613\n",
      "Trained batch 416 batch loss 1.22846127 epoch total loss 1.232656\n",
      "Trained batch 417 batch loss 1.29325652 epoch total loss 1.23280144\n",
      "Trained batch 418 batch loss 1.22026324 epoch total loss 1.2327714\n",
      "Trained batch 419 batch loss 1.15446889 epoch total loss 1.2325846\n",
      "Trained batch 420 batch loss 1.20451117 epoch total loss 1.23251784\n",
      "Trained batch 421 batch loss 1.19295967 epoch total loss 1.23242378\n",
      "Trained batch 422 batch loss 1.22148609 epoch total loss 1.23239791\n",
      "Trained batch 423 batch loss 1.12623417 epoch total loss 1.23214686\n",
      "Trained batch 424 batch loss 1.06900144 epoch total loss 1.23176217\n",
      "Trained batch 425 batch loss 1.12453508 epoch total loss 1.2315098\n",
      "Trained batch 426 batch loss 1.1772356 epoch total loss 1.23138237\n",
      "Trained batch 427 batch loss 1.30583107 epoch total loss 1.23155677\n",
      "Trained batch 428 batch loss 1.22784758 epoch total loss 1.23154819\n",
      "Trained batch 429 batch loss 1.14868355 epoch total loss 1.23135495\n",
      "Trained batch 430 batch loss 0.999567688 epoch total loss 1.23081589\n",
      "Trained batch 431 batch loss 0.96525228 epoch total loss 1.23019981\n",
      "Trained batch 432 batch loss 1.0078721 epoch total loss 1.22968519\n",
      "Trained batch 433 batch loss 1.07977557 epoch total loss 1.229339\n",
      "Trained batch 434 batch loss 1.09337687 epoch total loss 1.22902572\n",
      "Trained batch 435 batch loss 1.20602465 epoch total loss 1.22897291\n",
      "Trained batch 436 batch loss 1.28535461 epoch total loss 1.22910213\n",
      "Trained batch 437 batch loss 1.36129308 epoch total loss 1.22940457\n",
      "Trained batch 438 batch loss 1.32152677 epoch total loss 1.22961497\n",
      "Trained batch 439 batch loss 1.21567035 epoch total loss 1.22958326\n",
      "Trained batch 440 batch loss 1.33541918 epoch total loss 1.22982383\n",
      "Trained batch 441 batch loss 1.31338096 epoch total loss 1.23001325\n",
      "Trained batch 442 batch loss 1.26291382 epoch total loss 1.23008776\n",
      "Trained batch 443 batch loss 1.2610333 epoch total loss 1.23015761\n",
      "Trained batch 444 batch loss 1.18176413 epoch total loss 1.23004866\n",
      "Trained batch 445 batch loss 1.24131346 epoch total loss 1.23007405\n",
      "Trained batch 446 batch loss 1.48135424 epoch total loss 1.23063743\n",
      "Trained batch 447 batch loss 1.27664077 epoch total loss 1.23074031\n",
      "Trained batch 448 batch loss 1.31287134 epoch total loss 1.23092365\n",
      "Trained batch 449 batch loss 1.24000299 epoch total loss 1.2309438\n",
      "Trained batch 450 batch loss 1.03806794 epoch total loss 1.23051524\n",
      "Trained batch 451 batch loss 1.12895429 epoch total loss 1.23029\n",
      "Trained batch 452 batch loss 1.12700653 epoch total loss 1.23006165\n",
      "Trained batch 453 batch loss 1.25306821 epoch total loss 1.23011243\n",
      "Trained batch 454 batch loss 1.20078635 epoch total loss 1.23004782\n",
      "Trained batch 455 batch loss 1.38736558 epoch total loss 1.23039365\n",
      "Trained batch 456 batch loss 1.33948 epoch total loss 1.2306329\n",
      "Trained batch 457 batch loss 1.18357062 epoch total loss 1.2305299\n",
      "Trained batch 458 batch loss 1.23291552 epoch total loss 1.23053515\n",
      "Trained batch 459 batch loss 1.34561968 epoch total loss 1.23078585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 460 batch loss 1.21105814 epoch total loss 1.23074305\n",
      "Trained batch 461 batch loss 1.17749274 epoch total loss 1.23062754\n",
      "Trained batch 462 batch loss 1.102144 epoch total loss 1.23034942\n",
      "Trained batch 463 batch loss 1.05611801 epoch total loss 1.22997308\n",
      "Trained batch 464 batch loss 1.10894847 epoch total loss 1.22971225\n",
      "Trained batch 465 batch loss 1.19742298 epoch total loss 1.22964287\n",
      "Trained batch 466 batch loss 1.26750505 epoch total loss 1.22972417\n",
      "Trained batch 467 batch loss 1.15596569 epoch total loss 1.22956622\n",
      "Trained batch 468 batch loss 1.20831752 epoch total loss 1.2295208\n",
      "Trained batch 469 batch loss 1.128654 epoch total loss 1.22930574\n",
      "Trained batch 470 batch loss 1.19922042 epoch total loss 1.22924173\n",
      "Trained batch 471 batch loss 1.46089911 epoch total loss 1.22973347\n",
      "Trained batch 472 batch loss 1.2484349 epoch total loss 1.22977304\n",
      "Trained batch 473 batch loss 1.17702568 epoch total loss 1.22966146\n",
      "Trained batch 474 batch loss 1.00345683 epoch total loss 1.22918427\n",
      "Trained batch 475 batch loss 0.915039062 epoch total loss 1.2285229\n",
      "Trained batch 476 batch loss 0.895187497 epoch total loss 1.22782266\n",
      "Trained batch 477 batch loss 1.07321882 epoch total loss 1.22749865\n",
      "Trained batch 478 batch loss 1.15146148 epoch total loss 1.22733963\n",
      "Trained batch 479 batch loss 1.18707871 epoch total loss 1.22725558\n",
      "Trained batch 480 batch loss 1.19774342 epoch total loss 1.22719407\n",
      "Trained batch 481 batch loss 1.22646165 epoch total loss 1.22719252\n",
      "Trained batch 482 batch loss 1.59286189 epoch total loss 1.22795117\n",
      "Trained batch 483 batch loss 1.44980776 epoch total loss 1.22841048\n",
      "Trained batch 484 batch loss 1.2288568 epoch total loss 1.22841144\n",
      "Trained batch 485 batch loss 1.2849431 epoch total loss 1.22852814\n",
      "Trained batch 486 batch loss 1.10131025 epoch total loss 1.22826636\n",
      "Trained batch 487 batch loss 1.03344023 epoch total loss 1.22786629\n",
      "Trained batch 488 batch loss 1.07057905 epoch total loss 1.22754395\n",
      "Trained batch 489 batch loss 1.19629359 epoch total loss 1.22748\n",
      "Trained batch 490 batch loss 0.98989749 epoch total loss 1.22699511\n",
      "Trained batch 491 batch loss 0.983191252 epoch total loss 1.2264986\n",
      "Trained batch 492 batch loss 1.04245698 epoch total loss 1.22612464\n",
      "Trained batch 493 batch loss 1.04910493 epoch total loss 1.22576559\n",
      "Trained batch 494 batch loss 1.16934371 epoch total loss 1.22565138\n",
      "Trained batch 495 batch loss 1.19807792 epoch total loss 1.22559571\n",
      "Trained batch 496 batch loss 1.25306463 epoch total loss 1.22565103\n",
      "Trained batch 497 batch loss 1.24912286 epoch total loss 1.22569835\n",
      "Trained batch 498 batch loss 1.30120611 epoch total loss 1.22585\n",
      "Trained batch 499 batch loss 1.30358231 epoch total loss 1.22600567\n",
      "Trained batch 500 batch loss 1.20523596 epoch total loss 1.22596419\n",
      "Trained batch 501 batch loss 1.3385886 epoch total loss 1.22618902\n",
      "Trained batch 502 batch loss 1.43392909 epoch total loss 1.22660279\n",
      "Trained batch 503 batch loss 1.32895422 epoch total loss 1.22680628\n",
      "Trained batch 504 batch loss 1.33275795 epoch total loss 1.22701657\n",
      "Trained batch 505 batch loss 1.21107459 epoch total loss 1.22698486\n",
      "Trained batch 506 batch loss 1.27343476 epoch total loss 1.22707677\n",
      "Trained batch 507 batch loss 1.13517249 epoch total loss 1.22689545\n",
      "Trained batch 508 batch loss 1.40216899 epoch total loss 1.22724044\n",
      "Trained batch 509 batch loss 1.24078131 epoch total loss 1.22726715\n",
      "Trained batch 510 batch loss 1.13277936 epoch total loss 1.22708178\n",
      "Trained batch 511 batch loss 1.14306831 epoch total loss 1.22691739\n",
      "Trained batch 512 batch loss 1.08170879 epoch total loss 1.22663379\n",
      "Trained batch 513 batch loss 1.03470409 epoch total loss 1.22625971\n",
      "Trained batch 514 batch loss 1.12641215 epoch total loss 1.2260654\n",
      "Trained batch 515 batch loss 1.44257987 epoch total loss 1.22648585\n",
      "Trained batch 516 batch loss 1.52634943 epoch total loss 1.22706699\n",
      "Trained batch 517 batch loss 1.53674805 epoch total loss 1.22766602\n",
      "Trained batch 518 batch loss 1.33271396 epoch total loss 1.2278688\n",
      "Trained batch 519 batch loss 1.35165405 epoch total loss 1.22810733\n",
      "Trained batch 520 batch loss 1.37202215 epoch total loss 1.22838402\n",
      "Trained batch 521 batch loss 1.2849673 epoch total loss 1.22849262\n",
      "Trained batch 522 batch loss 1.22386634 epoch total loss 1.2284838\n",
      "Trained batch 523 batch loss 1.2168597 epoch total loss 1.22846162\n",
      "Trained batch 524 batch loss 1.25871706 epoch total loss 1.22851932\n",
      "Trained batch 525 batch loss 1.09560168 epoch total loss 1.22826612\n",
      "Trained batch 526 batch loss 1.27357328 epoch total loss 1.22835219\n",
      "Trained batch 527 batch loss 1.16132283 epoch total loss 1.22822499\n",
      "Trained batch 528 batch loss 1.21669567 epoch total loss 1.22820318\n",
      "Trained batch 529 batch loss 1.16536903 epoch total loss 1.22808433\n",
      "Trained batch 530 batch loss 1.17844069 epoch total loss 1.22799075\n",
      "Trained batch 531 batch loss 1.26179481 epoch total loss 1.2280544\n",
      "Trained batch 532 batch loss 1.25623798 epoch total loss 1.22810733\n",
      "Trained batch 533 batch loss 1.20983267 epoch total loss 1.228073\n",
      "Trained batch 534 batch loss 1.29099023 epoch total loss 1.2281909\n",
      "Trained batch 535 batch loss 1.14851475 epoch total loss 1.22804189\n",
      "Trained batch 536 batch loss 1.29069519 epoch total loss 1.22815883\n",
      "Trained batch 537 batch loss 1.31818318 epoch total loss 1.22832644\n",
      "Trained batch 538 batch loss 1.23511338 epoch total loss 1.22833908\n",
      "Trained batch 539 batch loss 1.29870546 epoch total loss 1.22846961\n",
      "Trained batch 540 batch loss 1.15547693 epoch total loss 1.22833443\n",
      "Trained batch 541 batch loss 1.19716966 epoch total loss 1.22827673\n",
      "Trained batch 542 batch loss 1.23133612 epoch total loss 1.22828245\n",
      "Trained batch 543 batch loss 1.2153132 epoch total loss 1.22825861\n",
      "Trained batch 544 batch loss 1.28564024 epoch total loss 1.22836399\n",
      "Trained batch 545 batch loss 1.04236269 epoch total loss 1.22802269\n",
      "Trained batch 546 batch loss 0.969464362 epoch total loss 1.2275492\n",
      "Trained batch 547 batch loss 1.0962801 epoch total loss 1.22730923\n",
      "Trained batch 548 batch loss 1.20363891 epoch total loss 1.22726595\n",
      "Trained batch 549 batch loss 1.16282058 epoch total loss 1.22714865\n",
      "Trained batch 550 batch loss 1.1932261 epoch total loss 1.2270869\n",
      "Trained batch 551 batch loss 1.11080337 epoch total loss 1.2268759\n",
      "Trained batch 552 batch loss 1.22960782 epoch total loss 1.22688079\n",
      "Trained batch 553 batch loss 1.32479489 epoch total loss 1.22705781\n",
      "Trained batch 554 batch loss 1.33547592 epoch total loss 1.22725356\n",
      "Trained batch 555 batch loss 1.36031222 epoch total loss 1.22749317\n",
      "Trained batch 556 batch loss 1.06653821 epoch total loss 1.22720373\n",
      "Trained batch 557 batch loss 1.1217711 epoch total loss 1.22701442\n",
      "Trained batch 558 batch loss 1.22474694 epoch total loss 1.22701037\n",
      "Trained batch 559 batch loss 1.24355173 epoch total loss 1.22703981\n",
      "Trained batch 560 batch loss 1.44186044 epoch total loss 1.22742343\n",
      "Trained batch 561 batch loss 1.26753449 epoch total loss 1.22749484\n",
      "Trained batch 562 batch loss 1.29742646 epoch total loss 1.22761929\n",
      "Trained batch 563 batch loss 1.16093326 epoch total loss 1.22750092\n",
      "Trained batch 564 batch loss 1.26172733 epoch total loss 1.22756159\n",
      "Trained batch 565 batch loss 1.15873289 epoch total loss 1.22743976\n",
      "Trained batch 566 batch loss 1.32945609 epoch total loss 1.22762\n",
      "Trained batch 567 batch loss 1.20424986 epoch total loss 1.22757876\n",
      "Trained batch 568 batch loss 1.23064792 epoch total loss 1.22758424\n",
      "Trained batch 569 batch loss 1.107535 epoch total loss 1.22737324\n",
      "Trained batch 570 batch loss 1.12910438 epoch total loss 1.22720075\n",
      "Trained batch 571 batch loss 1.1710149 epoch total loss 1.2271024\n",
      "Trained batch 572 batch loss 1.12284946 epoch total loss 1.22692013\n",
      "Trained batch 573 batch loss 1.06638908 epoch total loss 1.22664\n",
      "Trained batch 574 batch loss 1.089257 epoch total loss 1.22640061\n",
      "Trained batch 575 batch loss 1.15292025 epoch total loss 1.22627282\n",
      "Trained batch 576 batch loss 1.12810135 epoch total loss 1.22610235\n",
      "Trained batch 577 batch loss 1.2586962 epoch total loss 1.22615886\n",
      "Trained batch 578 batch loss 1.29672134 epoch total loss 1.22628093\n",
      "Trained batch 579 batch loss 1.08343673 epoch total loss 1.22603416\n",
      "Trained batch 580 batch loss 1.3450321 epoch total loss 1.22623932\n",
      "Trained batch 581 batch loss 1.16672564 epoch total loss 1.22613692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 582 batch loss 1.27878571 epoch total loss 1.2262274\n",
      "Trained batch 583 batch loss 1.17771769 epoch total loss 1.22614431\n",
      "Trained batch 584 batch loss 1.23714209 epoch total loss 1.22616303\n",
      "Trained batch 585 batch loss 1.34120882 epoch total loss 1.22635972\n",
      "Trained batch 586 batch loss 1.20548344 epoch total loss 1.22632408\n",
      "Trained batch 587 batch loss 1.33313453 epoch total loss 1.22650599\n",
      "Trained batch 588 batch loss 1.33520937 epoch total loss 1.22669089\n",
      "Trained batch 589 batch loss 1.28035176 epoch total loss 1.22678196\n",
      "Trained batch 590 batch loss 1.17229629 epoch total loss 1.2266897\n",
      "Trained batch 591 batch loss 0.974690318 epoch total loss 1.22626317\n",
      "Trained batch 592 batch loss 1.10037613 epoch total loss 1.22605062\n",
      "Trained batch 593 batch loss 1.16549444 epoch total loss 1.22594845\n",
      "Trained batch 594 batch loss 1.35015655 epoch total loss 1.22615755\n",
      "Trained batch 595 batch loss 1.26321745 epoch total loss 1.22621989\n",
      "Trained batch 596 batch loss 1.21732354 epoch total loss 1.22620499\n",
      "Trained batch 597 batch loss 1.2708 epoch total loss 1.22627974\n",
      "Trained batch 598 batch loss 1.24486804 epoch total loss 1.22631085\n",
      "Trained batch 599 batch loss 1.17919326 epoch total loss 1.22623217\n",
      "Trained batch 600 batch loss 1.16642344 epoch total loss 1.22613251\n",
      "Trained batch 601 batch loss 1.28126621 epoch total loss 1.22622418\n",
      "Trained batch 602 batch loss 1.34119523 epoch total loss 1.22641516\n",
      "Trained batch 603 batch loss 1.28756571 epoch total loss 1.2265166\n",
      "Trained batch 604 batch loss 1.10295153 epoch total loss 1.22631204\n",
      "Trained batch 605 batch loss 1.20796442 epoch total loss 1.22628164\n",
      "Trained batch 606 batch loss 1.07056129 epoch total loss 1.22602463\n",
      "Trained batch 607 batch loss 1.25679827 epoch total loss 1.22607529\n",
      "Trained batch 608 batch loss 1.25827479 epoch total loss 1.22612834\n",
      "Trained batch 609 batch loss 1.12921882 epoch total loss 1.2259692\n",
      "Trained batch 610 batch loss 1.1967175 epoch total loss 1.22592127\n",
      "Trained batch 611 batch loss 1.31381333 epoch total loss 1.22606516\n",
      "Trained batch 612 batch loss 1.27223933 epoch total loss 1.2261405\n",
      "Trained batch 613 batch loss 1.18391335 epoch total loss 1.2260716\n",
      "Trained batch 614 batch loss 1.16119087 epoch total loss 1.22596598\n",
      "Trained batch 615 batch loss 1.13544989 epoch total loss 1.22581875\n",
      "Trained batch 616 batch loss 1.14242315 epoch total loss 1.22568333\n",
      "Trained batch 617 batch loss 1.26421595 epoch total loss 1.2257458\n",
      "Trained batch 618 batch loss 1.28256345 epoch total loss 1.22583783\n",
      "Trained batch 619 batch loss 1.06235588 epoch total loss 1.22557366\n",
      "Trained batch 620 batch loss 1.18525553 epoch total loss 1.22550869\n",
      "Trained batch 621 batch loss 1.23903847 epoch total loss 1.22553039\n",
      "Trained batch 622 batch loss 1.3305608 epoch total loss 1.22569931\n",
      "Trained batch 623 batch loss 1.27536106 epoch total loss 1.22577906\n",
      "Trained batch 624 batch loss 1.32193136 epoch total loss 1.22593319\n",
      "Trained batch 625 batch loss 1.12471819 epoch total loss 1.22577119\n",
      "Trained batch 626 batch loss 1.07880032 epoch total loss 1.22553647\n",
      "Trained batch 627 batch loss 1.15137935 epoch total loss 1.22541809\n",
      "Trained batch 628 batch loss 1.15197229 epoch total loss 1.22530115\n",
      "Trained batch 629 batch loss 1.15159655 epoch total loss 1.22518396\n",
      "Trained batch 630 batch loss 1.19668281 epoch total loss 1.22513878\n",
      "Trained batch 631 batch loss 1.26501346 epoch total loss 1.22520196\n",
      "Trained batch 632 batch loss 1.1412425 epoch total loss 1.22506905\n",
      "Trained batch 633 batch loss 1.19666719 epoch total loss 1.22502422\n",
      "Trained batch 634 batch loss 1.22857141 epoch total loss 1.22502983\n",
      "Trained batch 635 batch loss 1.22692323 epoch total loss 1.22503281\n",
      "Trained batch 636 batch loss 1.26334167 epoch total loss 1.22509301\n",
      "Trained batch 637 batch loss 1.22281337 epoch total loss 1.22508955\n",
      "Trained batch 638 batch loss 1.25974727 epoch total loss 1.22514391\n",
      "Trained batch 639 batch loss 1.34780955 epoch total loss 1.22533584\n",
      "Trained batch 640 batch loss 1.24090111 epoch total loss 1.22536016\n",
      "Trained batch 641 batch loss 1.31210935 epoch total loss 1.22549558\n",
      "Trained batch 642 batch loss 1.37311125 epoch total loss 1.22572553\n",
      "Trained batch 643 batch loss 1.44223356 epoch total loss 1.2260623\n",
      "Trained batch 644 batch loss 1.30393124 epoch total loss 1.22618318\n",
      "Trained batch 645 batch loss 1.11698365 epoch total loss 1.2260139\n",
      "Trained batch 646 batch loss 1.19510186 epoch total loss 1.2259661\n",
      "Trained batch 647 batch loss 1.118927 epoch total loss 1.22580063\n",
      "Trained batch 648 batch loss 1.1050868 epoch total loss 1.22561443\n",
      "Trained batch 649 batch loss 1.03362322 epoch total loss 1.22531855\n",
      "Trained batch 650 batch loss 1.15282059 epoch total loss 1.22520709\n",
      "Trained batch 651 batch loss 1.30026245 epoch total loss 1.22532225\n",
      "Trained batch 652 batch loss 1.18624389 epoch total loss 1.22526228\n",
      "Trained batch 653 batch loss 1.30513895 epoch total loss 1.22538459\n",
      "Trained batch 654 batch loss 1.22527039 epoch total loss 1.22538447\n",
      "Trained batch 655 batch loss 1.22985184 epoch total loss 1.22539127\n",
      "Trained batch 656 batch loss 1.34652734 epoch total loss 1.22557592\n",
      "Trained batch 657 batch loss 1.30457926 epoch total loss 1.22569621\n",
      "Trained batch 658 batch loss 1.30604625 epoch total loss 1.22581828\n",
      "Trained batch 659 batch loss 1.23184133 epoch total loss 1.22582734\n",
      "Trained batch 660 batch loss 1.23677909 epoch total loss 1.22584391\n",
      "Trained batch 661 batch loss 1.24689889 epoch total loss 1.22587574\n",
      "Trained batch 662 batch loss 1.23851156 epoch total loss 1.22589493\n",
      "Trained batch 663 batch loss 1.40347433 epoch total loss 1.22616279\n",
      "Trained batch 664 batch loss 1.44601488 epoch total loss 1.22649395\n",
      "Trained batch 665 batch loss 1.41122 epoch total loss 1.22677171\n",
      "Trained batch 666 batch loss 1.22264981 epoch total loss 1.22676551\n",
      "Trained batch 667 batch loss 1.25848854 epoch total loss 1.22681308\n",
      "Trained batch 668 batch loss 1.29108787 epoch total loss 1.22690928\n",
      "Trained batch 669 batch loss 1.35501921 epoch total loss 1.22710073\n",
      "Trained batch 670 batch loss 1.40405321 epoch total loss 1.2273649\n",
      "Trained batch 671 batch loss 1.14512134 epoch total loss 1.22724235\n",
      "Trained batch 672 batch loss 1.17592645 epoch total loss 1.22716594\n",
      "Trained batch 673 batch loss 1.12324345 epoch total loss 1.22701156\n",
      "Trained batch 674 batch loss 1.14276028 epoch total loss 1.22688651\n",
      "Trained batch 675 batch loss 1.3033067 epoch total loss 1.22699964\n",
      "Trained batch 676 batch loss 1.28123224 epoch total loss 1.22708\n",
      "Trained batch 677 batch loss 1.35254097 epoch total loss 1.22726524\n",
      "Trained batch 678 batch loss 1.33043277 epoch total loss 1.22741747\n",
      "Trained batch 679 batch loss 1.24212027 epoch total loss 1.22743905\n",
      "Trained batch 680 batch loss 1.32861078 epoch total loss 1.22758794\n",
      "Trained batch 681 batch loss 1.30454242 epoch total loss 1.22770095\n",
      "Trained batch 682 batch loss 1.29931355 epoch total loss 1.22780597\n",
      "Trained batch 683 batch loss 1.28741384 epoch total loss 1.22789323\n",
      "Trained batch 684 batch loss 1.23164082 epoch total loss 1.22789872\n",
      "Trained batch 685 batch loss 1.30452549 epoch total loss 1.22801054\n",
      "Trained batch 686 batch loss 1.34711397 epoch total loss 1.2281841\n",
      "Trained batch 687 batch loss 1.28046644 epoch total loss 1.22826016\n",
      "Trained batch 688 batch loss 1.21127081 epoch total loss 1.22823548\n",
      "Trained batch 689 batch loss 1.29119778 epoch total loss 1.2283268\n",
      "Trained batch 690 batch loss 1.3643 epoch total loss 1.22852397\n",
      "Trained batch 691 batch loss 1.1772486 epoch total loss 1.2284497\n",
      "Trained batch 692 batch loss 1.10299933 epoch total loss 1.2282685\n",
      "Trained batch 693 batch loss 1.12296653 epoch total loss 1.22811651\n",
      "Trained batch 694 batch loss 1.31945169 epoch total loss 1.22824812\n",
      "Trained batch 695 batch loss 1.17710447 epoch total loss 1.22817457\n",
      "Trained batch 696 batch loss 1.25343907 epoch total loss 1.22821093\n",
      "Trained batch 697 batch loss 1.09107697 epoch total loss 1.22801411\n",
      "Trained batch 698 batch loss 1.0993191 epoch total loss 1.22782969\n",
      "Trained batch 699 batch loss 1.09225702 epoch total loss 1.22763586\n",
      "Trained batch 700 batch loss 1.23341465 epoch total loss 1.22764409\n",
      "Trained batch 701 batch loss 1.38606393 epoch total loss 1.22787\n",
      "Trained batch 702 batch loss 1.28225505 epoch total loss 1.22794747\n",
      "Trained batch 703 batch loss 1.12938774 epoch total loss 1.22780728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 704 batch loss 1.10221815 epoch total loss 1.22762883\n",
      "Trained batch 705 batch loss 1.22497463 epoch total loss 1.22762513\n",
      "Trained batch 706 batch loss 1.17540193 epoch total loss 1.22755122\n",
      "Trained batch 707 batch loss 1.07482243 epoch total loss 1.2273351\n",
      "Trained batch 708 batch loss 1.19792521 epoch total loss 1.22729361\n",
      "Trained batch 709 batch loss 1.25065219 epoch total loss 1.22732663\n",
      "Trained batch 710 batch loss 1.21647716 epoch total loss 1.22731137\n",
      "Trained batch 711 batch loss 1.2365129 epoch total loss 1.22732425\n",
      "Trained batch 712 batch loss 1.27182817 epoch total loss 1.22738683\n",
      "Trained batch 713 batch loss 1.05165815 epoch total loss 1.22714031\n",
      "Trained batch 714 batch loss 1.16562164 epoch total loss 1.22705424\n",
      "Trained batch 715 batch loss 1.17267263 epoch total loss 1.22697818\n",
      "Trained batch 716 batch loss 1.36600637 epoch total loss 1.22717237\n",
      "Trained batch 717 batch loss 1.27965307 epoch total loss 1.22724557\n",
      "Trained batch 718 batch loss 1.26193416 epoch total loss 1.22729385\n",
      "Trained batch 719 batch loss 1.24768925 epoch total loss 1.22732222\n",
      "Trained batch 720 batch loss 1.23560631 epoch total loss 1.22733378\n",
      "Trained batch 721 batch loss 1.26152146 epoch total loss 1.22738123\n",
      "Trained batch 722 batch loss 1.23410463 epoch total loss 1.22739053\n",
      "Trained batch 723 batch loss 1.14007473 epoch total loss 1.22726977\n",
      "Trained batch 724 batch loss 1.1235 epoch total loss 1.22712636\n",
      "Trained batch 725 batch loss 1.06892657 epoch total loss 1.22690821\n",
      "Trained batch 726 batch loss 1.09538174 epoch total loss 1.22672701\n",
      "Trained batch 727 batch loss 1.15185738 epoch total loss 1.22662401\n",
      "Trained batch 728 batch loss 1.0498476 epoch total loss 1.22638118\n",
      "Trained batch 729 batch loss 1.14719033 epoch total loss 1.2262727\n",
      "Trained batch 730 batch loss 1.21515846 epoch total loss 1.22625744\n",
      "Trained batch 731 batch loss 1.33119488 epoch total loss 1.22640097\n",
      "Trained batch 732 batch loss 1.12042761 epoch total loss 1.22625613\n",
      "Trained batch 733 batch loss 1.12724733 epoch total loss 1.22612107\n",
      "Trained batch 734 batch loss 1.13288224 epoch total loss 1.22599411\n",
      "Trained batch 735 batch loss 1.38892686 epoch total loss 1.22621572\n",
      "Trained batch 736 batch loss 1.32850182 epoch total loss 1.22635472\n",
      "Trained batch 737 batch loss 1.24551034 epoch total loss 1.22638059\n",
      "Trained batch 738 batch loss 1.27161479 epoch total loss 1.22644186\n",
      "Trained batch 739 batch loss 1.42714155 epoch total loss 1.22671342\n",
      "Trained batch 740 batch loss 1.32876706 epoch total loss 1.22685146\n",
      "Trained batch 741 batch loss 1.23810315 epoch total loss 1.2268666\n",
      "Trained batch 742 batch loss 1.24059224 epoch total loss 1.22688508\n",
      "Trained batch 743 batch loss 1.21892047 epoch total loss 1.22687447\n",
      "Trained batch 744 batch loss 1.27083421 epoch total loss 1.22693348\n",
      "Trained batch 745 batch loss 1.19636977 epoch total loss 1.22689247\n",
      "Trained batch 746 batch loss 1.20284557 epoch total loss 1.22686017\n",
      "Trained batch 747 batch loss 1.20051575 epoch total loss 1.22682488\n",
      "Trained batch 748 batch loss 1.22067928 epoch total loss 1.22681665\n",
      "Trained batch 749 batch loss 1.23148882 epoch total loss 1.22682297\n",
      "Trained batch 750 batch loss 1.22662425 epoch total loss 1.22682261\n",
      "Trained batch 751 batch loss 1.2058388 epoch total loss 1.22679472\n",
      "Trained batch 752 batch loss 1.03944874 epoch total loss 1.22654557\n",
      "Trained batch 753 batch loss 1.18733668 epoch total loss 1.22649348\n",
      "Trained batch 754 batch loss 1.19485903 epoch total loss 1.22645152\n",
      "Trained batch 755 batch loss 1.0981549 epoch total loss 1.22628152\n",
      "Trained batch 756 batch loss 1.11623967 epoch total loss 1.22613597\n",
      "Trained batch 757 batch loss 1.1178391 epoch total loss 1.22599292\n",
      "Trained batch 758 batch loss 1.24310923 epoch total loss 1.22601557\n",
      "Trained batch 759 batch loss 1.1785717 epoch total loss 1.22595298\n",
      "Trained batch 760 batch loss 1.03537309 epoch total loss 1.22570229\n",
      "Trained batch 761 batch loss 1.00177312 epoch total loss 1.22540808\n",
      "Trained batch 762 batch loss 1.18403411 epoch total loss 1.22535372\n",
      "Trained batch 763 batch loss 1.29159474 epoch total loss 1.2254405\n",
      "Trained batch 764 batch loss 1.60590136 epoch total loss 1.22593844\n",
      "Trained batch 765 batch loss 1.27449763 epoch total loss 1.22600198\n",
      "Trained batch 766 batch loss 1.40922904 epoch total loss 1.22624111\n",
      "Trained batch 767 batch loss 1.06740844 epoch total loss 1.22603405\n",
      "Trained batch 768 batch loss 1.26924479 epoch total loss 1.22609031\n",
      "Trained batch 769 batch loss 1.3008424 epoch total loss 1.22618747\n",
      "Trained batch 770 batch loss 1.29964018 epoch total loss 1.22628284\n",
      "Trained batch 771 batch loss 1.22329521 epoch total loss 1.2262789\n",
      "Trained batch 772 batch loss 1.24708056 epoch total loss 1.22630584\n",
      "Trained batch 773 batch loss 1.27890754 epoch total loss 1.22637391\n",
      "Trained batch 774 batch loss 1.25424647 epoch total loss 1.22641\n",
      "Trained batch 775 batch loss 1.18509316 epoch total loss 1.22635674\n",
      "Trained batch 776 batch loss 1.15071642 epoch total loss 1.22625923\n",
      "Trained batch 777 batch loss 1.17270422 epoch total loss 1.22619033\n",
      "Trained batch 778 batch loss 1.23361921 epoch total loss 1.22619987\n",
      "Trained batch 779 batch loss 1.18914866 epoch total loss 1.2261523\n",
      "Trained batch 780 batch loss 1.08859015 epoch total loss 1.22597599\n",
      "Trained batch 781 batch loss 1.08731127 epoch total loss 1.22579837\n",
      "Trained batch 782 batch loss 1.09182024 epoch total loss 1.22562706\n",
      "Trained batch 783 batch loss 1.10137701 epoch total loss 1.2254684\n",
      "Trained batch 784 batch loss 1.13983965 epoch total loss 1.2253592\n",
      "Trained batch 785 batch loss 1.23543954 epoch total loss 1.22537196\n",
      "Trained batch 786 batch loss 1.19205666 epoch total loss 1.22532964\n",
      "Trained batch 787 batch loss 1.30871117 epoch total loss 1.22543561\n",
      "Trained batch 788 batch loss 1.26444459 epoch total loss 1.22548509\n",
      "Trained batch 789 batch loss 1.15751171 epoch total loss 1.22539902\n",
      "Trained batch 790 batch loss 1.30452347 epoch total loss 1.22549915\n",
      "Trained batch 791 batch loss 1.32322252 epoch total loss 1.22562265\n",
      "Trained batch 792 batch loss 1.35254097 epoch total loss 1.22578287\n",
      "Trained batch 793 batch loss 1.18326306 epoch total loss 1.22572935\n",
      "Trained batch 794 batch loss 1.13384414 epoch total loss 1.22561359\n",
      "Trained batch 795 batch loss 1.29053283 epoch total loss 1.22569525\n",
      "Trained batch 796 batch loss 1.23903918 epoch total loss 1.22571194\n",
      "Trained batch 797 batch loss 1.19600153 epoch total loss 1.22567475\n",
      "Trained batch 798 batch loss 1.32808828 epoch total loss 1.22580302\n",
      "Trained batch 799 batch loss 1.3238833 epoch total loss 1.2259258\n",
      "Trained batch 800 batch loss 1.22505403 epoch total loss 1.22592473\n",
      "Trained batch 801 batch loss 1.23704755 epoch total loss 1.22593856\n",
      "Trained batch 802 batch loss 1.24966407 epoch total loss 1.22596812\n",
      "Trained batch 803 batch loss 1.221573 epoch total loss 1.22596264\n",
      "Trained batch 804 batch loss 1.29200315 epoch total loss 1.22604477\n",
      "Trained batch 805 batch loss 1.22120941 epoch total loss 1.22603869\n",
      "Trained batch 806 batch loss 1.21269786 epoch total loss 1.22602224\n",
      "Trained batch 807 batch loss 1.24861336 epoch total loss 1.22605014\n",
      "Trained batch 808 batch loss 1.35930514 epoch total loss 1.22621512\n",
      "Trained batch 809 batch loss 1.1893388 epoch total loss 1.22616947\n",
      "Trained batch 810 batch loss 1.20038915 epoch total loss 1.22613764\n",
      "Trained batch 811 batch loss 1.12510562 epoch total loss 1.22601306\n",
      "Trained batch 812 batch loss 1.14872336 epoch total loss 1.22591794\n",
      "Trained batch 813 batch loss 1.23200452 epoch total loss 1.22592545\n",
      "Trained batch 814 batch loss 1.27295971 epoch total loss 1.22598314\n",
      "Trained batch 815 batch loss 1.27845919 epoch total loss 1.22604752\n",
      "Trained batch 816 batch loss 1.2142117 epoch total loss 1.22603309\n",
      "Trained batch 817 batch loss 1.22871053 epoch total loss 1.22603631\n",
      "Trained batch 818 batch loss 1.17024374 epoch total loss 1.22596812\n",
      "Trained batch 819 batch loss 1.21437919 epoch total loss 1.22595394\n",
      "Trained batch 820 batch loss 1.07476878 epoch total loss 1.22576952\n",
      "Trained batch 821 batch loss 1.26137042 epoch total loss 1.22581291\n",
      "Trained batch 822 batch loss 1.32768178 epoch total loss 1.22593689\n",
      "Trained batch 823 batch loss 1.32323742 epoch total loss 1.22605503\n",
      "Trained batch 824 batch loss 1.18528223 epoch total loss 1.22600567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 825 batch loss 1.14838529 epoch total loss 1.2259115\n",
      "Trained batch 826 batch loss 1.2844311 epoch total loss 1.22598243\n",
      "Trained batch 827 batch loss 1.3009007 epoch total loss 1.22607291\n",
      "Trained batch 828 batch loss 1.49689007 epoch total loss 1.2264\n",
      "Trained batch 829 batch loss 1.49039316 epoch total loss 1.22671854\n",
      "Trained batch 830 batch loss 1.31717753 epoch total loss 1.2268275\n",
      "Trained batch 831 batch loss 1.35524607 epoch total loss 1.226982\n",
      "Trained batch 832 batch loss 1.2327019 epoch total loss 1.22698891\n",
      "Trained batch 833 batch loss 1.15472341 epoch total loss 1.22690213\n",
      "Trained batch 834 batch loss 1.1205976 epoch total loss 1.22677469\n",
      "Trained batch 835 batch loss 1.19131374 epoch total loss 1.22673225\n",
      "Trained batch 836 batch loss 1.29255 epoch total loss 1.22681105\n",
      "Trained batch 837 batch loss 1.28555381 epoch total loss 1.22688115\n",
      "Trained batch 838 batch loss 1.24319696 epoch total loss 1.22690058\n",
      "Trained batch 839 batch loss 1.10342908 epoch total loss 1.22675335\n",
      "Trained batch 840 batch loss 1.22837543 epoch total loss 1.22675538\n",
      "Trained batch 841 batch loss 1.10194206 epoch total loss 1.22660697\n",
      "Trained batch 842 batch loss 1.13742983 epoch total loss 1.22650099\n",
      "Trained batch 843 batch loss 1.1791445 epoch total loss 1.22644496\n",
      "Trained batch 844 batch loss 1.11409831 epoch total loss 1.2263118\n",
      "Trained batch 845 batch loss 1.17648387 epoch total loss 1.22625291\n",
      "Trained batch 846 batch loss 1.11574304 epoch total loss 1.22612226\n",
      "Trained batch 847 batch loss 1.25898564 epoch total loss 1.22616112\n",
      "Trained batch 848 batch loss 1.13496494 epoch total loss 1.2260536\n",
      "Trained batch 849 batch loss 1.13497293 epoch total loss 1.22594643\n",
      "Trained batch 850 batch loss 1.0793494 epoch total loss 1.22577393\n",
      "Trained batch 851 batch loss 1.14393938 epoch total loss 1.22567773\n",
      "Trained batch 852 batch loss 1.12692571 epoch total loss 1.22556186\n",
      "Trained batch 853 batch loss 1.07089448 epoch total loss 1.22538054\n",
      "Trained batch 854 batch loss 1.13690519 epoch total loss 1.22527707\n",
      "Trained batch 855 batch loss 1.06259477 epoch total loss 1.22508681\n",
      "Trained batch 856 batch loss 1.12384176 epoch total loss 1.22496855\n",
      "Trained batch 857 batch loss 1.08147502 epoch total loss 1.22480106\n",
      "Trained batch 858 batch loss 1.06269395 epoch total loss 1.22461224\n",
      "Trained batch 859 batch loss 1.27121723 epoch total loss 1.22466648\n",
      "Trained batch 860 batch loss 1.56516 epoch total loss 1.22506249\n",
      "Trained batch 861 batch loss 1.17958236 epoch total loss 1.22500956\n",
      "Trained batch 862 batch loss 1.29064846 epoch total loss 1.22508574\n",
      "Trained batch 863 batch loss 1.31519008 epoch total loss 1.22519016\n",
      "Trained batch 864 batch loss 1.35201 epoch total loss 1.22533703\n",
      "Trained batch 865 batch loss 1.23327363 epoch total loss 1.22534621\n",
      "Trained batch 866 batch loss 1.23948956 epoch total loss 1.22536254\n",
      "Trained batch 867 batch loss 1.24516904 epoch total loss 1.22538531\n",
      "Trained batch 868 batch loss 1.26276016 epoch total loss 1.22542846\n",
      "Trained batch 869 batch loss 1.16269433 epoch total loss 1.22535622\n",
      "Trained batch 870 batch loss 1.27626765 epoch total loss 1.22541475\n",
      "Trained batch 871 batch loss 1.26647258 epoch total loss 1.22546196\n",
      "Trained batch 872 batch loss 1.24522889 epoch total loss 1.22548461\n",
      "Trained batch 873 batch loss 1.31756234 epoch total loss 1.22559\n",
      "Trained batch 874 batch loss 1.27365732 epoch total loss 1.22564507\n",
      "Trained batch 875 batch loss 1.29113007 epoch total loss 1.22571981\n",
      "Trained batch 876 batch loss 1.42222309 epoch total loss 1.22594416\n",
      "Trained batch 877 batch loss 1.23765302 epoch total loss 1.22595763\n",
      "Trained batch 878 batch loss 1.29414487 epoch total loss 1.22603524\n",
      "Trained batch 879 batch loss 1.26172769 epoch total loss 1.22607589\n",
      "Trained batch 880 batch loss 1.33483386 epoch total loss 1.22619951\n",
      "Trained batch 881 batch loss 1.27144623 epoch total loss 1.22625089\n",
      "Trained batch 882 batch loss 1.1275121 epoch total loss 1.22613895\n",
      "Trained batch 883 batch loss 1.06920099 epoch total loss 1.22596133\n",
      "Trained batch 884 batch loss 1.0638684 epoch total loss 1.22577786\n",
      "Trained batch 885 batch loss 1.17428279 epoch total loss 1.22571969\n",
      "Trained batch 886 batch loss 1.07450891 epoch total loss 1.22554898\n",
      "Trained batch 887 batch loss 1.15740061 epoch total loss 1.22547209\n",
      "Trained batch 888 batch loss 1.10876024 epoch total loss 1.22534072\n",
      "Trained batch 889 batch loss 1.16116631 epoch total loss 1.22526848\n",
      "Trained batch 890 batch loss 1.23976302 epoch total loss 1.2252847\n",
      "Trained batch 891 batch loss 1.19480324 epoch total loss 1.2252506\n",
      "Trained batch 892 batch loss 1.17798924 epoch total loss 1.22519755\n",
      "Trained batch 893 batch loss 1.07400012 epoch total loss 1.22502816\n",
      "Trained batch 894 batch loss 1.1269449 epoch total loss 1.22491848\n",
      "Trained batch 895 batch loss 1.002599 epoch total loss 1.22467\n",
      "Trained batch 896 batch loss 1.22162962 epoch total loss 1.22466671\n",
      "Trained batch 897 batch loss 1.23769212 epoch total loss 1.22468126\n",
      "Trained batch 898 batch loss 1.37459612 epoch total loss 1.22484827\n",
      "Trained batch 899 batch loss 1.41638076 epoch total loss 1.2250613\n",
      "Trained batch 900 batch loss 1.36399436 epoch total loss 1.22521567\n",
      "Trained batch 901 batch loss 1.23304105 epoch total loss 1.22522438\n",
      "Trained batch 902 batch loss 1.2833159 epoch total loss 1.22528875\n",
      "Trained batch 903 batch loss 1.11431158 epoch total loss 1.22516584\n",
      "Trained batch 904 batch loss 1.11076784 epoch total loss 1.22503924\n",
      "Trained batch 905 batch loss 1.25550103 epoch total loss 1.22507286\n",
      "Trained batch 906 batch loss 1.10223603 epoch total loss 1.22493732\n",
      "Trained batch 907 batch loss 1.28376877 epoch total loss 1.22500229\n",
      "Trained batch 908 batch loss 1.31531942 epoch total loss 1.22510171\n",
      "Trained batch 909 batch loss 1.14620697 epoch total loss 1.22501493\n",
      "Trained batch 910 batch loss 1.28397477 epoch total loss 1.22507966\n",
      "Trained batch 911 batch loss 1.27757478 epoch total loss 1.22513735\n",
      "Trained batch 912 batch loss 1.14195251 epoch total loss 1.22504616\n",
      "Trained batch 913 batch loss 1.28464675 epoch total loss 1.22511148\n",
      "Trained batch 914 batch loss 1.237427 epoch total loss 1.22512496\n",
      "Trained batch 915 batch loss 1.41047168 epoch total loss 1.22532749\n",
      "Trained batch 916 batch loss 1.34982479 epoch total loss 1.22546351\n",
      "Trained batch 917 batch loss 1.35653 epoch total loss 1.22560644\n",
      "Trained batch 918 batch loss 1.28979588 epoch total loss 1.22567642\n",
      "Trained batch 919 batch loss 1.22699738 epoch total loss 1.22567785\n",
      "Trained batch 920 batch loss 1.18862903 epoch total loss 1.22563756\n",
      "Trained batch 921 batch loss 1.22303164 epoch total loss 1.22563469\n",
      "Trained batch 922 batch loss 1.25923157 epoch total loss 1.22567117\n",
      "Trained batch 923 batch loss 1.35123515 epoch total loss 1.22580719\n",
      "Trained batch 924 batch loss 1.24155641 epoch total loss 1.22582424\n",
      "Trained batch 925 batch loss 1.41694248 epoch total loss 1.22603095\n",
      "Trained batch 926 batch loss 1.38114309 epoch total loss 1.22619843\n",
      "Trained batch 927 batch loss 1.44029331 epoch total loss 1.22642934\n",
      "Trained batch 928 batch loss 1.39797127 epoch total loss 1.22661424\n",
      "Trained batch 929 batch loss 1.4104079 epoch total loss 1.22681201\n",
      "Trained batch 930 batch loss 1.34409618 epoch total loss 1.22693813\n",
      "Trained batch 931 batch loss 1.31674278 epoch total loss 1.22703469\n",
      "Trained batch 932 batch loss 1.39448547 epoch total loss 1.22721434\n",
      "Trained batch 933 batch loss 1.3895421 epoch total loss 1.22738838\n",
      "Trained batch 934 batch loss 1.38211036 epoch total loss 1.22755396\n",
      "Trained batch 935 batch loss 1.20908761 epoch total loss 1.22753417\n",
      "Trained batch 936 batch loss 1.01776385 epoch total loss 1.22731018\n",
      "Trained batch 937 batch loss 1.33099079 epoch total loss 1.22742081\n",
      "Trained batch 938 batch loss 1.27278829 epoch total loss 1.22746921\n",
      "Trained batch 939 batch loss 1.26309621 epoch total loss 1.22750711\n",
      "Trained batch 940 batch loss 1.35209823 epoch total loss 1.22763956\n",
      "Trained batch 941 batch loss 1.32756269 epoch total loss 1.22774577\n",
      "Trained batch 942 batch loss 1.23026454 epoch total loss 1.22774839\n",
      "Trained batch 943 batch loss 1.2353425 epoch total loss 1.22775638\n",
      "Trained batch 944 batch loss 1.19845533 epoch total loss 1.22772539\n",
      "Trained batch 945 batch loss 1.29473877 epoch total loss 1.22779644\n",
      "Trained batch 946 batch loss 1.23223186 epoch total loss 1.22780097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 947 batch loss 1.20605326 epoch total loss 1.22777808\n",
      "Trained batch 948 batch loss 1.21184015 epoch total loss 1.22776115\n",
      "Trained batch 949 batch loss 1.23471498 epoch total loss 1.22776854\n",
      "Trained batch 950 batch loss 1.26342905 epoch total loss 1.22780609\n",
      "Trained batch 951 batch loss 1.19981253 epoch total loss 1.22777665\n",
      "Trained batch 952 batch loss 1.35252714 epoch total loss 1.22790766\n",
      "Trained batch 953 batch loss 1.27651381 epoch total loss 1.22795868\n",
      "Trained batch 954 batch loss 1.28339207 epoch total loss 1.22801685\n",
      "Trained batch 955 batch loss 1.28300381 epoch total loss 1.22807443\n",
      "Trained batch 956 batch loss 1.17479455 epoch total loss 1.22801864\n",
      "Trained batch 957 batch loss 1.18289089 epoch total loss 1.22797143\n",
      "Trained batch 958 batch loss 0.986043811 epoch total loss 1.22771895\n",
      "Trained batch 959 batch loss 1.04363549 epoch total loss 1.22752702\n",
      "Trained batch 960 batch loss 1.15715766 epoch total loss 1.22745359\n",
      "Trained batch 961 batch loss 1.23970723 epoch total loss 1.22746646\n",
      "Trained batch 962 batch loss 1.32943726 epoch total loss 1.22757244\n",
      "Trained batch 963 batch loss 1.5241437 epoch total loss 1.22788048\n",
      "Trained batch 964 batch loss 1.13500476 epoch total loss 1.22778404\n",
      "Trained batch 965 batch loss 1.07046962 epoch total loss 1.22762108\n",
      "Trained batch 966 batch loss 1.2278322 epoch total loss 1.2276212\n",
      "Trained batch 967 batch loss 1.33322656 epoch total loss 1.22773039\n",
      "Trained batch 968 batch loss 1.29549289 epoch total loss 1.22780049\n",
      "Trained batch 969 batch loss 1.18506527 epoch total loss 1.22775638\n",
      "Trained batch 970 batch loss 1.30305886 epoch total loss 1.22783399\n",
      "Trained batch 971 batch loss 1.47438121 epoch total loss 1.2280879\n",
      "Trained batch 972 batch loss 1.23087764 epoch total loss 1.22809076\n",
      "Trained batch 973 batch loss 1.25236869 epoch total loss 1.22811568\n",
      "Trained batch 974 batch loss 1.23341894 epoch total loss 1.22812104\n",
      "Trained batch 975 batch loss 1.14685 epoch total loss 1.22803771\n",
      "Trained batch 976 batch loss 1.14445162 epoch total loss 1.227952\n",
      "Trained batch 977 batch loss 1.23552871 epoch total loss 1.22795975\n",
      "Trained batch 978 batch loss 1.13386214 epoch total loss 1.22786355\n",
      "Trained batch 979 batch loss 1.30800223 epoch total loss 1.22794545\n",
      "Trained batch 980 batch loss 1.1820519 epoch total loss 1.2278986\n",
      "Trained batch 981 batch loss 1.24036181 epoch total loss 1.22791123\n",
      "Trained batch 982 batch loss 1.3811543 epoch total loss 1.22806728\n",
      "Trained batch 983 batch loss 1.28480494 epoch total loss 1.22812498\n",
      "Trained batch 984 batch loss 1.19718611 epoch total loss 1.2280935\n",
      "Trained batch 985 batch loss 1.36098039 epoch total loss 1.22822833\n",
      "Trained batch 986 batch loss 1.15159357 epoch total loss 1.22815061\n",
      "Trained batch 987 batch loss 1.24614108 epoch total loss 1.22816885\n",
      "Trained batch 988 batch loss 1.24256253 epoch total loss 1.22818339\n",
      "Trained batch 989 batch loss 1.18735027 epoch total loss 1.22814214\n",
      "Trained batch 990 batch loss 1.21022415 epoch total loss 1.22812402\n",
      "Trained batch 991 batch loss 1.28570807 epoch total loss 1.2281822\n",
      "Trained batch 992 batch loss 1.24975228 epoch total loss 1.22820389\n",
      "Trained batch 993 batch loss 1.36938858 epoch total loss 1.22834611\n",
      "Trained batch 994 batch loss 1.24829233 epoch total loss 1.22836614\n",
      "Trained batch 995 batch loss 1.26461053 epoch total loss 1.22840261\n",
      "Trained batch 996 batch loss 1.27411485 epoch total loss 1.22844863\n",
      "Trained batch 997 batch loss 1.27042782 epoch total loss 1.22849059\n",
      "Trained batch 998 batch loss 1.15651536 epoch total loss 1.22841847\n",
      "Trained batch 999 batch loss 1.21670663 epoch total loss 1.22840679\n",
      "Trained batch 1000 batch loss 1.20275974 epoch total loss 1.22838116\n",
      "Trained batch 1001 batch loss 1.13405681 epoch total loss 1.22828686\n",
      "Trained batch 1002 batch loss 1.17540908 epoch total loss 1.22823405\n",
      "Trained batch 1003 batch loss 1.22148061 epoch total loss 1.22822726\n",
      "Trained batch 1004 batch loss 1.12726426 epoch total loss 1.22812676\n",
      "Trained batch 1005 batch loss 1.08452415 epoch total loss 1.22798383\n",
      "Trained batch 1006 batch loss 1.25245738 epoch total loss 1.22800815\n",
      "Trained batch 1007 batch loss 1.11538029 epoch total loss 1.22789633\n",
      "Trained batch 1008 batch loss 1.10682607 epoch total loss 1.22777617\n",
      "Trained batch 1009 batch loss 1.16589427 epoch total loss 1.2277149\n",
      "Trained batch 1010 batch loss 1.11099064 epoch total loss 1.22759926\n",
      "Trained batch 1011 batch loss 1.14143825 epoch total loss 1.22751403\n",
      "Trained batch 1012 batch loss 1.14893341 epoch total loss 1.22743642\n",
      "Trained batch 1013 batch loss 1.14924014 epoch total loss 1.22735929\n",
      "Trained batch 1014 batch loss 1.25192165 epoch total loss 1.22738349\n",
      "Trained batch 1015 batch loss 1.26314449 epoch total loss 1.22741878\n",
      "Trained batch 1016 batch loss 1.27221239 epoch total loss 1.22746289\n",
      "Trained batch 1017 batch loss 1.31586337 epoch total loss 1.22754991\n",
      "Trained batch 1018 batch loss 1.1952703 epoch total loss 1.2275182\n",
      "Trained batch 1019 batch loss 1.29119515 epoch total loss 1.22758067\n",
      "Trained batch 1020 batch loss 1.19378054 epoch total loss 1.22754741\n",
      "Trained batch 1021 batch loss 1.14118767 epoch total loss 1.22746289\n",
      "Trained batch 1022 batch loss 1.06832159 epoch total loss 1.2273072\n",
      "Trained batch 1023 batch loss 1.15107882 epoch total loss 1.22723269\n",
      "Trained batch 1024 batch loss 1.34754157 epoch total loss 1.22735023\n",
      "Trained batch 1025 batch loss 1.24032903 epoch total loss 1.22736287\n",
      "Trained batch 1026 batch loss 1.23208809 epoch total loss 1.22736752\n",
      "Trained batch 1027 batch loss 1.27597785 epoch total loss 1.22741485\n",
      "Trained batch 1028 batch loss 1.24538922 epoch total loss 1.22743225\n",
      "Trained batch 1029 batch loss 1.2924217 epoch total loss 1.22749555\n",
      "Trained batch 1030 batch loss 1.23622584 epoch total loss 1.22750401\n",
      "Trained batch 1031 batch loss 1.19352174 epoch total loss 1.22747099\n",
      "Trained batch 1032 batch loss 1.1492753 epoch total loss 1.22739518\n",
      "Trained batch 1033 batch loss 1.1835227 epoch total loss 1.22735274\n",
      "Trained batch 1034 batch loss 1.13171387 epoch total loss 1.22726023\n",
      "Trained batch 1035 batch loss 1.13475895 epoch total loss 1.22717083\n",
      "Trained batch 1036 batch loss 1.31725347 epoch total loss 1.22725785\n",
      "Trained batch 1037 batch loss 1.11053181 epoch total loss 1.2271452\n",
      "Trained batch 1038 batch loss 1.11795366 epoch total loss 1.22703993\n",
      "Trained batch 1039 batch loss 1.07725227 epoch total loss 1.22689581\n",
      "Trained batch 1040 batch loss 1.17509604 epoch total loss 1.22684598\n",
      "Trained batch 1041 batch loss 1.25554323 epoch total loss 1.22687352\n",
      "Trained batch 1042 batch loss 1.24161506 epoch total loss 1.22688758\n",
      "Trained batch 1043 batch loss 1.22725058 epoch total loss 1.22688794\n",
      "Trained batch 1044 batch loss 1.19470286 epoch total loss 1.22685719\n",
      "Trained batch 1045 batch loss 1.31451023 epoch total loss 1.22694099\n",
      "Trained batch 1046 batch loss 1.11551666 epoch total loss 1.22683442\n",
      "Trained batch 1047 batch loss 1.19497156 epoch total loss 1.22680402\n",
      "Trained batch 1048 batch loss 1.20659447 epoch total loss 1.22678459\n",
      "Trained batch 1049 batch loss 1.12005293 epoch total loss 1.22668278\n",
      "Trained batch 1050 batch loss 1.06020546 epoch total loss 1.22652423\n",
      "Trained batch 1051 batch loss 1.01695478 epoch total loss 1.22632492\n",
      "Trained batch 1052 batch loss 1.06529272 epoch total loss 1.22617185\n",
      "Trained batch 1053 batch loss 1.16455102 epoch total loss 1.22611332\n",
      "Trained batch 1054 batch loss 1.05812716 epoch total loss 1.22595394\n",
      "Trained batch 1055 batch loss 1.13160241 epoch total loss 1.22586441\n",
      "Trained batch 1056 batch loss 1.04829443 epoch total loss 1.22569633\n",
      "Trained batch 1057 batch loss 1.10907793 epoch total loss 1.22558606\n",
      "Trained batch 1058 batch loss 1.20772839 epoch total loss 1.22556925\n",
      "Trained batch 1059 batch loss 1.25103939 epoch total loss 1.22559333\n",
      "Trained batch 1060 batch loss 1.4431448 epoch total loss 1.22579849\n",
      "Trained batch 1061 batch loss 1.34776068 epoch total loss 1.22591352\n",
      "Trained batch 1062 batch loss 1.27339923 epoch total loss 1.22595823\n",
      "Trained batch 1063 batch loss 1.05094 epoch total loss 1.2257936\n",
      "Trained batch 1064 batch loss 0.951504588 epoch total loss 1.22553575\n",
      "Trained batch 1065 batch loss 1.28718328 epoch total loss 1.22559369\n",
      "Trained batch 1066 batch loss 1.27385139 epoch total loss 1.22563899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1067 batch loss 1.21685505 epoch total loss 1.22563064\n",
      "Trained batch 1068 batch loss 1.28692126 epoch total loss 1.22568798\n",
      "Trained batch 1069 batch loss 1.26174045 epoch total loss 1.22572172\n",
      "Trained batch 1070 batch loss 1.14597487 epoch total loss 1.22564721\n",
      "Trained batch 1071 batch loss 1.16735661 epoch total loss 1.22559273\n",
      "Trained batch 1072 batch loss 1.11959553 epoch total loss 1.22549391\n",
      "Trained batch 1073 batch loss 1.16076684 epoch total loss 1.22543359\n",
      "Trained batch 1074 batch loss 1.26515853 epoch total loss 1.22547054\n",
      "Trained batch 1075 batch loss 1.23502767 epoch total loss 1.22547948\n",
      "Trained batch 1076 batch loss 1.38814664 epoch total loss 1.22563064\n",
      "Trained batch 1077 batch loss 1.21567202 epoch total loss 1.22562146\n",
      "Trained batch 1078 batch loss 1.25553918 epoch total loss 1.22564912\n",
      "Trained batch 1079 batch loss 1.26042736 epoch total loss 1.2256813\n",
      "Trained batch 1080 batch loss 1.44655252 epoch total loss 1.22588575\n",
      "Trained batch 1081 batch loss 1.31167316 epoch total loss 1.22596514\n",
      "Trained batch 1082 batch loss 1.43628991 epoch total loss 1.22615945\n",
      "Trained batch 1083 batch loss 1.26507139 epoch total loss 1.22619534\n",
      "Trained batch 1084 batch loss 1.21936536 epoch total loss 1.22618914\n",
      "Trained batch 1085 batch loss 1.04421639 epoch total loss 1.22602129\n",
      "Trained batch 1086 batch loss 1.02305543 epoch total loss 1.22583449\n",
      "Trained batch 1087 batch loss 1.30812931 epoch total loss 1.22591019\n",
      "Trained batch 1088 batch loss 1.31668699 epoch total loss 1.22599351\n",
      "Trained batch 1089 batch loss 1.31727922 epoch total loss 1.22607732\n",
      "Trained batch 1090 batch loss 1.22674656 epoch total loss 1.22607803\n",
      "Trained batch 1091 batch loss 1.39231062 epoch total loss 1.22623038\n",
      "Trained batch 1092 batch loss 1.30198646 epoch total loss 1.22629976\n",
      "Trained batch 1093 batch loss 1.24013352 epoch total loss 1.2263124\n",
      "Trained batch 1094 batch loss 1.30106401 epoch total loss 1.22638071\n",
      "Trained batch 1095 batch loss 1.36886311 epoch total loss 1.22651088\n",
      "Trained batch 1096 batch loss 1.29474735 epoch total loss 1.22657323\n",
      "Trained batch 1097 batch loss 1.12701726 epoch total loss 1.22648251\n",
      "Trained batch 1098 batch loss 1.108325 epoch total loss 1.22637486\n",
      "Trained batch 1099 batch loss 1.12112689 epoch total loss 1.22627902\n",
      "Trained batch 1100 batch loss 1.14592767 epoch total loss 1.22620595\n",
      "Trained batch 1101 batch loss 1.18994534 epoch total loss 1.22617304\n",
      "Trained batch 1102 batch loss 1.15292048 epoch total loss 1.22610652\n",
      "Trained batch 1103 batch loss 1.27394581 epoch total loss 1.22614992\n",
      "Trained batch 1104 batch loss 1.42986345 epoch total loss 1.22633433\n",
      "Trained batch 1105 batch loss 1.16587615 epoch total loss 1.22627974\n",
      "Trained batch 1106 batch loss 1.20701289 epoch total loss 1.22626233\n",
      "Trained batch 1107 batch loss 1.20245647 epoch total loss 1.22624087\n",
      "Trained batch 1108 batch loss 1.256477 epoch total loss 1.22626817\n",
      "Trained batch 1109 batch loss 1.22875106 epoch total loss 1.22627032\n",
      "Trained batch 1110 batch loss 1.31485415 epoch total loss 1.22635019\n",
      "Trained batch 1111 batch loss 1.34490156 epoch total loss 1.22645676\n",
      "Trained batch 1112 batch loss 1.44369614 epoch total loss 1.22665215\n",
      "Trained batch 1113 batch loss 1.47251058 epoch total loss 1.22687316\n",
      "Trained batch 1114 batch loss 1.30005479 epoch total loss 1.22693884\n",
      "Trained batch 1115 batch loss 1.06991553 epoch total loss 1.22679794\n",
      "Trained batch 1116 batch loss 1.24445987 epoch total loss 1.22681391\n",
      "Trained batch 1117 batch loss 1.09026241 epoch total loss 1.2266916\n",
      "Trained batch 1118 batch loss 1.09798515 epoch total loss 1.22657645\n",
      "Trained batch 1119 batch loss 1.12902665 epoch total loss 1.22648931\n",
      "Trained batch 1120 batch loss 1.15983248 epoch total loss 1.2264297\n",
      "Trained batch 1121 batch loss 1.06480956 epoch total loss 1.22628558\n",
      "Trained batch 1122 batch loss 1.08190119 epoch total loss 1.22615695\n",
      "Trained batch 1123 batch loss 1.02011359 epoch total loss 1.22597349\n",
      "Trained batch 1124 batch loss 1.18450332 epoch total loss 1.22593653\n",
      "Trained batch 1125 batch loss 1.2150296 epoch total loss 1.22592688\n",
      "Trained batch 1126 batch loss 1.10029006 epoch total loss 1.2258153\n",
      "Trained batch 1127 batch loss 0.98417 epoch total loss 1.22560084\n",
      "Trained batch 1128 batch loss 1.19848537 epoch total loss 1.22557688\n",
      "Trained batch 1129 batch loss 1.18472767 epoch total loss 1.22554064\n",
      "Trained batch 1130 batch loss 1.19067 epoch total loss 1.22550976\n",
      "Trained batch 1131 batch loss 1.10369051 epoch total loss 1.225402\n",
      "Trained batch 1132 batch loss 1.06968343 epoch total loss 1.22526443\n",
      "Trained batch 1133 batch loss 1.03678632 epoch total loss 1.22509813\n",
      "Trained batch 1134 batch loss 1.10507095 epoch total loss 1.22499228\n",
      "Trained batch 1135 batch loss 1.11983061 epoch total loss 1.22489965\n",
      "Trained batch 1136 batch loss 1.15541184 epoch total loss 1.2248385\n",
      "Trained batch 1137 batch loss 1.14818299 epoch total loss 1.22477102\n",
      "Trained batch 1138 batch loss 1.13033223 epoch total loss 1.22468805\n",
      "Trained batch 1139 batch loss 1.05358434 epoch total loss 1.22453785\n",
      "Trained batch 1140 batch loss 1.09862578 epoch total loss 1.22442746\n",
      "Trained batch 1141 batch loss 1.14212298 epoch total loss 1.22435534\n",
      "Trained batch 1142 batch loss 1.28581023 epoch total loss 1.2244091\n",
      "Trained batch 1143 batch loss 1.32356882 epoch total loss 1.22449589\n",
      "Trained batch 1144 batch loss 1.32106709 epoch total loss 1.22458029\n",
      "Trained batch 1145 batch loss 1.25473094 epoch total loss 1.22460663\n",
      "Trained batch 1146 batch loss 1.22678792 epoch total loss 1.22460854\n",
      "Trained batch 1147 batch loss 1.17937589 epoch total loss 1.22456908\n",
      "Trained batch 1148 batch loss 1.19609928 epoch total loss 1.22454417\n",
      "Trained batch 1149 batch loss 1.14749932 epoch total loss 1.22447705\n",
      "Trained batch 1150 batch loss 1.19730639 epoch total loss 1.22445345\n",
      "Trained batch 1151 batch loss 1.17334354 epoch total loss 1.22440898\n",
      "Trained batch 1152 batch loss 1.22539806 epoch total loss 1.22440982\n",
      "Trained batch 1153 batch loss 1.21547854 epoch total loss 1.22440207\n",
      "Trained batch 1154 batch loss 1.27462459 epoch total loss 1.22444558\n",
      "Trained batch 1155 batch loss 1.30074549 epoch total loss 1.22451174\n",
      "Trained batch 1156 batch loss 1.19656754 epoch total loss 1.22448754\n",
      "Trained batch 1157 batch loss 1.16351283 epoch total loss 1.22443473\n",
      "Trained batch 1158 batch loss 1.15170908 epoch total loss 1.22437203\n",
      "Trained batch 1159 batch loss 1.03683138 epoch total loss 1.22421014\n",
      "Trained batch 1160 batch loss 1.12459064 epoch total loss 1.22412431\n",
      "Trained batch 1161 batch loss 1.13445783 epoch total loss 1.22404706\n",
      "Trained batch 1162 batch loss 1.14379215 epoch total loss 1.22397804\n",
      "Trained batch 1163 batch loss 1.15053451 epoch total loss 1.22391486\n",
      "Trained batch 1164 batch loss 1.18258011 epoch total loss 1.22387934\n",
      "Trained batch 1165 batch loss 1.18480301 epoch total loss 1.22384584\n",
      "Trained batch 1166 batch loss 1.02805662 epoch total loss 1.22367799\n",
      "Trained batch 1167 batch loss 1.14173174 epoch total loss 1.22360766\n",
      "Trained batch 1168 batch loss 1.25401914 epoch total loss 1.22363377\n",
      "Trained batch 1169 batch loss 1.3512969 epoch total loss 1.22374296\n",
      "Trained batch 1170 batch loss 1.24408662 epoch total loss 1.22376037\n",
      "Trained batch 1171 batch loss 1.25065112 epoch total loss 1.22378337\n",
      "Trained batch 1172 batch loss 1.20441031 epoch total loss 1.2237668\n",
      "Trained batch 1173 batch loss 1.2578367 epoch total loss 1.22379589\n",
      "Trained batch 1174 batch loss 1.23412895 epoch total loss 1.22380471\n",
      "Trained batch 1175 batch loss 1.17207825 epoch total loss 1.22376072\n",
      "Trained batch 1176 batch loss 1.34879351 epoch total loss 1.22386694\n",
      "Trained batch 1177 batch loss 1.3998723 epoch total loss 1.22401655\n",
      "Trained batch 1178 batch loss 1.22735631 epoch total loss 1.22401941\n",
      "Trained batch 1179 batch loss 1.27558076 epoch total loss 1.22406316\n",
      "Trained batch 1180 batch loss 1.36504698 epoch total loss 1.22418261\n",
      "Trained batch 1181 batch loss 1.34318805 epoch total loss 1.22428334\n",
      "Trained batch 1182 batch loss 1.1850214 epoch total loss 1.2242502\n",
      "Trained batch 1183 batch loss 1.18673944 epoch total loss 1.22421849\n",
      "Trained batch 1184 batch loss 1.12921691 epoch total loss 1.22413826\n",
      "Trained batch 1185 batch loss 1.20557606 epoch total loss 1.22412264\n",
      "Trained batch 1186 batch loss 1.28452265 epoch total loss 1.22417355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1187 batch loss 1.26065779 epoch total loss 1.2242043\n",
      "Trained batch 1188 batch loss 1.25412178 epoch total loss 1.22422945\n",
      "Trained batch 1189 batch loss 1.16710949 epoch total loss 1.22418141\n",
      "Trained batch 1190 batch loss 1.17778957 epoch total loss 1.22414243\n",
      "Trained batch 1191 batch loss 1.201702 epoch total loss 1.2241236\n",
      "Trained batch 1192 batch loss 1.20130515 epoch total loss 1.2241044\n",
      "Trained batch 1193 batch loss 1.15734017 epoch total loss 1.2240485\n",
      "Trained batch 1194 batch loss 1.24888086 epoch total loss 1.22406924\n",
      "Trained batch 1195 batch loss 1.37732887 epoch total loss 1.22419751\n",
      "Trained batch 1196 batch loss 1.19215393 epoch total loss 1.22417068\n",
      "Trained batch 1197 batch loss 1.26427174 epoch total loss 1.22420418\n",
      "Trained batch 1198 batch loss 1.31106615 epoch total loss 1.22427666\n",
      "Trained batch 1199 batch loss 1.20907009 epoch total loss 1.22426403\n",
      "Trained batch 1200 batch loss 1.20912063 epoch total loss 1.22425139\n",
      "Trained batch 1201 batch loss 1.4089582 epoch total loss 1.22440517\n",
      "Trained batch 1202 batch loss 1.3471415 epoch total loss 1.22450733\n",
      "Trained batch 1203 batch loss 1.32268476 epoch total loss 1.22458887\n",
      "Trained batch 1204 batch loss 1.22380137 epoch total loss 1.22458816\n",
      "Trained batch 1205 batch loss 1.26469779 epoch total loss 1.22462142\n",
      "Trained batch 1206 batch loss 1.24369609 epoch total loss 1.22463715\n",
      "Trained batch 1207 batch loss 1.30962873 epoch total loss 1.2247076\n",
      "Trained batch 1208 batch loss 1.22997594 epoch total loss 1.22471189\n",
      "Trained batch 1209 batch loss 1.21593869 epoch total loss 1.22470474\n",
      "Trained batch 1210 batch loss 1.23123455 epoch total loss 1.22471011\n",
      "Trained batch 1211 batch loss 1.25662756 epoch total loss 1.22473633\n",
      "Trained batch 1212 batch loss 1.29057205 epoch total loss 1.22479069\n",
      "Trained batch 1213 batch loss 1.27158749 epoch total loss 1.22482932\n",
      "Trained batch 1214 batch loss 1.1426053 epoch total loss 1.22476149\n",
      "Trained batch 1215 batch loss 1.19829071 epoch total loss 1.22473967\n",
      "Trained batch 1216 batch loss 1.20649719 epoch total loss 1.22472477\n",
      "Trained batch 1217 batch loss 1.24854207 epoch total loss 1.22474432\n",
      "Trained batch 1218 batch loss 1.23099899 epoch total loss 1.22474933\n",
      "Trained batch 1219 batch loss 1.19058204 epoch total loss 1.22472131\n",
      "Trained batch 1220 batch loss 1.0228 epoch total loss 1.22455585\n",
      "Trained batch 1221 batch loss 1.05155098 epoch total loss 1.22441411\n",
      "Trained batch 1222 batch loss 1.26005185 epoch total loss 1.2244432\n",
      "Trained batch 1223 batch loss 1.24762511 epoch total loss 1.22446227\n",
      "Trained batch 1224 batch loss 1.30212319 epoch total loss 1.22452569\n",
      "Trained batch 1225 batch loss 1.35592508 epoch total loss 1.22463298\n",
      "Trained batch 1226 batch loss 1.33922923 epoch total loss 1.22472644\n",
      "Trained batch 1227 batch loss 1.25708902 epoch total loss 1.22475278\n",
      "Trained batch 1228 batch loss 1.20983863 epoch total loss 1.22474074\n",
      "Trained batch 1229 batch loss 1.30271494 epoch total loss 1.22480416\n",
      "Trained batch 1230 batch loss 1.27458334 epoch total loss 1.22484457\n",
      "Trained batch 1231 batch loss 1.27676272 epoch total loss 1.22488678\n",
      "Trained batch 1232 batch loss 1.1561985 epoch total loss 1.22483099\n",
      "Trained batch 1233 batch loss 1.35842657 epoch total loss 1.22493935\n",
      "Trained batch 1234 batch loss 1.33888543 epoch total loss 1.22503173\n",
      "Trained batch 1235 batch loss 1.24609828 epoch total loss 1.22504878\n",
      "Trained batch 1236 batch loss 1.22732663 epoch total loss 1.22505057\n",
      "Trained batch 1237 batch loss 1.20774055 epoch total loss 1.22503662\n",
      "Trained batch 1238 batch loss 1.24186575 epoch total loss 1.22505009\n",
      "Trained batch 1239 batch loss 1.22028553 epoch total loss 1.22504628\n",
      "Trained batch 1240 batch loss 1.21514976 epoch total loss 1.22503841\n",
      "Trained batch 1241 batch loss 1.33107865 epoch total loss 1.22512376\n",
      "Trained batch 1242 batch loss 1.15816641 epoch total loss 1.22506988\n",
      "Trained batch 1243 batch loss 1.24419 epoch total loss 1.22508526\n",
      "Trained batch 1244 batch loss 1.20035481 epoch total loss 1.22506535\n",
      "Trained batch 1245 batch loss 1.28777909 epoch total loss 1.22511566\n",
      "Trained batch 1246 batch loss 1.04832149 epoch total loss 1.2249738\n",
      "Trained batch 1247 batch loss 1.13068223 epoch total loss 1.22489822\n",
      "Trained batch 1248 batch loss 1.14112663 epoch total loss 1.2248311\n",
      "Trained batch 1249 batch loss 1.35536551 epoch total loss 1.22493565\n",
      "Trained batch 1250 batch loss 1.39286387 epoch total loss 1.22506988\n",
      "Trained batch 1251 batch loss 1.34137774 epoch total loss 1.22516298\n",
      "Trained batch 1252 batch loss 1.37547147 epoch total loss 1.22528303\n",
      "Trained batch 1253 batch loss 1.25231147 epoch total loss 1.2253046\n",
      "Trained batch 1254 batch loss 1.24651718 epoch total loss 1.22532141\n",
      "Trained batch 1255 batch loss 1.02013052 epoch total loss 1.22515798\n",
      "Trained batch 1256 batch loss 0.993295193 epoch total loss 1.22497332\n",
      "Trained batch 1257 batch loss 0.986582637 epoch total loss 1.22478366\n",
      "Trained batch 1258 batch loss 1.19590604 epoch total loss 1.22476077\n",
      "Trained batch 1259 batch loss 1.37890458 epoch total loss 1.2248832\n",
      "Trained batch 1260 batch loss 1.28512907 epoch total loss 1.224931\n",
      "Trained batch 1261 batch loss 1.22041023 epoch total loss 1.22492743\n",
      "Trained batch 1262 batch loss 1.08405364 epoch total loss 1.22481585\n",
      "Trained batch 1263 batch loss 1.13453054 epoch total loss 1.22474444\n",
      "Trained batch 1264 batch loss 1.09301496 epoch total loss 1.22464013\n",
      "Trained batch 1265 batch loss 1.26396883 epoch total loss 1.22467124\n",
      "Trained batch 1266 batch loss 1.29058218 epoch total loss 1.22472322\n",
      "Trained batch 1267 batch loss 1.2075634 epoch total loss 1.22470963\n",
      "Trained batch 1268 batch loss 1.16585875 epoch total loss 1.22466326\n",
      "Trained batch 1269 batch loss 1.15746117 epoch total loss 1.22461033\n",
      "Trained batch 1270 batch loss 1.16297662 epoch total loss 1.22456181\n",
      "Trained batch 1271 batch loss 1.14025891 epoch total loss 1.22449541\n",
      "Trained batch 1272 batch loss 1.15479612 epoch total loss 1.22444069\n",
      "Trained batch 1273 batch loss 1.06983244 epoch total loss 1.22431922\n",
      "Trained batch 1274 batch loss 1.28443861 epoch total loss 1.22436643\n",
      "Trained batch 1275 batch loss 1.16936088 epoch total loss 1.22432315\n",
      "Trained batch 1276 batch loss 1.27210188 epoch total loss 1.22436059\n",
      "Trained batch 1277 batch loss 1.19247901 epoch total loss 1.22433567\n",
      "Trained batch 1278 batch loss 1.16775513 epoch total loss 1.22429144\n",
      "Trained batch 1279 batch loss 1.09768438 epoch total loss 1.22419238\n",
      "Trained batch 1280 batch loss 1.10236764 epoch total loss 1.22409725\n",
      "Trained batch 1281 batch loss 1.1281321 epoch total loss 1.22402239\n",
      "Trained batch 1282 batch loss 1.20749915 epoch total loss 1.22400951\n",
      "Trained batch 1283 batch loss 1.09400606 epoch total loss 1.22390819\n",
      "Trained batch 1284 batch loss 1.0900116 epoch total loss 1.22380388\n",
      "Trained batch 1285 batch loss 1.12334466 epoch total loss 1.22372568\n",
      "Trained batch 1286 batch loss 1.28231907 epoch total loss 1.22377121\n",
      "Trained batch 1287 batch loss 1.10537 epoch total loss 1.22367918\n",
      "Trained batch 1288 batch loss 1.18436134 epoch total loss 1.22364867\n",
      "Trained batch 1289 batch loss 0.961021543 epoch total loss 1.22344494\n",
      "Trained batch 1290 batch loss 0.945307 epoch total loss 1.22322929\n",
      "Trained batch 1291 batch loss 1.22221303 epoch total loss 1.22322845\n",
      "Trained batch 1292 batch loss 1.29570544 epoch total loss 1.2232846\n",
      "Trained batch 1293 batch loss 1.17235589 epoch total loss 1.22324514\n",
      "Trained batch 1294 batch loss 1.17962885 epoch total loss 1.22321153\n",
      "Trained batch 1295 batch loss 1.19785213 epoch total loss 1.22319198\n",
      "Trained batch 1296 batch loss 1.08727598 epoch total loss 1.22308707\n",
      "Trained batch 1297 batch loss 1.14496565 epoch total loss 1.22302687\n",
      "Trained batch 1298 batch loss 1.2236073 epoch total loss 1.22302735\n",
      "Trained batch 1299 batch loss 1.155478 epoch total loss 1.22297537\n",
      "Trained batch 1300 batch loss 1.28668475 epoch total loss 1.22302449\n",
      "Trained batch 1301 batch loss 1.11338973 epoch total loss 1.22294021\n",
      "Trained batch 1302 batch loss 1.21678042 epoch total loss 1.22293544\n",
      "Trained batch 1303 batch loss 1.45257783 epoch total loss 1.22311175\n",
      "Trained batch 1304 batch loss 1.32549024 epoch total loss 1.22319019\n",
      "Trained batch 1305 batch loss 1.40866709 epoch total loss 1.22333241\n",
      "Trained batch 1306 batch loss 1.35110128 epoch total loss 1.22343016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1307 batch loss 1.29281497 epoch total loss 1.22348332\n",
      "Trained batch 1308 batch loss 1.28534448 epoch total loss 1.22353065\n",
      "Trained batch 1309 batch loss 1.1997931 epoch total loss 1.22351253\n",
      "Trained batch 1310 batch loss 1.2550137 epoch total loss 1.22353661\n",
      "Trained batch 1311 batch loss 1.2217257 epoch total loss 1.22353518\n",
      "Trained batch 1312 batch loss 1.23007131 epoch total loss 1.22354019\n",
      "Trained batch 1313 batch loss 1.30903876 epoch total loss 1.22360528\n",
      "Trained batch 1314 batch loss 1.26206422 epoch total loss 1.2236346\n",
      "Trained batch 1315 batch loss 1.14207196 epoch total loss 1.22357261\n",
      "Trained batch 1316 batch loss 1.12594557 epoch total loss 1.22349846\n",
      "Trained batch 1317 batch loss 1.10243154 epoch total loss 1.22340643\n",
      "Trained batch 1318 batch loss 1.00523388 epoch total loss 1.22324097\n",
      "Trained batch 1319 batch loss 1.32152069 epoch total loss 1.22331548\n",
      "Trained batch 1320 batch loss 1.16266322 epoch total loss 1.22326958\n",
      "Trained batch 1321 batch loss 1.39779866 epoch total loss 1.22340167\n",
      "Trained batch 1322 batch loss 1.29890347 epoch total loss 1.22345889\n",
      "Trained batch 1323 batch loss 1.23484576 epoch total loss 1.22346747\n",
      "Trained batch 1324 batch loss 1.21195865 epoch total loss 1.22345877\n",
      "Trained batch 1325 batch loss 1.15092492 epoch total loss 1.22340393\n",
      "Trained batch 1326 batch loss 1.12889802 epoch total loss 1.22333264\n",
      "Trained batch 1327 batch loss 1.29428434 epoch total loss 1.22338617\n",
      "Trained batch 1328 batch loss 1.30847681 epoch total loss 1.2234503\n",
      "Trained batch 1329 batch loss 1.26461768 epoch total loss 1.2234813\n",
      "Trained batch 1330 batch loss 1.16741896 epoch total loss 1.2234391\n",
      "Trained batch 1331 batch loss 1.07122827 epoch total loss 1.22332478\n",
      "Trained batch 1332 batch loss 1.16268337 epoch total loss 1.22327924\n",
      "Trained batch 1333 batch loss 1.08164287 epoch total loss 1.22317302\n",
      "Trained batch 1334 batch loss 1.08184671 epoch total loss 1.22306705\n",
      "Trained batch 1335 batch loss 1.29608274 epoch total loss 1.22312176\n",
      "Trained batch 1336 batch loss 1.31881428 epoch total loss 1.22319341\n",
      "Trained batch 1337 batch loss 1.27662992 epoch total loss 1.22323334\n",
      "Trained batch 1338 batch loss 1.21887398 epoch total loss 1.22323012\n",
      "Trained batch 1339 batch loss 1.18707931 epoch total loss 1.22320318\n",
      "Trained batch 1340 batch loss 1.2403996 epoch total loss 1.22321594\n",
      "Trained batch 1341 batch loss 1.18910098 epoch total loss 1.22319055\n",
      "Trained batch 1342 batch loss 1.21328557 epoch total loss 1.22318316\n",
      "Trained batch 1343 batch loss 1.21089256 epoch total loss 1.22317398\n",
      "Trained batch 1344 batch loss 1.29429281 epoch total loss 1.2232269\n",
      "Trained batch 1345 batch loss 1.32736301 epoch total loss 1.22330439\n",
      "Trained batch 1346 batch loss 1.17582905 epoch total loss 1.2232691\n",
      "Trained batch 1347 batch loss 1.308429 epoch total loss 1.22333229\n",
      "Trained batch 1348 batch loss 1.22335303 epoch total loss 1.22333241\n",
      "Trained batch 1349 batch loss 1.14882183 epoch total loss 1.22327709\n",
      "Trained batch 1350 batch loss 1.10939467 epoch total loss 1.22319269\n",
      "Trained batch 1351 batch loss 1.23688388 epoch total loss 1.22320294\n",
      "Trained batch 1352 batch loss 1.30067492 epoch total loss 1.22326016\n",
      "Trained batch 1353 batch loss 1.28593087 epoch total loss 1.22330654\n",
      "Trained batch 1354 batch loss 1.04266047 epoch total loss 1.22317302\n",
      "Trained batch 1355 batch loss 1.0778532 epoch total loss 1.22306585\n",
      "Trained batch 1356 batch loss 1.15303171 epoch total loss 1.22301424\n",
      "Trained batch 1357 batch loss 1.17049015 epoch total loss 1.22297549\n",
      "Trained batch 1358 batch loss 1.17896438 epoch total loss 1.22294307\n",
      "Trained batch 1359 batch loss 1.12503088 epoch total loss 1.22287107\n",
      "Trained batch 1360 batch loss 1.14935184 epoch total loss 1.22281694\n",
      "Trained batch 1361 batch loss 1.16788137 epoch total loss 1.22277653\n",
      "Trained batch 1362 batch loss 1.30669034 epoch total loss 1.22283804\n",
      "Trained batch 1363 batch loss 1.28429699 epoch total loss 1.22288322\n",
      "Trained batch 1364 batch loss 1.33508086 epoch total loss 1.22296548\n",
      "Trained batch 1365 batch loss 1.45787501 epoch total loss 1.2231375\n",
      "Trained batch 1366 batch loss 1.42700958 epoch total loss 1.22328675\n",
      "Trained batch 1367 batch loss 1.21149278 epoch total loss 1.22327816\n",
      "Trained batch 1368 batch loss 1.18820882 epoch total loss 1.22325253\n",
      "Trained batch 1369 batch loss 1.20294476 epoch total loss 1.22323775\n",
      "Trained batch 1370 batch loss 1.09654546 epoch total loss 1.22314537\n",
      "Trained batch 1371 batch loss 1.3098073 epoch total loss 1.22320855\n",
      "Trained batch 1372 batch loss 1.25193977 epoch total loss 1.22322953\n",
      "Trained batch 1373 batch loss 1.44760966 epoch total loss 1.22339296\n",
      "Trained batch 1374 batch loss 1.33499098 epoch total loss 1.22347414\n",
      "Trained batch 1375 batch loss 1.32026374 epoch total loss 1.2235446\n",
      "Trained batch 1376 batch loss 1.31837797 epoch total loss 1.2236135\n",
      "Trained batch 1377 batch loss 1.39566469 epoch total loss 1.22373843\n",
      "Trained batch 1378 batch loss 1.19875669 epoch total loss 1.22372019\n",
      "Trained batch 1379 batch loss 1.20378363 epoch total loss 1.22370577\n",
      "Trained batch 1380 batch loss 1.28992021 epoch total loss 1.22375369\n",
      "Trained batch 1381 batch loss 1.19442677 epoch total loss 1.22373247\n",
      "Trained batch 1382 batch loss 1.16138339 epoch total loss 1.22368741\n",
      "Trained batch 1383 batch loss 1.29301429 epoch total loss 1.22373748\n",
      "Trained batch 1384 batch loss 1.21958041 epoch total loss 1.2237345\n",
      "Trained batch 1385 batch loss 1.17055702 epoch total loss 1.22369611\n",
      "Trained batch 1386 batch loss 1.18969131 epoch total loss 1.22367156\n",
      "Trained batch 1387 batch loss 1.16401446 epoch total loss 1.22362864\n",
      "Trained batch 1388 batch loss 1.17932832 epoch total loss 1.22359669\n",
      "Epoch 4 train loss 1.2235966920852661\n",
      "Validated batch 1 batch loss 1.12333512\n",
      "Validated batch 2 batch loss 1.19236708\n",
      "Validated batch 3 batch loss 1.26971424\n",
      "Validated batch 4 batch loss 1.14587784\n",
      "Validated batch 5 batch loss 1.27648902\n",
      "Validated batch 6 batch loss 1.32450056\n",
      "Validated batch 7 batch loss 1.06503487\n",
      "Validated batch 8 batch loss 1.1693058\n",
      "Validated batch 9 batch loss 1.17702246\n",
      "Validated batch 10 batch loss 1.29449105\n",
      "Validated batch 11 batch loss 1.20468163\n",
      "Validated batch 12 batch loss 1.04166651\n",
      "Validated batch 13 batch loss 1.09874868\n",
      "Validated batch 14 batch loss 1.15455043\n",
      "Validated batch 15 batch loss 1.11816108\n",
      "Validated batch 16 batch loss 1.2128948\n",
      "Validated batch 17 batch loss 1.17535055\n",
      "Validated batch 18 batch loss 1.11922169\n",
      "Validated batch 19 batch loss 1.22083735\n",
      "Validated batch 20 batch loss 1.25268054\n",
      "Validated batch 21 batch loss 1.26971984\n",
      "Validated batch 22 batch loss 1.21177638\n",
      "Validated batch 23 batch loss 1.10590768\n",
      "Validated batch 24 batch loss 1.18738532\n",
      "Validated batch 25 batch loss 1.14981329\n",
      "Validated batch 26 batch loss 1.1941942\n",
      "Validated batch 27 batch loss 1.13122928\n",
      "Validated batch 28 batch loss 1.13510728\n",
      "Validated batch 29 batch loss 1.27139235\n",
      "Validated batch 30 batch loss 1.19904137\n",
      "Validated batch 31 batch loss 1.08421564\n",
      "Validated batch 32 batch loss 1.16325474\n",
      "Validated batch 33 batch loss 1.1626668\n",
      "Validated batch 34 batch loss 1.12348557\n",
      "Validated batch 35 batch loss 1.18266773\n",
      "Validated batch 36 batch loss 1.17215753\n",
      "Validated batch 37 batch loss 1.18133402\n",
      "Validated batch 38 batch loss 1.31962109\n",
      "Validated batch 39 batch loss 1.25467515\n",
      "Validated batch 40 batch loss 1.19431329\n",
      "Validated batch 41 batch loss 1.28850079\n",
      "Validated batch 42 batch loss 1.00897312\n",
      "Validated batch 43 batch loss 1.14565265\n",
      "Validated batch 44 batch loss 1.11410987\n",
      "Validated batch 45 batch loss 1.22205353\n",
      "Validated batch 46 batch loss 1.3886919\n",
      "Validated batch 47 batch loss 1.32426846\n",
      "Validated batch 48 batch loss 1.23066711\n",
      "Validated batch 49 batch loss 1.17434621\n",
      "Validated batch 50 batch loss 1.16927695\n",
      "Validated batch 51 batch loss 1.13344741\n",
      "Validated batch 52 batch loss 1.22490835\n",
      "Validated batch 53 batch loss 1.26719069\n",
      "Validated batch 54 batch loss 1.06379664\n",
      "Validated batch 55 batch loss 1.19731057\n",
      "Validated batch 56 batch loss 1.18208992\n",
      "Validated batch 57 batch loss 1.24060881\n",
      "Validated batch 58 batch loss 1.24681759\n",
      "Validated batch 59 batch loss 1.24887192\n",
      "Validated batch 60 batch loss 1.15123963\n",
      "Validated batch 61 batch loss 1.1575551\n",
      "Validated batch 62 batch loss 1.19911373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 63 batch loss 1.19805551\n",
      "Validated batch 64 batch loss 1.26349354\n",
      "Validated batch 65 batch loss 1.25912082\n",
      "Validated batch 66 batch loss 1.4330734\n",
      "Validated batch 67 batch loss 1.24075198\n",
      "Validated batch 68 batch loss 1.24905062\n",
      "Validated batch 69 batch loss 1.10416126\n",
      "Validated batch 70 batch loss 1.14294469\n",
      "Validated batch 71 batch loss 1.2425555\n",
      "Validated batch 72 batch loss 1.18014693\n",
      "Validated batch 73 batch loss 1.16752303\n",
      "Validated batch 74 batch loss 1.1843822\n",
      "Validated batch 75 batch loss 1.28005648\n",
      "Validated batch 76 batch loss 1.29196632\n",
      "Validated batch 77 batch loss 1.3110075\n",
      "Validated batch 78 batch loss 1.25784564\n",
      "Validated batch 79 batch loss 1.15188563\n",
      "Validated batch 80 batch loss 1.27059913\n",
      "Validated batch 81 batch loss 1.20538568\n",
      "Validated batch 82 batch loss 1.22559488\n",
      "Validated batch 83 batch loss 1.33081102\n",
      "Validated batch 84 batch loss 1.28453839\n",
      "Validated batch 85 batch loss 1.22341537\n",
      "Validated batch 86 batch loss 1.417606\n",
      "Validated batch 87 batch loss 1.11185563\n",
      "Validated batch 88 batch loss 1.28528965\n",
      "Validated batch 89 batch loss 1.09387434\n",
      "Validated batch 90 batch loss 1.1606952\n",
      "Validated batch 91 batch loss 1.30877268\n",
      "Validated batch 92 batch loss 1.14641297\n",
      "Validated batch 93 batch loss 1.20723736\n",
      "Validated batch 94 batch loss 1.1049695\n",
      "Validated batch 95 batch loss 1.17161798\n",
      "Validated batch 96 batch loss 1.14631855\n",
      "Validated batch 97 batch loss 1.16113961\n",
      "Validated batch 98 batch loss 1.27804685\n",
      "Validated batch 99 batch loss 1.21685779\n",
      "Validated batch 100 batch loss 1.28712511\n",
      "Validated batch 101 batch loss 1.24389648\n",
      "Validated batch 102 batch loss 1.20849967\n",
      "Validated batch 103 batch loss 1.22696269\n",
      "Validated batch 104 batch loss 1.3749795\n",
      "Validated batch 105 batch loss 1.23994327\n",
      "Validated batch 106 batch loss 1.31948674\n",
      "Validated batch 107 batch loss 1.27212358\n",
      "Validated batch 108 batch loss 1.3383019\n",
      "Validated batch 109 batch loss 1.28844023\n",
      "Validated batch 110 batch loss 1.12452674\n",
      "Validated batch 111 batch loss 1.20458019\n",
      "Validated batch 112 batch loss 1.26899147\n",
      "Validated batch 113 batch loss 1.26273966\n",
      "Validated batch 114 batch loss 1.17843485\n",
      "Validated batch 115 batch loss 1.22033918\n",
      "Validated batch 116 batch loss 1.19625986\n",
      "Validated batch 117 batch loss 1.08562803\n",
      "Validated batch 118 batch loss 1.22023273\n",
      "Validated batch 119 batch loss 1.15949833\n",
      "Validated batch 120 batch loss 1.15949166\n",
      "Validated batch 121 batch loss 1.24566817\n",
      "Validated batch 122 batch loss 1.17225921\n",
      "Validated batch 123 batch loss 1.24885082\n",
      "Validated batch 124 batch loss 1.25771523\n",
      "Validated batch 125 batch loss 1.11015344\n",
      "Validated batch 126 batch loss 1.28146243\n",
      "Validated batch 127 batch loss 1.25819969\n",
      "Validated batch 128 batch loss 1.13087595\n",
      "Validated batch 129 batch loss 1.22317481\n",
      "Validated batch 130 batch loss 1.21163261\n",
      "Validated batch 131 batch loss 1.15345895\n",
      "Validated batch 132 batch loss 1.33810472\n",
      "Validated batch 133 batch loss 1.26613343\n",
      "Validated batch 134 batch loss 1.22364783\n",
      "Validated batch 135 batch loss 1.21594906\n",
      "Validated batch 136 batch loss 1.17892838\n",
      "Validated batch 137 batch loss 1.20336914\n",
      "Validated batch 138 batch loss 1.21101737\n",
      "Validated batch 139 batch loss 1.24303281\n",
      "Validated batch 140 batch loss 1.23401141\n",
      "Validated batch 141 batch loss 1.14536381\n",
      "Validated batch 142 batch loss 1.20445728\n",
      "Validated batch 143 batch loss 1.21112311\n",
      "Validated batch 144 batch loss 1.21091604\n",
      "Validated batch 145 batch loss 1.23586988\n",
      "Validated batch 146 batch loss 1.20874488\n",
      "Validated batch 147 batch loss 1.15324438\n",
      "Validated batch 148 batch loss 1.34001172\n",
      "Validated batch 149 batch loss 1.14755666\n",
      "Validated batch 150 batch loss 1.08323479\n",
      "Validated batch 151 batch loss 1.18283415\n",
      "Validated batch 152 batch loss 1.32398796\n",
      "Validated batch 153 batch loss 1.29748774\n",
      "Validated batch 154 batch loss 1.33906102\n",
      "Validated batch 155 batch loss 1.18659568\n",
      "Validated batch 156 batch loss 1.37867785\n",
      "Validated batch 157 batch loss 1.2280643\n",
      "Validated batch 158 batch loss 1.30731201\n",
      "Validated batch 159 batch loss 1.28186536\n",
      "Validated batch 160 batch loss 0.998891473\n",
      "Validated batch 161 batch loss 1.16484165\n",
      "Validated batch 162 batch loss 1.25079083\n",
      "Validated batch 163 batch loss 1.20948946\n",
      "Validated batch 164 batch loss 1.21376383\n",
      "Validated batch 165 batch loss 1.18052912\n",
      "Validated batch 166 batch loss 1.27378392\n",
      "Validated batch 167 batch loss 1.40128493\n",
      "Validated batch 168 batch loss 1.15394771\n",
      "Validated batch 169 batch loss 1.21243834\n",
      "Validated batch 170 batch loss 1.17596221\n",
      "Validated batch 171 batch loss 1.27215624\n",
      "Validated batch 172 batch loss 1.20201635\n",
      "Validated batch 173 batch loss 1.20299315\n",
      "Validated batch 174 batch loss 1.24195325\n",
      "Validated batch 175 batch loss 1.25633335\n",
      "Validated batch 176 batch loss 1.23933983\n",
      "Validated batch 177 batch loss 1.23934698\n",
      "Validated batch 178 batch loss 1.27641988\n",
      "Validated batch 179 batch loss 1.18875861\n",
      "Validated batch 180 batch loss 1.1473949\n",
      "Validated batch 181 batch loss 1.22871435\n",
      "Validated batch 182 batch loss 1.17502427\n",
      "Validated batch 183 batch loss 1.17494273\n",
      "Validated batch 184 batch loss 1.22463119\n",
      "Validated batch 185 batch loss 1.32535303\n",
      "Epoch 4 val loss 1.2129998207092285\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-4-loss-1.2130.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.19474 epoch total loss 1.19474\n",
      "Trained batch 2 batch loss 1.29474366 epoch total loss 1.24474192\n",
      "Trained batch 3 batch loss 1.35422111 epoch total loss 1.28123498\n",
      "Trained batch 4 batch loss 1.2446233 epoch total loss 1.27208209\n",
      "Trained batch 5 batch loss 1.25464118 epoch total loss 1.26859391\n",
      "Trained batch 6 batch loss 1.27931821 epoch total loss 1.27038133\n",
      "Trained batch 7 batch loss 1.27551699 epoch total loss 1.27111495\n",
      "Trained batch 8 batch loss 1.11211169 epoch total loss 1.25123954\n",
      "Trained batch 9 batch loss 1.24907541 epoch total loss 1.25099897\n",
      "Trained batch 10 batch loss 1.40725279 epoch total loss 1.26662445\n",
      "Trained batch 11 batch loss 1.26614654 epoch total loss 1.26658106\n",
      "Trained batch 12 batch loss 1.26396513 epoch total loss 1.26636302\n",
      "Trained batch 13 batch loss 1.09655845 epoch total loss 1.25330102\n",
      "Trained batch 14 batch loss 1.31899238 epoch total loss 1.25799334\n",
      "Trained batch 15 batch loss 1.19888854 epoch total loss 1.254053\n",
      "Trained batch 16 batch loss 1.29232252 epoch total loss 1.25644481\n",
      "Trained batch 17 batch loss 1.38086009 epoch total loss 1.26376331\n",
      "Trained batch 18 batch loss 1.45242453 epoch total loss 1.27424455\n",
      "Trained batch 19 batch loss 1.23581028 epoch total loss 1.27222157\n",
      "Trained batch 20 batch loss 1.16117918 epoch total loss 1.26666951\n",
      "Trained batch 21 batch loss 1.09501505 epoch total loss 1.25849545\n",
      "Trained batch 22 batch loss 1.13604343 epoch total loss 1.25292945\n",
      "Trained batch 23 batch loss 1.06906056 epoch total loss 1.24493515\n",
      "Trained batch 24 batch loss 1.23915386 epoch total loss 1.24469423\n",
      "Trained batch 25 batch loss 1.2502718 epoch total loss 1.24491727\n",
      "Trained batch 26 batch loss 1.28581858 epoch total loss 1.24649048\n",
      "Trained batch 27 batch loss 1.23876643 epoch total loss 1.24620438\n",
      "Trained batch 28 batch loss 1.1497227 epoch total loss 1.24275863\n",
      "Trained batch 29 batch loss 1.11828375 epoch total loss 1.23846638\n",
      "Trained batch 30 batch loss 1.03261828 epoch total loss 1.23160481\n",
      "Trained batch 31 batch loss 1.20293796 epoch total loss 1.23068\n",
      "Trained batch 32 batch loss 1.19210148 epoch total loss 1.22947443\n",
      "Trained batch 33 batch loss 1.21479189 epoch total loss 1.22902942\n",
      "Trained batch 34 batch loss 1.11767483 epoch total loss 1.22575438\n",
      "Trained batch 35 batch loss 1.10511029 epoch total loss 1.22230732\n",
      "Trained batch 36 batch loss 1.1269002 epoch total loss 1.21965718\n",
      "Trained batch 37 batch loss 1.41204274 epoch total loss 1.22485685\n",
      "Trained batch 38 batch loss 1.17918456 epoch total loss 1.22365487\n",
      "Trained batch 39 batch loss 1.15770638 epoch total loss 1.22196388\n",
      "Trained batch 40 batch loss 1.06461549 epoch total loss 1.21803021\n",
      "Trained batch 41 batch loss 0.91823107 epoch total loss 1.21071815\n",
      "Trained batch 42 batch loss 0.866019368 epoch total loss 1.20251107\n",
      "Trained batch 43 batch loss 0.974817038 epoch total loss 1.1972158\n",
      "Trained batch 44 batch loss 1.16489744 epoch total loss 1.19648123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 45 batch loss 1.15064788 epoch total loss 1.1954627\n",
      "Trained batch 46 batch loss 1.09127617 epoch total loss 1.19319785\n",
      "Trained batch 47 batch loss 1.17523456 epoch total loss 1.19281566\n",
      "Trained batch 48 batch loss 1.10533333 epoch total loss 1.19099319\n",
      "Trained batch 49 batch loss 1.21646881 epoch total loss 1.19151306\n",
      "Trained batch 50 batch loss 1.14885867 epoch total loss 1.19066\n",
      "Trained batch 51 batch loss 1.00972855 epoch total loss 1.18711221\n",
      "Trained batch 52 batch loss 1.22118843 epoch total loss 1.18776751\n",
      "Trained batch 53 batch loss 1.18082798 epoch total loss 1.18763661\n",
      "Trained batch 54 batch loss 1.06255746 epoch total loss 1.18532038\n",
      "Trained batch 55 batch loss 1.12343478 epoch total loss 1.18419528\n",
      "Trained batch 56 batch loss 1.0734061 epoch total loss 1.18221676\n",
      "Trained batch 57 batch loss 1.18747616 epoch total loss 1.18230903\n",
      "Trained batch 58 batch loss 1.16791677 epoch total loss 1.18206096\n",
      "Trained batch 59 batch loss 1.15734053 epoch total loss 1.18164194\n",
      "Trained batch 60 batch loss 1.11495221 epoch total loss 1.18053043\n",
      "Trained batch 61 batch loss 1.22767222 epoch total loss 1.18130314\n",
      "Trained batch 62 batch loss 1.2508831 epoch total loss 1.1824255\n",
      "Trained batch 63 batch loss 1.22963297 epoch total loss 1.18317473\n",
      "Trained batch 64 batch loss 1.26999688 epoch total loss 1.18453133\n",
      "Trained batch 65 batch loss 1.2784059 epoch total loss 1.18597555\n",
      "Trained batch 66 batch loss 1.29492784 epoch total loss 1.18762636\n",
      "Trained batch 67 batch loss 1.20089865 epoch total loss 1.18782437\n",
      "Trained batch 68 batch loss 1.09081841 epoch total loss 1.18639791\n",
      "Trained batch 69 batch loss 1.07306468 epoch total loss 1.18475544\n",
      "Trained batch 70 batch loss 1.09622586 epoch total loss 1.18349075\n",
      "Trained batch 71 batch loss 1.07486081 epoch total loss 1.1819607\n",
      "Trained batch 72 batch loss 1.10929275 epoch total loss 1.18095148\n",
      "Trained batch 73 batch loss 1.14524353 epoch total loss 1.18046224\n",
      "Trained batch 74 batch loss 1.15635 epoch total loss 1.18013644\n",
      "Trained batch 75 batch loss 1.26012611 epoch total loss 1.18120289\n",
      "Trained batch 76 batch loss 1.15645337 epoch total loss 1.18087733\n",
      "Trained batch 77 batch loss 1.20511293 epoch total loss 1.18119204\n",
      "Trained batch 78 batch loss 1.2851553 epoch total loss 1.18252492\n",
      "Trained batch 79 batch loss 1.34016466 epoch total loss 1.18452036\n",
      "Trained batch 80 batch loss 1.42244816 epoch total loss 1.18749452\n",
      "Trained batch 81 batch loss 1.33473885 epoch total loss 1.18931234\n",
      "Trained batch 82 batch loss 1.21374202 epoch total loss 1.18961024\n",
      "Trained batch 83 batch loss 1.05579269 epoch total loss 1.18799806\n",
      "Trained batch 84 batch loss 1.34114337 epoch total loss 1.18982112\n",
      "Trained batch 85 batch loss 1.40569532 epoch total loss 1.19236088\n",
      "Trained batch 86 batch loss 1.24947762 epoch total loss 1.19302499\n",
      "Trained batch 87 batch loss 1.34684992 epoch total loss 1.19479311\n",
      "Trained batch 88 batch loss 1.13383627 epoch total loss 1.19410038\n",
      "Trained batch 89 batch loss 1.27109575 epoch total loss 1.19496548\n",
      "Trained batch 90 batch loss 1.15957975 epoch total loss 1.19457233\n",
      "Trained batch 91 batch loss 1.34678483 epoch total loss 1.19624496\n",
      "Trained batch 92 batch loss 1.33423615 epoch total loss 1.19774485\n",
      "Trained batch 93 batch loss 1.26906466 epoch total loss 1.19851172\n",
      "Trained batch 94 batch loss 1.26529717 epoch total loss 1.19922221\n",
      "Trained batch 95 batch loss 1.29102874 epoch total loss 1.20018864\n",
      "Trained batch 96 batch loss 1.19883692 epoch total loss 1.20017457\n",
      "Trained batch 97 batch loss 1.24216461 epoch total loss 1.20060742\n",
      "Trained batch 98 batch loss 1.16396165 epoch total loss 1.20023358\n",
      "Trained batch 99 batch loss 1.29058242 epoch total loss 1.20114613\n",
      "Trained batch 100 batch loss 1.32395911 epoch total loss 1.20237422\n",
      "Trained batch 101 batch loss 1.32449186 epoch total loss 1.20358336\n",
      "Trained batch 102 batch loss 1.31523633 epoch total loss 1.20467806\n",
      "Trained batch 103 batch loss 1.40046537 epoch total loss 1.20657885\n",
      "Trained batch 104 batch loss 1.37057579 epoch total loss 1.20815575\n",
      "Trained batch 105 batch loss 1.36074674 epoch total loss 1.20960903\n",
      "Trained batch 106 batch loss 1.30101275 epoch total loss 1.21047139\n",
      "Trained batch 107 batch loss 1.24370456 epoch total loss 1.21078193\n",
      "Trained batch 108 batch loss 1.28649402 epoch total loss 1.211483\n",
      "Trained batch 109 batch loss 1.2042774 epoch total loss 1.21141696\n",
      "Trained batch 110 batch loss 1.21815097 epoch total loss 1.21147823\n",
      "Trained batch 111 batch loss 1.24308717 epoch total loss 1.21176302\n",
      "Trained batch 112 batch loss 1.43382084 epoch total loss 1.21374571\n",
      "Trained batch 113 batch loss 1.39163363 epoch total loss 1.21531987\n",
      "Trained batch 114 batch loss 1.35152471 epoch total loss 1.21651471\n",
      "Trained batch 115 batch loss 1.29842663 epoch total loss 1.21722698\n",
      "Trained batch 116 batch loss 1.23001289 epoch total loss 1.21733725\n",
      "Trained batch 117 batch loss 1.21196389 epoch total loss 1.21729124\n",
      "Trained batch 118 batch loss 1.04141414 epoch total loss 1.21580076\n",
      "Trained batch 119 batch loss 1.21565354 epoch total loss 1.21579957\n",
      "Trained batch 120 batch loss 1.24708772 epoch total loss 1.21606028\n",
      "Trained batch 121 batch loss 1.19789457 epoch total loss 1.21591008\n",
      "Trained batch 122 batch loss 1.17675698 epoch total loss 1.21558917\n",
      "Trained batch 123 batch loss 1.1220119 epoch total loss 1.21482837\n",
      "Trained batch 124 batch loss 1.08767843 epoch total loss 1.21380293\n",
      "Trained batch 125 batch loss 1.07697511 epoch total loss 1.21270835\n",
      "Trained batch 126 batch loss 1.07813525 epoch total loss 1.21164036\n",
      "Trained batch 127 batch loss 1.07692361 epoch total loss 1.21057963\n",
      "Trained batch 128 batch loss 1.03098106 epoch total loss 1.20917642\n",
      "Trained batch 129 batch loss 1.16870892 epoch total loss 1.20886278\n",
      "Trained batch 130 batch loss 1.11849165 epoch total loss 1.20816755\n",
      "Trained batch 131 batch loss 1.22889626 epoch total loss 1.20832574\n",
      "Trained batch 132 batch loss 1.21970415 epoch total loss 1.20841205\n",
      "Trained batch 133 batch loss 1.10143459 epoch total loss 1.20760775\n",
      "Trained batch 134 batch loss 1.21876645 epoch total loss 1.20769107\n",
      "Trained batch 135 batch loss 1.34222579 epoch total loss 1.20868754\n",
      "Trained batch 136 batch loss 1.45877564 epoch total loss 1.21052647\n",
      "Trained batch 137 batch loss 1.27202201 epoch total loss 1.21097529\n",
      "Trained batch 138 batch loss 1.18163037 epoch total loss 1.21076262\n",
      "Trained batch 139 batch loss 1.13812923 epoch total loss 1.21024\n",
      "Trained batch 140 batch loss 1.2696811 epoch total loss 1.21066463\n",
      "Trained batch 141 batch loss 1.29886949 epoch total loss 1.21129024\n",
      "Trained batch 142 batch loss 1.25566208 epoch total loss 1.21160269\n",
      "Trained batch 143 batch loss 1.41329694 epoch total loss 1.21301317\n",
      "Trained batch 144 batch loss 1.39629793 epoch total loss 1.21428597\n",
      "Trained batch 145 batch loss 1.24470329 epoch total loss 1.21449578\n",
      "Trained batch 146 batch loss 1.22599947 epoch total loss 1.21457458\n",
      "Trained batch 147 batch loss 1.1623137 epoch total loss 1.21421897\n",
      "Trained batch 148 batch loss 1.31484365 epoch total loss 1.21489894\n",
      "Trained batch 149 batch loss 1.33945298 epoch total loss 1.21573484\n",
      "Trained batch 150 batch loss 1.3393054 epoch total loss 1.21655858\n",
      "Trained batch 151 batch loss 1.13295209 epoch total loss 1.21600497\n",
      "Trained batch 152 batch loss 1.08404744 epoch total loss 1.21513677\n",
      "Trained batch 153 batch loss 1.19529533 epoch total loss 1.21500707\n",
      "Trained batch 154 batch loss 1.16730428 epoch total loss 1.21469736\n",
      "Trained batch 155 batch loss 1.24929702 epoch total loss 1.21492052\n",
      "Trained batch 156 batch loss 1.19673848 epoch total loss 1.21480393\n",
      "Trained batch 157 batch loss 1.43630362 epoch total loss 1.21621478\n",
      "Trained batch 158 batch loss 1.2022078 epoch total loss 1.2161262\n",
      "Trained batch 159 batch loss 1.33550334 epoch total loss 1.21687698\n",
      "Trained batch 160 batch loss 1.22962666 epoch total loss 1.21695673\n",
      "Trained batch 161 batch loss 1.26164675 epoch total loss 1.21723425\n",
      "Trained batch 162 batch loss 1.22368717 epoch total loss 1.21727419\n",
      "Trained batch 163 batch loss 1.21885717 epoch total loss 1.21728384\n",
      "Trained batch 164 batch loss 1.15637779 epoch total loss 1.21691239\n",
      "Trained batch 165 batch loss 1.14278615 epoch total loss 1.21646321\n",
      "Trained batch 166 batch loss 1.22054708 epoch total loss 1.21648788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 167 batch loss 1.22686219 epoch total loss 1.21655\n",
      "Trained batch 168 batch loss 1.21035743 epoch total loss 1.21651316\n",
      "Trained batch 169 batch loss 1.22853184 epoch total loss 1.21658421\n",
      "Trained batch 170 batch loss 1.14254642 epoch total loss 1.21614873\n",
      "Trained batch 171 batch loss 1.17784846 epoch total loss 1.21592474\n",
      "Trained batch 172 batch loss 1.26349 epoch total loss 1.21620131\n",
      "Trained batch 173 batch loss 1.05506277 epoch total loss 1.2152698\n",
      "Trained batch 174 batch loss 1.21890271 epoch total loss 1.21529078\n",
      "Trained batch 175 batch loss 1.13835359 epoch total loss 1.21485102\n",
      "Trained batch 176 batch loss 1.12710428 epoch total loss 1.21435249\n",
      "Trained batch 177 batch loss 1.20298398 epoch total loss 1.21428835\n",
      "Trained batch 178 batch loss 1.15985632 epoch total loss 1.21398246\n",
      "Trained batch 179 batch loss 1.17323101 epoch total loss 1.21375489\n",
      "Trained batch 180 batch loss 1.40640736 epoch total loss 1.21482515\n",
      "Trained batch 181 batch loss 1.22325778 epoch total loss 1.21487164\n",
      "Trained batch 182 batch loss 1.33408678 epoch total loss 1.2155267\n",
      "Trained batch 183 batch loss 1.27250254 epoch total loss 1.21583807\n",
      "Trained batch 184 batch loss 1.3136003 epoch total loss 1.21636939\n",
      "Trained batch 185 batch loss 1.2149806 epoch total loss 1.21636188\n",
      "Trained batch 186 batch loss 1.26780593 epoch total loss 1.21663845\n",
      "Trained batch 187 batch loss 1.19199753 epoch total loss 1.21650672\n",
      "Trained batch 188 batch loss 1.14447069 epoch total loss 1.21612358\n",
      "Trained batch 189 batch loss 1.18732381 epoch total loss 1.21597111\n",
      "Trained batch 190 batch loss 1.19256711 epoch total loss 1.21584797\n",
      "Trained batch 191 batch loss 1.1613158 epoch total loss 1.21556246\n",
      "Trained batch 192 batch loss 1.1730938 epoch total loss 1.21534121\n",
      "Trained batch 193 batch loss 1.06590116 epoch total loss 1.21456695\n",
      "Trained batch 194 batch loss 1.15835798 epoch total loss 1.21427727\n",
      "Trained batch 195 batch loss 1.1579299 epoch total loss 1.21398818\n",
      "Trained batch 196 batch loss 1.17770422 epoch total loss 1.21380317\n",
      "Trained batch 197 batch loss 1.15684736 epoch total loss 1.21351397\n",
      "Trained batch 198 batch loss 1.22096539 epoch total loss 1.21355164\n",
      "Trained batch 199 batch loss 1.22662401 epoch total loss 1.21361732\n",
      "Trained batch 200 batch loss 1.17889082 epoch total loss 1.21344364\n",
      "Trained batch 201 batch loss 1.19347262 epoch total loss 1.21334434\n",
      "Trained batch 202 batch loss 1.1724422 epoch total loss 1.2131418\n",
      "Trained batch 203 batch loss 1.16178942 epoch total loss 1.21288884\n",
      "Trained batch 204 batch loss 1.26831377 epoch total loss 1.21316051\n",
      "Trained batch 205 batch loss 1.2464329 epoch total loss 1.21332276\n",
      "Trained batch 206 batch loss 1.3089745 epoch total loss 1.21378708\n",
      "Trained batch 207 batch loss 1.30133402 epoch total loss 1.21421\n",
      "Trained batch 208 batch loss 1.28383756 epoch total loss 1.21454477\n",
      "Trained batch 209 batch loss 1.20770848 epoch total loss 1.21451211\n",
      "Trained batch 210 batch loss 1.30050814 epoch total loss 1.21492159\n",
      "Trained batch 211 batch loss 1.23372614 epoch total loss 1.21501064\n",
      "Trained batch 212 batch loss 1.28713453 epoch total loss 1.21535087\n",
      "Trained batch 213 batch loss 1.12441111 epoch total loss 1.21492398\n",
      "Trained batch 214 batch loss 1.1247766 epoch total loss 1.21450281\n",
      "Trained batch 215 batch loss 1.13424563 epoch total loss 1.21412945\n",
      "Trained batch 216 batch loss 1.05847871 epoch total loss 1.21340883\n",
      "Trained batch 217 batch loss 1.16771877 epoch total loss 1.2131983\n",
      "Trained batch 218 batch loss 1.2420522 epoch total loss 1.21333075\n",
      "Trained batch 219 batch loss 1.17626429 epoch total loss 1.21316147\n",
      "Trained batch 220 batch loss 1.14598083 epoch total loss 1.21285617\n",
      "Trained batch 221 batch loss 1.18859696 epoch total loss 1.2127465\n",
      "Trained batch 222 batch loss 1.24957502 epoch total loss 1.21291232\n",
      "Trained batch 223 batch loss 1.2022903 epoch total loss 1.21286476\n",
      "Trained batch 224 batch loss 1.1558218 epoch total loss 1.21261013\n",
      "Trained batch 225 batch loss 1.18086076 epoch total loss 1.21246898\n",
      "Trained batch 226 batch loss 1.02807319 epoch total loss 1.21165299\n",
      "Trained batch 227 batch loss 1.21478319 epoch total loss 1.21166682\n",
      "Trained batch 228 batch loss 1.24163282 epoch total loss 1.21179831\n",
      "Trained batch 229 batch loss 1.28482318 epoch total loss 1.2121172\n",
      "Trained batch 230 batch loss 1.27931535 epoch total loss 1.21240938\n",
      "Trained batch 231 batch loss 1.16569424 epoch total loss 1.21220708\n",
      "Trained batch 232 batch loss 1.19124246 epoch total loss 1.21211672\n",
      "Trained batch 233 batch loss 1.16957569 epoch total loss 1.21193421\n",
      "Trained batch 234 batch loss 1.11163676 epoch total loss 1.21150565\n",
      "Trained batch 235 batch loss 1.09153628 epoch total loss 1.21099508\n",
      "Trained batch 236 batch loss 1.30416667 epoch total loss 1.21138978\n",
      "Trained batch 237 batch loss 1.17550349 epoch total loss 1.21123838\n",
      "Trained batch 238 batch loss 1.18846512 epoch total loss 1.21114278\n",
      "Trained batch 239 batch loss 1.17852616 epoch total loss 1.21100628\n",
      "Trained batch 240 batch loss 1.21129775 epoch total loss 1.2110076\n",
      "Trained batch 241 batch loss 1.24502158 epoch total loss 1.21114874\n",
      "Trained batch 242 batch loss 1.21904683 epoch total loss 1.2111814\n",
      "Trained batch 243 batch loss 1.14287257 epoch total loss 1.21090031\n",
      "Trained batch 244 batch loss 1.13719702 epoch total loss 1.21059835\n",
      "Trained batch 245 batch loss 1.10817695 epoch total loss 1.21018028\n",
      "Trained batch 246 batch loss 1.03374457 epoch total loss 1.20946312\n",
      "Trained batch 247 batch loss 1.24971175 epoch total loss 1.20962608\n",
      "Trained batch 248 batch loss 1.21009254 epoch total loss 1.20962799\n",
      "Trained batch 249 batch loss 1.14485598 epoch total loss 1.20936787\n",
      "Trained batch 250 batch loss 1.18593442 epoch total loss 1.20927417\n",
      "Trained batch 251 batch loss 1.21848559 epoch total loss 1.20931077\n",
      "Trained batch 252 batch loss 1.09306931 epoch total loss 1.20884955\n",
      "Trained batch 253 batch loss 1.18039131 epoch total loss 1.20873713\n",
      "Trained batch 254 batch loss 1.16543 epoch total loss 1.20856667\n",
      "Trained batch 255 batch loss 1.11031878 epoch total loss 1.20818138\n",
      "Trained batch 256 batch loss 1.11946392 epoch total loss 1.20783484\n",
      "Trained batch 257 batch loss 1.13893604 epoch total loss 1.20756674\n",
      "Trained batch 258 batch loss 1.09131074 epoch total loss 1.20711613\n",
      "Trained batch 259 batch loss 1.26353145 epoch total loss 1.20733392\n",
      "Trained batch 260 batch loss 1.21110821 epoch total loss 1.20734847\n",
      "Trained batch 261 batch loss 1.19848335 epoch total loss 1.20731461\n",
      "Trained batch 262 batch loss 1.11549628 epoch total loss 1.20696414\n",
      "Trained batch 263 batch loss 1.22247875 epoch total loss 1.20702314\n",
      "Trained batch 264 batch loss 1.21733773 epoch total loss 1.20706224\n",
      "Trained batch 265 batch loss 1.28714681 epoch total loss 1.20736444\n",
      "Trained batch 266 batch loss 1.19121563 epoch total loss 1.20730376\n",
      "Trained batch 267 batch loss 1.12146521 epoch total loss 1.20698225\n",
      "Trained batch 268 batch loss 1.11652398 epoch total loss 1.20664465\n",
      "Trained batch 269 batch loss 1.11798906 epoch total loss 1.20631504\n",
      "Trained batch 270 batch loss 1.04031694 epoch total loss 1.20570028\n",
      "Trained batch 271 batch loss 1.06679618 epoch total loss 1.20518768\n",
      "Trained batch 272 batch loss 1.10625744 epoch total loss 1.20482397\n",
      "Trained batch 273 batch loss 1.13269687 epoch total loss 1.2045598\n",
      "Trained batch 274 batch loss 1.02894962 epoch total loss 1.20391893\n",
      "Trained batch 275 batch loss 1.12974906 epoch total loss 1.20364928\n",
      "Trained batch 276 batch loss 1.12496042 epoch total loss 1.20336413\n",
      "Trained batch 277 batch loss 1.10687947 epoch total loss 1.2030158\n",
      "Trained batch 278 batch loss 1.1544416 epoch total loss 1.20284116\n",
      "Trained batch 279 batch loss 1.23004031 epoch total loss 1.20293856\n",
      "Trained batch 280 batch loss 1.27947974 epoch total loss 1.20321202\n",
      "Trained batch 281 batch loss 1.15727389 epoch total loss 1.20304859\n",
      "Trained batch 282 batch loss 1.10943794 epoch total loss 1.20271659\n",
      "Trained batch 283 batch loss 1.26234388 epoch total loss 1.20292723\n",
      "Trained batch 284 batch loss 1.34886718 epoch total loss 1.20344114\n",
      "Trained batch 285 batch loss 1.24633741 epoch total loss 1.2035917\n",
      "Trained batch 286 batch loss 1.22050285 epoch total loss 1.20365071\n",
      "Trained batch 287 batch loss 1.25021815 epoch total loss 1.20381296\n",
      "Trained batch 288 batch loss 1.13861513 epoch total loss 1.20358658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 289 batch loss 1.28542519 epoch total loss 1.20386982\n",
      "Trained batch 290 batch loss 1.24192667 epoch total loss 1.20400095\n",
      "Trained batch 291 batch loss 1.35348749 epoch total loss 1.20451462\n",
      "Trained batch 292 batch loss 1.29409599 epoch total loss 1.20482147\n",
      "Trained batch 293 batch loss 1.12504351 epoch total loss 1.20454907\n",
      "Trained batch 294 batch loss 1.29401028 epoch total loss 1.20485342\n",
      "Trained batch 295 batch loss 1.24948573 epoch total loss 1.20500469\n",
      "Trained batch 296 batch loss 1.27651262 epoch total loss 1.20524633\n",
      "Trained batch 297 batch loss 1.19275188 epoch total loss 1.20520425\n",
      "Trained batch 298 batch loss 1.12230754 epoch total loss 1.20492601\n",
      "Trained batch 299 batch loss 1.2308042 epoch total loss 1.20501256\n",
      "Trained batch 300 batch loss 1.20557165 epoch total loss 1.20501447\n",
      "Trained batch 301 batch loss 1.19723415 epoch total loss 1.2049886\n",
      "Trained batch 302 batch loss 1.30641079 epoch total loss 1.20532441\n",
      "Trained batch 303 batch loss 1.19510078 epoch total loss 1.20529068\n",
      "Trained batch 304 batch loss 0.95602572 epoch total loss 1.20447063\n",
      "Trained batch 305 batch loss 0.995219886 epoch total loss 1.20378458\n",
      "Trained batch 306 batch loss 1.21658242 epoch total loss 1.20382643\n",
      "Trained batch 307 batch loss 1.23488903 epoch total loss 1.20392764\n",
      "Trained batch 308 batch loss 1.18407738 epoch total loss 1.20386314\n",
      "Trained batch 309 batch loss 1.30679643 epoch total loss 1.20419633\n",
      "Trained batch 310 batch loss 1.23909354 epoch total loss 1.20430887\n",
      "Trained batch 311 batch loss 1.13642335 epoch total loss 1.2040906\n",
      "Trained batch 312 batch loss 1.05153561 epoch total loss 1.2036016\n",
      "Trained batch 313 batch loss 1.11413932 epoch total loss 1.20331585\n",
      "Trained batch 314 batch loss 1.14839888 epoch total loss 1.20314097\n",
      "Trained batch 315 batch loss 1.1397717 epoch total loss 1.20293975\n",
      "Trained batch 316 batch loss 1.17022371 epoch total loss 1.20283628\n",
      "Trained batch 317 batch loss 1.3606751 epoch total loss 1.20333421\n",
      "Trained batch 318 batch loss 1.26719189 epoch total loss 1.20353496\n",
      "Trained batch 319 batch loss 1.19465637 epoch total loss 1.20350707\n",
      "Trained batch 320 batch loss 1.29761147 epoch total loss 1.20380116\n",
      "Trained batch 321 batch loss 1.37547183 epoch total loss 1.20433593\n",
      "Trained batch 322 batch loss 1.31113732 epoch total loss 1.20466757\n",
      "Trained batch 323 batch loss 1.3151747 epoch total loss 1.2050097\n",
      "Trained batch 324 batch loss 1.29232872 epoch total loss 1.20527923\n",
      "Trained batch 325 batch loss 1.17505741 epoch total loss 1.20518625\n",
      "Trained batch 326 batch loss 1.27053237 epoch total loss 1.20538664\n",
      "Trained batch 327 batch loss 1.23940325 epoch total loss 1.20549071\n",
      "Trained batch 328 batch loss 1.27384138 epoch total loss 1.20569909\n",
      "Trained batch 329 batch loss 1.24264181 epoch total loss 1.20581138\n",
      "Trained batch 330 batch loss 1.18432605 epoch total loss 1.20574629\n",
      "Trained batch 331 batch loss 1.32169509 epoch total loss 1.20609653\n",
      "Trained batch 332 batch loss 1.2732718 epoch total loss 1.20629895\n",
      "Trained batch 333 batch loss 1.20518756 epoch total loss 1.20629561\n",
      "Trained batch 334 batch loss 1.29282033 epoch total loss 1.20655465\n",
      "Trained batch 335 batch loss 1.18371606 epoch total loss 1.20648646\n",
      "Trained batch 336 batch loss 1.28529418 epoch total loss 1.20672107\n",
      "Trained batch 337 batch loss 1.35764861 epoch total loss 1.20716894\n",
      "Trained batch 338 batch loss 1.25279474 epoch total loss 1.20730388\n",
      "Trained batch 339 batch loss 1.24358201 epoch total loss 1.20741093\n",
      "Trained batch 340 batch loss 1.28084087 epoch total loss 1.20762694\n",
      "Trained batch 341 batch loss 1.05383086 epoch total loss 1.20717597\n",
      "Trained batch 342 batch loss 1.07893729 epoch total loss 1.20680106\n",
      "Trained batch 343 batch loss 1.148597 epoch total loss 1.2066313\n",
      "Trained batch 344 batch loss 1.08524466 epoch total loss 1.20627844\n",
      "Trained batch 345 batch loss 1.24345207 epoch total loss 1.20638609\n",
      "Trained batch 346 batch loss 1.29051447 epoch total loss 1.20662928\n",
      "Trained batch 347 batch loss 1.20461392 epoch total loss 1.20662355\n",
      "Trained batch 348 batch loss 1.27343941 epoch total loss 1.20681548\n",
      "Trained batch 349 batch loss 1.18087792 epoch total loss 1.20674121\n",
      "Trained batch 350 batch loss 1.22463751 epoch total loss 1.20679235\n",
      "Trained batch 351 batch loss 1.2730186 epoch total loss 1.20698094\n",
      "Trained batch 352 batch loss 1.06716728 epoch total loss 1.20658386\n",
      "Trained batch 353 batch loss 1.18056798 epoch total loss 1.20651007\n",
      "Trained batch 354 batch loss 1.27278531 epoch total loss 1.20669734\n",
      "Trained batch 355 batch loss 1.41909933 epoch total loss 1.20729566\n",
      "Trained batch 356 batch loss 1.14022732 epoch total loss 1.20710731\n",
      "Trained batch 357 batch loss 1.01637232 epoch total loss 1.20657301\n",
      "Trained batch 358 batch loss 1.07639599 epoch total loss 1.2062093\n",
      "Trained batch 359 batch loss 1.06118584 epoch total loss 1.20580542\n",
      "Trained batch 360 batch loss 1.07561231 epoch total loss 1.20544374\n",
      "Trained batch 361 batch loss 1.01067162 epoch total loss 1.2049042\n",
      "Trained batch 362 batch loss 1.09878397 epoch total loss 1.20461106\n",
      "Trained batch 363 batch loss 1.04769039 epoch total loss 1.20417881\n",
      "Trained batch 364 batch loss 1.06650078 epoch total loss 1.20380056\n",
      "Trained batch 365 batch loss 1.17528963 epoch total loss 1.20372248\n",
      "Trained batch 366 batch loss 1.23684621 epoch total loss 1.20381296\n",
      "Trained batch 367 batch loss 1.21569443 epoch total loss 1.20384538\n",
      "Trained batch 368 batch loss 1.17386353 epoch total loss 1.20376384\n",
      "Trained batch 369 batch loss 1.18184221 epoch total loss 1.20370448\n",
      "Trained batch 370 batch loss 1.27067602 epoch total loss 1.20388556\n",
      "Trained batch 371 batch loss 1.13194752 epoch total loss 1.20369172\n",
      "Trained batch 372 batch loss 1.15650392 epoch total loss 1.20356476\n",
      "Trained batch 373 batch loss 1.08616257 epoch total loss 1.20325\n",
      "Trained batch 374 batch loss 1.021245 epoch total loss 1.20276332\n",
      "Trained batch 375 batch loss 1.09478521 epoch total loss 1.20247543\n",
      "Trained batch 376 batch loss 1.00067914 epoch total loss 1.20193875\n",
      "Trained batch 377 batch loss 1.04887915 epoch total loss 1.20153272\n",
      "Trained batch 378 batch loss 1.06404269 epoch total loss 1.20116901\n",
      "Trained batch 379 batch loss 1.16386795 epoch total loss 1.20107067\n",
      "Trained batch 380 batch loss 1.13923419 epoch total loss 1.20090795\n",
      "Trained batch 381 batch loss 1.04737222 epoch total loss 1.2005049\n",
      "Trained batch 382 batch loss 1.08792484 epoch total loss 1.20021021\n",
      "Trained batch 383 batch loss 1.28958726 epoch total loss 1.20044351\n",
      "Trained batch 384 batch loss 1.23393595 epoch total loss 1.20053077\n",
      "Trained batch 385 batch loss 1.26780295 epoch total loss 1.20070553\n",
      "Trained batch 386 batch loss 1.16545737 epoch total loss 1.20061421\n",
      "Trained batch 387 batch loss 1.19732118 epoch total loss 1.20060563\n",
      "Trained batch 388 batch loss 1.130584 epoch total loss 1.20042527\n",
      "Trained batch 389 batch loss 1.18927824 epoch total loss 1.20039654\n",
      "Trained batch 390 batch loss 1.11812091 epoch total loss 1.20018566\n",
      "Trained batch 391 batch loss 1.0879612 epoch total loss 1.1998986\n",
      "Trained batch 392 batch loss 1.19632483 epoch total loss 1.19988942\n",
      "Trained batch 393 batch loss 1.19583988 epoch total loss 1.19987917\n",
      "Trained batch 394 batch loss 1.25818551 epoch total loss 1.20002711\n",
      "Trained batch 395 batch loss 1.23752213 epoch total loss 1.200122\n",
      "Trained batch 396 batch loss 1.14704418 epoch total loss 1.19998789\n",
      "Trained batch 397 batch loss 1.13984585 epoch total loss 1.19983637\n",
      "Trained batch 398 batch loss 1.24759984 epoch total loss 1.19995642\n",
      "Trained batch 399 batch loss 1.36815846 epoch total loss 1.20037794\n",
      "Trained batch 400 batch loss 0.965732872 epoch total loss 1.19979131\n",
      "Trained batch 401 batch loss 0.906635284 epoch total loss 1.19906032\n",
      "Trained batch 402 batch loss 1.2727313 epoch total loss 1.19924355\n",
      "Trained batch 403 batch loss 1.13671088 epoch total loss 1.19908845\n",
      "Trained batch 404 batch loss 1.11059332 epoch total loss 1.19886935\n",
      "Trained batch 405 batch loss 1.13765109 epoch total loss 1.19871831\n",
      "Trained batch 406 batch loss 1.1028378 epoch total loss 1.19848216\n",
      "Trained batch 407 batch loss 1.32156992 epoch total loss 1.19878459\n",
      "Trained batch 408 batch loss 1.27585542 epoch total loss 1.19897342\n",
      "Trained batch 409 batch loss 1.21077502 epoch total loss 1.19900227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 410 batch loss 1.2502861 epoch total loss 1.19912732\n",
      "Trained batch 411 batch loss 1.40411079 epoch total loss 1.19962609\n",
      "Trained batch 412 batch loss 1.28696108 epoch total loss 1.19983804\n",
      "Trained batch 413 batch loss 1.11658108 epoch total loss 1.19963646\n",
      "Trained batch 414 batch loss 1.05887592 epoch total loss 1.19929647\n",
      "Trained batch 415 batch loss 1.21394253 epoch total loss 1.19933176\n",
      "Trained batch 416 batch loss 1.23364449 epoch total loss 1.19941413\n",
      "Trained batch 417 batch loss 1.14759707 epoch total loss 1.19928992\n",
      "Trained batch 418 batch loss 1.22419 epoch total loss 1.1993494\n",
      "Trained batch 419 batch loss 1.27929151 epoch total loss 1.19954026\n",
      "Trained batch 420 batch loss 1.42581141 epoch total loss 1.20007896\n",
      "Trained batch 421 batch loss 1.2984637 epoch total loss 1.20031261\n",
      "Trained batch 422 batch loss 1.36064422 epoch total loss 1.20069265\n",
      "Trained batch 423 batch loss 1.21148288 epoch total loss 1.20071816\n",
      "Trained batch 424 batch loss 1.13863 epoch total loss 1.20057178\n",
      "Trained batch 425 batch loss 1.23682463 epoch total loss 1.20065701\n",
      "Trained batch 426 batch loss 1.25622773 epoch total loss 1.20078743\n",
      "Trained batch 427 batch loss 1.17954147 epoch total loss 1.20073771\n",
      "Trained batch 428 batch loss 1.17284822 epoch total loss 1.20067263\n",
      "Trained batch 429 batch loss 1.10746288 epoch total loss 1.20045543\n",
      "Trained batch 430 batch loss 1.10196435 epoch total loss 1.20022643\n",
      "Trained batch 431 batch loss 1.06559384 epoch total loss 1.1999141\n",
      "Trained batch 432 batch loss 1.04530621 epoch total loss 1.19955611\n",
      "Trained batch 433 batch loss 1.2188797 epoch total loss 1.1996007\n",
      "Trained batch 434 batch loss 1.24188125 epoch total loss 1.19969821\n",
      "Trained batch 435 batch loss 1.13659191 epoch total loss 1.19955313\n",
      "Trained batch 436 batch loss 1.10865736 epoch total loss 1.19934464\n",
      "Trained batch 437 batch loss 1.31609809 epoch total loss 1.19961178\n",
      "Trained batch 438 batch loss 1.18233156 epoch total loss 1.19957232\n",
      "Trained batch 439 batch loss 1.20261705 epoch total loss 1.19957924\n",
      "Trained batch 440 batch loss 1.09278488 epoch total loss 1.19933653\n",
      "Trained batch 441 batch loss 1.07143104 epoch total loss 1.19904649\n",
      "Trained batch 442 batch loss 1.12889647 epoch total loss 1.19888771\n",
      "Trained batch 443 batch loss 1.17412436 epoch total loss 1.19883192\n",
      "Trained batch 444 batch loss 1.25564492 epoch total loss 1.19895971\n",
      "Trained batch 445 batch loss 1.13618827 epoch total loss 1.19881868\n",
      "Trained batch 446 batch loss 1.04085135 epoch total loss 1.19846439\n",
      "Trained batch 447 batch loss 1.15871572 epoch total loss 1.19837546\n",
      "Trained batch 448 batch loss 1.24663186 epoch total loss 1.19848323\n",
      "Trained batch 449 batch loss 1.20799291 epoch total loss 1.19850445\n",
      "Trained batch 450 batch loss 1.21101 epoch total loss 1.19853222\n",
      "Trained batch 451 batch loss 1.13931894 epoch total loss 1.19840097\n",
      "Trained batch 452 batch loss 1.08697927 epoch total loss 1.19815445\n",
      "Trained batch 453 batch loss 1.08561647 epoch total loss 1.19790602\n",
      "Trained batch 454 batch loss 1.15872121 epoch total loss 1.19781959\n",
      "Trained batch 455 batch loss 1.00903547 epoch total loss 1.19740474\n",
      "Trained batch 456 batch loss 1.23849344 epoch total loss 1.19749475\n",
      "Trained batch 457 batch loss 1.24343431 epoch total loss 1.19759524\n",
      "Trained batch 458 batch loss 1.12530446 epoch total loss 1.19743741\n",
      "Trained batch 459 batch loss 1.10494363 epoch total loss 1.19723582\n",
      "Trained batch 460 batch loss 0.936882555 epoch total loss 1.19666982\n",
      "Trained batch 461 batch loss 1.21798348 epoch total loss 1.19671607\n",
      "Trained batch 462 batch loss 1.29835069 epoch total loss 1.19693601\n",
      "Trained batch 463 batch loss 1.35531008 epoch total loss 1.19727802\n",
      "Trained batch 464 batch loss 1.09044778 epoch total loss 1.19704783\n",
      "Trained batch 465 batch loss 1.11294162 epoch total loss 1.19686687\n",
      "Trained batch 466 batch loss 1.20515895 epoch total loss 1.19688463\n",
      "Trained batch 467 batch loss 1.0913626 epoch total loss 1.19665873\n",
      "Trained batch 468 batch loss 1.10738647 epoch total loss 1.19646788\n",
      "Trained batch 469 batch loss 1.13140035 epoch total loss 1.19632912\n",
      "Trained batch 470 batch loss 1.2196089 epoch total loss 1.19637871\n",
      "Trained batch 471 batch loss 1.14577055 epoch total loss 1.19627118\n",
      "Trained batch 472 batch loss 1.22254026 epoch total loss 1.19632685\n",
      "Trained batch 473 batch loss 1.20166993 epoch total loss 1.19633806\n",
      "Trained batch 474 batch loss 1.1010468 epoch total loss 1.19613707\n",
      "Trained batch 475 batch loss 1.12626719 epoch total loss 1.19599009\n",
      "Trained batch 476 batch loss 1.1426785 epoch total loss 1.19587815\n",
      "Trained batch 477 batch loss 1.20399308 epoch total loss 1.19589508\n",
      "Trained batch 478 batch loss 1.28322077 epoch total loss 1.1960777\n",
      "Trained batch 479 batch loss 1.19018638 epoch total loss 1.19606543\n",
      "Trained batch 480 batch loss 1.13369262 epoch total loss 1.19593549\n",
      "Trained batch 481 batch loss 1.19443905 epoch total loss 1.19593239\n",
      "Trained batch 482 batch loss 1.13287926 epoch total loss 1.1958015\n",
      "Trained batch 483 batch loss 1.11011708 epoch total loss 1.19562411\n",
      "Trained batch 484 batch loss 1.06212449 epoch total loss 1.19534826\n",
      "Trained batch 485 batch loss 1.02352881 epoch total loss 1.19499397\n",
      "Trained batch 486 batch loss 1.10306358 epoch total loss 1.19480491\n",
      "Trained batch 487 batch loss 0.999660969 epoch total loss 1.19440413\n",
      "Trained batch 488 batch loss 1.09947443 epoch total loss 1.19420958\n",
      "Trained batch 489 batch loss 0.989096165 epoch total loss 1.19379008\n",
      "Trained batch 490 batch loss 1.20949411 epoch total loss 1.19382215\n",
      "Trained batch 491 batch loss 1.3372668 epoch total loss 1.19411433\n",
      "Trained batch 492 batch loss 1.26651824 epoch total loss 1.19426155\n",
      "Trained batch 493 batch loss 1.1198765 epoch total loss 1.19411063\n",
      "Trained batch 494 batch loss 1.20315611 epoch total loss 1.19412899\n",
      "Trained batch 495 batch loss 1.31584799 epoch total loss 1.19437492\n",
      "Trained batch 496 batch loss 1.08020318 epoch total loss 1.19414473\n",
      "Trained batch 497 batch loss 1.15759778 epoch total loss 1.19407117\n",
      "Trained batch 498 batch loss 1.2720468 epoch total loss 1.1942277\n",
      "Trained batch 499 batch loss 1.28083098 epoch total loss 1.19440126\n",
      "Trained batch 500 batch loss 1.28217447 epoch total loss 1.19457674\n",
      "Trained batch 501 batch loss 1.31222844 epoch total loss 1.1948117\n",
      "Trained batch 502 batch loss 1.25891924 epoch total loss 1.19493937\n",
      "Trained batch 503 batch loss 1.27751338 epoch total loss 1.19510353\n",
      "Trained batch 504 batch loss 1.29757166 epoch total loss 1.19530678\n",
      "Trained batch 505 batch loss 1.18407714 epoch total loss 1.19528461\n",
      "Trained batch 506 batch loss 1.18637621 epoch total loss 1.19526708\n",
      "Trained batch 507 batch loss 1.18584323 epoch total loss 1.19524848\n",
      "Trained batch 508 batch loss 0.992392898 epoch total loss 1.19484913\n",
      "Trained batch 509 batch loss 0.949931383 epoch total loss 1.194368\n",
      "Trained batch 510 batch loss 1.08785558 epoch total loss 1.19415903\n",
      "Trained batch 511 batch loss 1.15754533 epoch total loss 1.19408739\n",
      "Trained batch 512 batch loss 0.960081935 epoch total loss 1.19363034\n",
      "Trained batch 513 batch loss 0.971855462 epoch total loss 1.19319808\n",
      "Trained batch 514 batch loss 0.860673845 epoch total loss 1.19255102\n",
      "Trained batch 515 batch loss 0.892226577 epoch total loss 1.19196784\n",
      "Trained batch 516 batch loss 1.16645348 epoch total loss 1.19191837\n",
      "Trained batch 517 batch loss 1.13886857 epoch total loss 1.19181573\n",
      "Trained batch 518 batch loss 1.07972693 epoch total loss 1.19159937\n",
      "Trained batch 519 batch loss 1.20368171 epoch total loss 1.19162261\n",
      "Trained batch 520 batch loss 1.24986076 epoch total loss 1.19173467\n",
      "Trained batch 521 batch loss 1.19768786 epoch total loss 1.19174612\n",
      "Trained batch 522 batch loss 1.22538221 epoch total loss 1.19181061\n",
      "Trained batch 523 batch loss 1.1960386 epoch total loss 1.19181871\n",
      "Trained batch 524 batch loss 0.972424269 epoch total loss 1.19139993\n",
      "Trained batch 525 batch loss 1.01456249 epoch total loss 1.19106317\n",
      "Trained batch 526 batch loss 1.06207037 epoch total loss 1.19081795\n",
      "Trained batch 527 batch loss 1.15770888 epoch total loss 1.19075513\n",
      "Trained batch 528 batch loss 1.09071684 epoch total loss 1.19056559\n",
      "Trained batch 529 batch loss 1.07727134 epoch total loss 1.19035149\n",
      "Trained batch 530 batch loss 1.16007316 epoch total loss 1.19029438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 531 batch loss 1.41695833 epoch total loss 1.19072115\n",
      "Trained batch 532 batch loss 1.21774447 epoch total loss 1.19077206\n",
      "Trained batch 533 batch loss 1.22778285 epoch total loss 1.19084144\n",
      "Trained batch 534 batch loss 1.14381742 epoch total loss 1.19075334\n",
      "Trained batch 535 batch loss 1.16007924 epoch total loss 1.19069612\n",
      "Trained batch 536 batch loss 1.18414 epoch total loss 1.19068384\n",
      "Trained batch 537 batch loss 1.15933108 epoch total loss 1.19062543\n",
      "Trained batch 538 batch loss 1.32382393 epoch total loss 1.19087303\n",
      "Trained batch 539 batch loss 1.26988888 epoch total loss 1.19101965\n",
      "Trained batch 540 batch loss 1.23986828 epoch total loss 1.19111013\n",
      "Trained batch 541 batch loss 1.12218893 epoch total loss 1.1909827\n",
      "Trained batch 542 batch loss 1.17797852 epoch total loss 1.19095874\n",
      "Trained batch 543 batch loss 1.34901392 epoch total loss 1.19124973\n",
      "Trained batch 544 batch loss 1.28331804 epoch total loss 1.19141901\n",
      "Trained batch 545 batch loss 1.17675745 epoch total loss 1.19139218\n",
      "Trained batch 546 batch loss 1.20179439 epoch total loss 1.19141114\n",
      "Trained batch 547 batch loss 1.22745 epoch total loss 1.19147706\n",
      "Trained batch 548 batch loss 1.20698595 epoch total loss 1.19150543\n",
      "Trained batch 549 batch loss 1.19840133 epoch total loss 1.19151795\n",
      "Trained batch 550 batch loss 1.08718324 epoch total loss 1.19132829\n",
      "Trained batch 551 batch loss 1.0127548 epoch total loss 1.19100416\n",
      "Trained batch 552 batch loss 1.0814203 epoch total loss 1.19080567\n",
      "Trained batch 553 batch loss 1.09204292 epoch total loss 1.19062698\n",
      "Trained batch 554 batch loss 1.05924976 epoch total loss 1.19038987\n",
      "Trained batch 555 batch loss 1.05408645 epoch total loss 1.1901443\n",
      "Trained batch 556 batch loss 1.08569717 epoch total loss 1.18995643\n",
      "Trained batch 557 batch loss 1.18743932 epoch total loss 1.1899519\n",
      "Trained batch 558 batch loss 1.20982921 epoch total loss 1.18998754\n",
      "Trained batch 559 batch loss 1.09077692 epoch total loss 1.18981\n",
      "Trained batch 560 batch loss 1.10929275 epoch total loss 1.18966627\n",
      "Trained batch 561 batch loss 1.26668906 epoch total loss 1.1898036\n",
      "Trained batch 562 batch loss 1.26276731 epoch total loss 1.18993342\n",
      "Trained batch 563 batch loss 1.27110958 epoch total loss 1.19007754\n",
      "Trained batch 564 batch loss 1.26980519 epoch total loss 1.19021893\n",
      "Trained batch 565 batch loss 1.25030923 epoch total loss 1.19032526\n",
      "Trained batch 566 batch loss 1.22206676 epoch total loss 1.19038129\n",
      "Trained batch 567 batch loss 1.32398057 epoch total loss 1.19061685\n",
      "Trained batch 568 batch loss 1.25708652 epoch total loss 1.19073391\n",
      "Trained batch 569 batch loss 1.27545333 epoch total loss 1.1908828\n",
      "Trained batch 570 batch loss 1.25055552 epoch total loss 1.19098747\n",
      "Trained batch 571 batch loss 1.23519111 epoch total loss 1.19106483\n",
      "Trained batch 572 batch loss 1.23926246 epoch total loss 1.19114912\n",
      "Trained batch 573 batch loss 1.14074922 epoch total loss 1.19106114\n",
      "Trained batch 574 batch loss 1.14102364 epoch total loss 1.190974\n",
      "Trained batch 575 batch loss 1.14607298 epoch total loss 1.19089592\n",
      "Trained batch 576 batch loss 1.1889255 epoch total loss 1.19089246\n",
      "Trained batch 577 batch loss 1.11519229 epoch total loss 1.19076121\n",
      "Trained batch 578 batch loss 1.16414642 epoch total loss 1.19071507\n",
      "Trained batch 579 batch loss 1.01041865 epoch total loss 1.1904037\n",
      "Trained batch 580 batch loss 1.07351589 epoch total loss 1.19020212\n",
      "Trained batch 581 batch loss 1.12106586 epoch total loss 1.19008327\n",
      "Trained batch 582 batch loss 1.25235748 epoch total loss 1.19019032\n",
      "Trained batch 583 batch loss 1.23366368 epoch total loss 1.19026482\n",
      "Trained batch 584 batch loss 1.27339447 epoch total loss 1.19040716\n",
      "Trained batch 585 batch loss 1.09929776 epoch total loss 1.19025135\n",
      "Trained batch 586 batch loss 1.32262135 epoch total loss 1.19047725\n",
      "Trained batch 587 batch loss 1.19703197 epoch total loss 1.19048846\n",
      "Trained batch 588 batch loss 1.15762854 epoch total loss 1.19043255\n",
      "Trained batch 589 batch loss 1.27179575 epoch total loss 1.19057071\n",
      "Trained batch 590 batch loss 1.22340417 epoch total loss 1.19062638\n",
      "Trained batch 591 batch loss 1.21448851 epoch total loss 1.19066668\n",
      "Trained batch 592 batch loss 1.1890558 epoch total loss 1.19066393\n",
      "Trained batch 593 batch loss 1.11779916 epoch total loss 1.19054103\n",
      "Trained batch 594 batch loss 1.11986709 epoch total loss 1.19042206\n",
      "Trained batch 595 batch loss 1.06155217 epoch total loss 1.19020545\n",
      "Trained batch 596 batch loss 1.20016623 epoch total loss 1.19022226\n",
      "Trained batch 597 batch loss 1.30690098 epoch total loss 1.19041765\n",
      "Trained batch 598 batch loss 1.22094893 epoch total loss 1.19046867\n",
      "Trained batch 599 batch loss 1.2447716 epoch total loss 1.19055927\n",
      "Trained batch 600 batch loss 1.22609174 epoch total loss 1.19061852\n",
      "Trained batch 601 batch loss 1.14970374 epoch total loss 1.19055045\n",
      "Trained batch 602 batch loss 1.24082458 epoch total loss 1.19063401\n",
      "Trained batch 603 batch loss 1.1847713 epoch total loss 1.19062424\n",
      "Trained batch 604 batch loss 1.1639303 epoch total loss 1.19058\n",
      "Trained batch 605 batch loss 1.12554789 epoch total loss 1.1904726\n",
      "Trained batch 606 batch loss 1.1098578 epoch total loss 1.19033957\n",
      "Trained batch 607 batch loss 1.1634115 epoch total loss 1.1902951\n",
      "Trained batch 608 batch loss 1.27922058 epoch total loss 1.19044149\n",
      "Trained batch 609 batch loss 1.19341779 epoch total loss 1.19044638\n",
      "Trained batch 610 batch loss 1.2085886 epoch total loss 1.19047606\n",
      "Trained batch 611 batch loss 1.17458832 epoch total loss 1.19045007\n",
      "Trained batch 612 batch loss 1.04992485 epoch total loss 1.19022048\n",
      "Trained batch 613 batch loss 1.17020106 epoch total loss 1.19018781\n",
      "Trained batch 614 batch loss 1.21474814 epoch total loss 1.19022775\n",
      "Trained batch 615 batch loss 1.2959044 epoch total loss 1.19039965\n",
      "Trained batch 616 batch loss 1.21208322 epoch total loss 1.19043481\n",
      "Trained batch 617 batch loss 1.22224927 epoch total loss 1.19048631\n",
      "Trained batch 618 batch loss 1.31239414 epoch total loss 1.1906836\n",
      "Trained batch 619 batch loss 1.21805167 epoch total loss 1.19072783\n",
      "Trained batch 620 batch loss 1.3022337 epoch total loss 1.19090772\n",
      "Trained batch 621 batch loss 1.33838844 epoch total loss 1.19114518\n",
      "Trained batch 622 batch loss 1.23011065 epoch total loss 1.19120789\n",
      "Trained batch 623 batch loss 1.16955853 epoch total loss 1.19117308\n",
      "Trained batch 624 batch loss 1.16055119 epoch total loss 1.19112396\n",
      "Trained batch 625 batch loss 1.2221899 epoch total loss 1.19117367\n",
      "Trained batch 626 batch loss 1.07497776 epoch total loss 1.19098794\n",
      "Trained batch 627 batch loss 1.07301211 epoch total loss 1.19079983\n",
      "Trained batch 628 batch loss 1.14278913 epoch total loss 1.1907233\n",
      "Trained batch 629 batch loss 1.18312144 epoch total loss 1.19071114\n",
      "Trained batch 630 batch loss 1.15585852 epoch total loss 1.19065595\n",
      "Trained batch 631 batch loss 1.23678398 epoch total loss 1.19072902\n",
      "Trained batch 632 batch loss 1.21642113 epoch total loss 1.19076967\n",
      "Trained batch 633 batch loss 1.24001622 epoch total loss 1.1908474\n",
      "Trained batch 634 batch loss 1.33675456 epoch total loss 1.19107747\n",
      "Trained batch 635 batch loss 1.16336417 epoch total loss 1.19103384\n",
      "Trained batch 636 batch loss 1.24079192 epoch total loss 1.19111216\n",
      "Trained batch 637 batch loss 1.23965442 epoch total loss 1.19118822\n",
      "Trained batch 638 batch loss 1.25937366 epoch total loss 1.19129515\n",
      "Trained batch 639 batch loss 1.24308562 epoch total loss 1.19137621\n",
      "Trained batch 640 batch loss 1.25962663 epoch total loss 1.1914829\n",
      "Trained batch 641 batch loss 1.19116306 epoch total loss 1.19148242\n",
      "Trained batch 642 batch loss 1.36850095 epoch total loss 1.19175816\n",
      "Trained batch 643 batch loss 1.24455917 epoch total loss 1.19184029\n",
      "Trained batch 644 batch loss 1.20962954 epoch total loss 1.19186795\n",
      "Trained batch 645 batch loss 1.20167232 epoch total loss 1.19188321\n",
      "Trained batch 646 batch loss 1.22103405 epoch total loss 1.19192827\n",
      "Trained batch 647 batch loss 1.26315868 epoch total loss 1.19203842\n",
      "Trained batch 648 batch loss 1.23592401 epoch total loss 1.19210613\n",
      "Trained batch 649 batch loss 1.12129617 epoch total loss 1.19199693\n",
      "Trained batch 650 batch loss 1.09264708 epoch total loss 1.19184411\n",
      "Trained batch 651 batch loss 1.32249355 epoch total loss 1.19204485\n",
      "Trained batch 652 batch loss 1.17924345 epoch total loss 1.19202518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 653 batch loss 1.09187484 epoch total loss 1.19187176\n",
      "Trained batch 654 batch loss 1.23994708 epoch total loss 1.19194531\n",
      "Trained batch 655 batch loss 1.29477203 epoch total loss 1.19210231\n",
      "Trained batch 656 batch loss 1.13723195 epoch total loss 1.19201863\n",
      "Trained batch 657 batch loss 1.21981406 epoch total loss 1.19206095\n",
      "Trained batch 658 batch loss 1.27638149 epoch total loss 1.1921891\n",
      "Trained batch 659 batch loss 1.29746711 epoch total loss 1.19234884\n",
      "Trained batch 660 batch loss 1.15852 epoch total loss 1.19229758\n",
      "Trained batch 661 batch loss 1.17678499 epoch total loss 1.19227409\n",
      "Trained batch 662 batch loss 1.17020738 epoch total loss 1.19224072\n",
      "Trained batch 663 batch loss 1.30779767 epoch total loss 1.192415\n",
      "Trained batch 664 batch loss 1.19197011 epoch total loss 1.19241428\n",
      "Trained batch 665 batch loss 1.12461567 epoch total loss 1.19231236\n",
      "Trained batch 666 batch loss 1.04891539 epoch total loss 1.19209707\n",
      "Trained batch 667 batch loss 0.982115 epoch total loss 1.19178224\n",
      "Trained batch 668 batch loss 1.06412256 epoch total loss 1.19159114\n",
      "Trained batch 669 batch loss 1.10570598 epoch total loss 1.19146276\n",
      "Trained batch 670 batch loss 1.09583712 epoch total loss 1.19132006\n",
      "Trained batch 671 batch loss 1.13058233 epoch total loss 1.19122946\n",
      "Trained batch 672 batch loss 1.03891599 epoch total loss 1.19100285\n",
      "Trained batch 673 batch loss 1.11101627 epoch total loss 1.19088399\n",
      "Trained batch 674 batch loss 1.02389646 epoch total loss 1.1906364\n",
      "Trained batch 675 batch loss 1.08595109 epoch total loss 1.19048119\n",
      "Trained batch 676 batch loss 1.10505104 epoch total loss 1.19035482\n",
      "Trained batch 677 batch loss 1.07264531 epoch total loss 1.1901809\n",
      "Trained batch 678 batch loss 1.18265343 epoch total loss 1.19016993\n",
      "Trained batch 679 batch loss 1.37030315 epoch total loss 1.19043517\n",
      "Trained batch 680 batch loss 1.22226357 epoch total loss 1.19048202\n",
      "Trained batch 681 batch loss 1.24769235 epoch total loss 1.19056606\n",
      "Trained batch 682 batch loss 1.27078366 epoch total loss 1.19068372\n",
      "Trained batch 683 batch loss 1.18134642 epoch total loss 1.19067\n",
      "Trained batch 684 batch loss 1.12837887 epoch total loss 1.19057894\n",
      "Trained batch 685 batch loss 1.01759398 epoch total loss 1.19032633\n",
      "Trained batch 686 batch loss 1.12276077 epoch total loss 1.19022787\n",
      "Trained batch 687 batch loss 1.18069434 epoch total loss 1.19021392\n",
      "Trained batch 688 batch loss 1.31140363 epoch total loss 1.19039\n",
      "Trained batch 689 batch loss 1.23448634 epoch total loss 1.19045401\n",
      "Trained batch 690 batch loss 1.1467675 epoch total loss 1.19039083\n",
      "Trained batch 691 batch loss 1.17110813 epoch total loss 1.19036281\n",
      "Trained batch 692 batch loss 1.25345957 epoch total loss 1.19045401\n",
      "Trained batch 693 batch loss 1.13204 epoch total loss 1.19036973\n",
      "Trained batch 694 batch loss 0.990985632 epoch total loss 1.19008243\n",
      "Trained batch 695 batch loss 1.02279425 epoch total loss 1.18984163\n",
      "Trained batch 696 batch loss 1.10242033 epoch total loss 1.18971598\n",
      "Trained batch 697 batch loss 1.04856431 epoch total loss 1.18951356\n",
      "Trained batch 698 batch loss 1.24179959 epoch total loss 1.18958843\n",
      "Trained batch 699 batch loss 1.24454784 epoch total loss 1.18966711\n",
      "Trained batch 700 batch loss 1.22081113 epoch total loss 1.18971169\n",
      "Trained batch 701 batch loss 1.18778634 epoch total loss 1.18970895\n",
      "Trained batch 702 batch loss 1.2555263 epoch total loss 1.18980277\n",
      "Trained batch 703 batch loss 1.37438798 epoch total loss 1.19006526\n",
      "Trained batch 704 batch loss 1.24272227 epoch total loss 1.19014013\n",
      "Trained batch 705 batch loss 1.26302314 epoch total loss 1.19024348\n",
      "Trained batch 706 batch loss 1.16608822 epoch total loss 1.19020927\n",
      "Trained batch 707 batch loss 1.08703661 epoch total loss 1.19006336\n",
      "Trained batch 708 batch loss 1.07933319 epoch total loss 1.18990695\n",
      "Trained batch 709 batch loss 1.19962823 epoch total loss 1.18992066\n",
      "Trained batch 710 batch loss 1.26686847 epoch total loss 1.19002903\n",
      "Trained batch 711 batch loss 1.33442461 epoch total loss 1.19023204\n",
      "Trained batch 712 batch loss 1.2172488 epoch total loss 1.19027\n",
      "Trained batch 713 batch loss 1.22170138 epoch total loss 1.19031405\n",
      "Trained batch 714 batch loss 1.13763463 epoch total loss 1.19024026\n",
      "Trained batch 715 batch loss 1.1717943 epoch total loss 1.19021451\n",
      "Trained batch 716 batch loss 1.14361942 epoch total loss 1.19014943\n",
      "Trained batch 717 batch loss 1.15919232 epoch total loss 1.19010615\n",
      "Trained batch 718 batch loss 1.29879284 epoch total loss 1.19025755\n",
      "Trained batch 719 batch loss 1.11643291 epoch total loss 1.19015491\n",
      "Trained batch 720 batch loss 1.30031753 epoch total loss 1.19030786\n",
      "Trained batch 721 batch loss 1.43461013 epoch total loss 1.19064677\n",
      "Trained batch 722 batch loss 1.34573686 epoch total loss 1.19086158\n",
      "Trained batch 723 batch loss 1.2273252 epoch total loss 1.19091201\n",
      "Trained batch 724 batch loss 1.19057989 epoch total loss 1.19091153\n",
      "Trained batch 725 batch loss 0.939353645 epoch total loss 1.19056451\n",
      "Trained batch 726 batch loss 0.941707551 epoch total loss 1.19022167\n",
      "Trained batch 727 batch loss 1.16967762 epoch total loss 1.19019341\n",
      "Trained batch 728 batch loss 1.10966253 epoch total loss 1.19008279\n",
      "Trained batch 729 batch loss 0.882722 epoch total loss 1.18966126\n",
      "Trained batch 730 batch loss 0.885379195 epoch total loss 1.18924439\n",
      "Trained batch 731 batch loss 1.02281606 epoch total loss 1.18901682\n",
      "Trained batch 732 batch loss 1.01365447 epoch total loss 1.18877721\n",
      "Trained batch 733 batch loss 1.16167033 epoch total loss 1.18874025\n",
      "Trained batch 734 batch loss 1.12830806 epoch total loss 1.18865788\n",
      "Trained batch 735 batch loss 1.15671539 epoch total loss 1.18861449\n",
      "Trained batch 736 batch loss 1.19307685 epoch total loss 1.18862057\n",
      "Trained batch 737 batch loss 1.24533319 epoch total loss 1.18869746\n",
      "Trained batch 738 batch loss 1.21633542 epoch total loss 1.18873489\n",
      "Trained batch 739 batch loss 1.12518072 epoch total loss 1.18864894\n",
      "Trained batch 740 batch loss 1.17073095 epoch total loss 1.18862474\n",
      "Trained batch 741 batch loss 1.03113663 epoch total loss 1.18841219\n",
      "Trained batch 742 batch loss 0.974484086 epoch total loss 1.18812382\n",
      "Trained batch 743 batch loss 0.992325485 epoch total loss 1.18786025\n",
      "Trained batch 744 batch loss 1.18106222 epoch total loss 1.18785119\n",
      "Trained batch 745 batch loss 1.1977793 epoch total loss 1.18786454\n",
      "Trained batch 746 batch loss 1.32965112 epoch total loss 1.18805456\n",
      "Trained batch 747 batch loss 1.33926404 epoch total loss 1.18825698\n",
      "Trained batch 748 batch loss 1.3965863 epoch total loss 1.18853557\n",
      "Trained batch 749 batch loss 1.19540632 epoch total loss 1.18854475\n",
      "Trained batch 750 batch loss 1.32650852 epoch total loss 1.18872881\n",
      "Trained batch 751 batch loss 1.13506603 epoch total loss 1.18865728\n",
      "Trained batch 752 batch loss 1.02659941 epoch total loss 1.18844187\n",
      "Trained batch 753 batch loss 1.15146625 epoch total loss 1.18839276\n",
      "Trained batch 754 batch loss 1.20044208 epoch total loss 1.18840873\n",
      "Trained batch 755 batch loss 1.22880387 epoch total loss 1.18846226\n",
      "Trained batch 756 batch loss 1.14151442 epoch total loss 1.18840015\n",
      "Trained batch 757 batch loss 1.23109972 epoch total loss 1.18845654\n",
      "Trained batch 758 batch loss 1.14893794 epoch total loss 1.18840444\n",
      "Trained batch 759 batch loss 1.17191136 epoch total loss 1.18838274\n",
      "Trained batch 760 batch loss 1.20046139 epoch total loss 1.1883986\n",
      "Trained batch 761 batch loss 1.11322093 epoch total loss 1.18829978\n",
      "Trained batch 762 batch loss 1.24022162 epoch total loss 1.18836796\n",
      "Trained batch 763 batch loss 1.19955659 epoch total loss 1.18838263\n",
      "Trained batch 764 batch loss 1.08730245 epoch total loss 1.1882503\n",
      "Trained batch 765 batch loss 1.20091236 epoch total loss 1.18826687\n",
      "Trained batch 766 batch loss 1.10709488 epoch total loss 1.1881609\n",
      "Trained batch 767 batch loss 1.17269647 epoch total loss 1.18814075\n",
      "Trained batch 768 batch loss 1.26553214 epoch total loss 1.18824148\n",
      "Trained batch 769 batch loss 1.24030018 epoch total loss 1.18830919\n",
      "Trained batch 770 batch loss 1.19663954 epoch total loss 1.18832\n",
      "Trained batch 771 batch loss 1.10034478 epoch total loss 1.18820596\n",
      "Trained batch 772 batch loss 1.09896731 epoch total loss 1.18809032\n",
      "Trained batch 773 batch loss 1.15376973 epoch total loss 1.18804586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 774 batch loss 1.21667123 epoch total loss 1.18808281\n",
      "Trained batch 775 batch loss 1.24207377 epoch total loss 1.18815243\n",
      "Trained batch 776 batch loss 1.30879915 epoch total loss 1.18830788\n",
      "Trained batch 777 batch loss 1.36592567 epoch total loss 1.18853652\n",
      "Trained batch 778 batch loss 1.29198599 epoch total loss 1.18866944\n",
      "Trained batch 779 batch loss 1.20118093 epoch total loss 1.18868554\n",
      "Trained batch 780 batch loss 1.0307591 epoch total loss 1.188483\n",
      "Trained batch 781 batch loss 1.09932864 epoch total loss 1.18836892\n",
      "Trained batch 782 batch loss 1.12530422 epoch total loss 1.18828821\n",
      "Trained batch 783 batch loss 1.07079208 epoch total loss 1.18813813\n",
      "Trained batch 784 batch loss 1.16462469 epoch total loss 1.18810821\n",
      "Trained batch 785 batch loss 1.20418513 epoch total loss 1.18812859\n",
      "Trained batch 786 batch loss 1.1513176 epoch total loss 1.18808174\n",
      "Trained batch 787 batch loss 1.23161221 epoch total loss 1.18813705\n",
      "Trained batch 788 batch loss 1.21754336 epoch total loss 1.18817437\n",
      "Trained batch 789 batch loss 1.0587256 epoch total loss 1.18801033\n",
      "Trained batch 790 batch loss 1.01793706 epoch total loss 1.18779504\n",
      "Trained batch 791 batch loss 1.13966238 epoch total loss 1.18773425\n",
      "Trained batch 792 batch loss 1.17899072 epoch total loss 1.18772316\n",
      "Trained batch 793 batch loss 1.2044071 epoch total loss 1.18774426\n",
      "Trained batch 794 batch loss 1.28320158 epoch total loss 1.18786442\n",
      "Trained batch 795 batch loss 1.25729144 epoch total loss 1.18795168\n",
      "Trained batch 796 batch loss 1.18089199 epoch total loss 1.18794286\n",
      "Trained batch 797 batch loss 1.21499979 epoch total loss 1.18797684\n",
      "Trained batch 798 batch loss 1.12843084 epoch total loss 1.18790221\n",
      "Trained batch 799 batch loss 1.22679567 epoch total loss 1.18795097\n",
      "Trained batch 800 batch loss 1.17531347 epoch total loss 1.18793511\n",
      "Trained batch 801 batch loss 1.17550707 epoch total loss 1.18791962\n",
      "Trained batch 802 batch loss 1.22080588 epoch total loss 1.18796062\n",
      "Trained batch 803 batch loss 1.23421955 epoch total loss 1.1880182\n",
      "Trained batch 804 batch loss 1.17208827 epoch total loss 1.18799841\n",
      "Trained batch 805 batch loss 1.23353279 epoch total loss 1.18805492\n",
      "Trained batch 806 batch loss 1.23635304 epoch total loss 1.18811476\n",
      "Trained batch 807 batch loss 1.13772261 epoch total loss 1.1880523\n",
      "Trained batch 808 batch loss 1.10612893 epoch total loss 1.18795097\n",
      "Trained batch 809 batch loss 0.953305423 epoch total loss 1.18766093\n",
      "Trained batch 810 batch loss 1.24544549 epoch total loss 1.18773222\n",
      "Trained batch 811 batch loss 1.17004013 epoch total loss 1.1877104\n",
      "Trained batch 812 batch loss 1.26551557 epoch total loss 1.18780625\n",
      "Trained batch 813 batch loss 1.31334043 epoch total loss 1.18796062\n",
      "Trained batch 814 batch loss 1.24253809 epoch total loss 1.18802774\n",
      "Trained batch 815 batch loss 1.33935606 epoch total loss 1.18821335\n",
      "Trained batch 816 batch loss 1.25198686 epoch total loss 1.18829155\n",
      "Trained batch 817 batch loss 1.24824071 epoch total loss 1.18836498\n",
      "Trained batch 818 batch loss 1.12618899 epoch total loss 1.18828893\n",
      "Trained batch 819 batch loss 1.27695835 epoch total loss 1.18839717\n",
      "Trained batch 820 batch loss 1.22048438 epoch total loss 1.18843627\n",
      "Trained batch 821 batch loss 1.30073071 epoch total loss 1.188573\n",
      "Trained batch 822 batch loss 1.28535497 epoch total loss 1.18869078\n",
      "Trained batch 823 batch loss 1.19138503 epoch total loss 1.18869412\n",
      "Trained batch 824 batch loss 1.18093503 epoch total loss 1.18868458\n",
      "Trained batch 825 batch loss 1.21729183 epoch total loss 1.18871927\n",
      "Trained batch 826 batch loss 1.1248585 epoch total loss 1.18864202\n",
      "Trained batch 827 batch loss 1.0761652 epoch total loss 1.18850601\n",
      "Trained batch 828 batch loss 1.03178239 epoch total loss 1.1883167\n",
      "Trained batch 829 batch loss 0.99943161 epoch total loss 1.18808889\n",
      "Trained batch 830 batch loss 1.1720202 epoch total loss 1.18806958\n",
      "Trained batch 831 batch loss 1.10536706 epoch total loss 1.18797\n",
      "Trained batch 832 batch loss 1.27104568 epoch total loss 1.18806982\n",
      "Trained batch 833 batch loss 1.38765764 epoch total loss 1.18830943\n",
      "Trained batch 834 batch loss 1.17248178 epoch total loss 1.18829048\n",
      "Trained batch 835 batch loss 1.09785736 epoch total loss 1.18818212\n",
      "Trained batch 836 batch loss 1.24938607 epoch total loss 1.18825531\n",
      "Trained batch 837 batch loss 1.08815706 epoch total loss 1.18813574\n",
      "Trained batch 838 batch loss 1.25625145 epoch total loss 1.18821692\n",
      "Trained batch 839 batch loss 1.1774019 epoch total loss 1.18820417\n",
      "Trained batch 840 batch loss 1.30562401 epoch total loss 1.18834388\n",
      "Trained batch 841 batch loss 1.32202792 epoch total loss 1.18850279\n",
      "Trained batch 842 batch loss 1.18258977 epoch total loss 1.18849587\n",
      "Trained batch 843 batch loss 1.21068299 epoch total loss 1.18852222\n",
      "Trained batch 844 batch loss 1.12008595 epoch total loss 1.18844104\n",
      "Trained batch 845 batch loss 1.06563139 epoch total loss 1.18829572\n",
      "Trained batch 846 batch loss 1.17622876 epoch total loss 1.18828142\n",
      "Trained batch 847 batch loss 1.15903354 epoch total loss 1.18824697\n",
      "Trained batch 848 batch loss 1.2122674 epoch total loss 1.18827522\n",
      "Trained batch 849 batch loss 1.39271879 epoch total loss 1.18851602\n",
      "Trained batch 850 batch loss 1.23417509 epoch total loss 1.18856978\n",
      "Trained batch 851 batch loss 1.1817925 epoch total loss 1.1885618\n",
      "Trained batch 852 batch loss 1.22423923 epoch total loss 1.18860364\n",
      "Trained batch 853 batch loss 1.19738269 epoch total loss 1.18861389\n",
      "Trained batch 854 batch loss 1.13102508 epoch total loss 1.18854654\n",
      "Trained batch 855 batch loss 1.17866635 epoch total loss 1.18853498\n",
      "Trained batch 856 batch loss 1.19113529 epoch total loss 1.18853807\n",
      "Trained batch 857 batch loss 1.14842916 epoch total loss 1.18849123\n",
      "Trained batch 858 batch loss 1.04937971 epoch total loss 1.1883291\n",
      "Trained batch 859 batch loss 1.03653669 epoch total loss 1.18815243\n",
      "Trained batch 860 batch loss 0.949763536 epoch total loss 1.18787527\n",
      "Trained batch 861 batch loss 1.09541631 epoch total loss 1.18776786\n",
      "Trained batch 862 batch loss 1.43540907 epoch total loss 1.18805516\n",
      "Trained batch 863 batch loss 1.44756091 epoch total loss 1.1883558\n",
      "Trained batch 864 batch loss 1.39689 epoch total loss 1.18859708\n",
      "Trained batch 865 batch loss 1.28086925 epoch total loss 1.18870378\n",
      "Trained batch 866 batch loss 1.30390799 epoch total loss 1.18883681\n",
      "Trained batch 867 batch loss 1.4401679 epoch total loss 1.18912673\n",
      "Trained batch 868 batch loss 1.15870202 epoch total loss 1.18909168\n",
      "Trained batch 869 batch loss 1.04827559 epoch total loss 1.18892956\n",
      "Trained batch 870 batch loss 1.24240053 epoch total loss 1.18899107\n",
      "Trained batch 871 batch loss 1.21271718 epoch total loss 1.18901837\n",
      "Trained batch 872 batch loss 1.15841866 epoch total loss 1.18898332\n",
      "Trained batch 873 batch loss 0.999869406 epoch total loss 1.18876672\n",
      "Trained batch 874 batch loss 1.08737302 epoch total loss 1.18865073\n",
      "Trained batch 875 batch loss 1.20091164 epoch total loss 1.18866479\n",
      "Trained batch 876 batch loss 1.1370734 epoch total loss 1.1886059\n",
      "Trained batch 877 batch loss 1.13957989 epoch total loss 1.18854988\n",
      "Trained batch 878 batch loss 1.09850311 epoch total loss 1.18844736\n",
      "Trained batch 879 batch loss 1.25731313 epoch total loss 1.18852568\n",
      "Trained batch 880 batch loss 1.22965181 epoch total loss 1.18857241\n",
      "Trained batch 881 batch loss 1.1432538 epoch total loss 1.18852103\n",
      "Trained batch 882 batch loss 1.15428495 epoch total loss 1.18848228\n",
      "Trained batch 883 batch loss 1.26146364 epoch total loss 1.1885649\n",
      "Trained batch 884 batch loss 1.21944 epoch total loss 1.18859982\n",
      "Trained batch 885 batch loss 1.19107533 epoch total loss 1.18860269\n",
      "Trained batch 886 batch loss 1.18879867 epoch total loss 1.18860292\n",
      "Trained batch 887 batch loss 1.11824441 epoch total loss 1.18852365\n",
      "Trained batch 888 batch loss 1.1397506 epoch total loss 1.18846869\n",
      "Trained batch 889 batch loss 1.16840506 epoch total loss 1.18844616\n",
      "Trained batch 890 batch loss 1.17255592 epoch total loss 1.1884284\n",
      "Trained batch 891 batch loss 1.10195601 epoch total loss 1.18833137\n",
      "Trained batch 892 batch loss 1.13553345 epoch total loss 1.18827212\n",
      "Trained batch 893 batch loss 1.01780367 epoch total loss 1.18808126\n",
      "Trained batch 894 batch loss 1.10679483 epoch total loss 1.18799031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 895 batch loss 1.21697497 epoch total loss 1.18802261\n",
      "Trained batch 896 batch loss 1.17920017 epoch total loss 1.18801284\n",
      "Trained batch 897 batch loss 1.02979672 epoch total loss 1.18783641\n",
      "Trained batch 898 batch loss 1.08973145 epoch total loss 1.18772709\n",
      "Trained batch 899 batch loss 0.923779845 epoch total loss 1.1874336\n",
      "Trained batch 900 batch loss 1.02746427 epoch total loss 1.18725586\n",
      "Trained batch 901 batch loss 1.16468239 epoch total loss 1.18723083\n",
      "Trained batch 902 batch loss 1.2348274 epoch total loss 1.18728364\n",
      "Trained batch 903 batch loss 1.15031147 epoch total loss 1.18724263\n",
      "Trained batch 904 batch loss 1.24154127 epoch total loss 1.18730271\n",
      "Trained batch 905 batch loss 1.16086578 epoch total loss 1.1872735\n",
      "Trained batch 906 batch loss 1.05746448 epoch total loss 1.18713033\n",
      "Trained batch 907 batch loss 1.05404818 epoch total loss 1.18698359\n",
      "Trained batch 908 batch loss 1.06884384 epoch total loss 1.18685353\n",
      "Trained batch 909 batch loss 1.23883414 epoch total loss 1.18691075\n",
      "Trained batch 910 batch loss 1.17367 epoch total loss 1.1868962\n",
      "Trained batch 911 batch loss 1.35872257 epoch total loss 1.18708491\n",
      "Trained batch 912 batch loss 1.29274797 epoch total loss 1.18720067\n",
      "Trained batch 913 batch loss 1.17922616 epoch total loss 1.18719196\n",
      "Trained batch 914 batch loss 1.03015924 epoch total loss 1.18702018\n",
      "Trained batch 915 batch loss 1.08284664 epoch total loss 1.18690634\n",
      "Trained batch 916 batch loss 1.30556798 epoch total loss 1.1870358\n",
      "Trained batch 917 batch loss 1.29132724 epoch total loss 1.18714964\n",
      "Trained batch 918 batch loss 1.30148757 epoch total loss 1.18727422\n",
      "Trained batch 919 batch loss 1.22188354 epoch total loss 1.18731189\n",
      "Trained batch 920 batch loss 1.18797433 epoch total loss 1.1873126\n",
      "Trained batch 921 batch loss 1.24820876 epoch total loss 1.18737876\n",
      "Trained batch 922 batch loss 1.18491018 epoch total loss 1.18737602\n",
      "Trained batch 923 batch loss 1.30550528 epoch total loss 1.18750405\n",
      "Trained batch 924 batch loss 1.13456035 epoch total loss 1.18744671\n",
      "Trained batch 925 batch loss 1.20024753 epoch total loss 1.18746054\n",
      "Trained batch 926 batch loss 1.2089417 epoch total loss 1.18748379\n",
      "Trained batch 927 batch loss 1.20516574 epoch total loss 1.18750286\n",
      "Trained batch 928 batch loss 1.20518076 epoch total loss 1.18752193\n",
      "Trained batch 929 batch loss 1.21530008 epoch total loss 1.18755186\n",
      "Trained batch 930 batch loss 1.22723508 epoch total loss 1.18759465\n",
      "Trained batch 931 batch loss 1.31874907 epoch total loss 1.18773544\n",
      "Trained batch 932 batch loss 1.23810589 epoch total loss 1.18778956\n",
      "Trained batch 933 batch loss 1.07883 epoch total loss 1.18767285\n",
      "Trained batch 934 batch loss 1.00849569 epoch total loss 1.18748105\n",
      "Trained batch 935 batch loss 1.20084536 epoch total loss 1.18749535\n",
      "Trained batch 936 batch loss 1.22112691 epoch total loss 1.18753111\n",
      "Trained batch 937 batch loss 1.32884848 epoch total loss 1.18768203\n",
      "Trained batch 938 batch loss 1.25977278 epoch total loss 1.1877588\n",
      "Trained batch 939 batch loss 1.2719276 epoch total loss 1.18784857\n",
      "Trained batch 940 batch loss 1.34351349 epoch total loss 1.18801415\n",
      "Trained batch 941 batch loss 1.22361803 epoch total loss 1.18805194\n",
      "Trained batch 942 batch loss 1.30477643 epoch total loss 1.18817592\n",
      "Trained batch 943 batch loss 1.37862098 epoch total loss 1.18837798\n",
      "Trained batch 944 batch loss 1.22035134 epoch total loss 1.18841183\n",
      "Trained batch 945 batch loss 1.1155386 epoch total loss 1.18833458\n",
      "Trained batch 946 batch loss 1.11680818 epoch total loss 1.18825901\n",
      "Trained batch 947 batch loss 0.95244807 epoch total loss 1.18801\n",
      "Trained batch 948 batch loss 1.11769104 epoch total loss 1.18793571\n",
      "Trained batch 949 batch loss 1.1166786 epoch total loss 1.18786073\n",
      "Trained batch 950 batch loss 1.17887592 epoch total loss 1.18785119\n",
      "Trained batch 951 batch loss 1.27770841 epoch total loss 1.18794572\n",
      "Trained batch 952 batch loss 1.28424025 epoch total loss 1.18804669\n",
      "Trained batch 953 batch loss 1.22335911 epoch total loss 1.18808389\n",
      "Trained batch 954 batch loss 1.28176975 epoch total loss 1.188182\n",
      "Trained batch 955 batch loss 1.3944124 epoch total loss 1.188398\n",
      "Trained batch 956 batch loss 1.53424335 epoch total loss 1.1887598\n",
      "Trained batch 957 batch loss 1.25203156 epoch total loss 1.18882596\n",
      "Trained batch 958 batch loss 1.15519226 epoch total loss 1.1887908\n",
      "Trained batch 959 batch loss 1.1085403 epoch total loss 1.18870711\n",
      "Trained batch 960 batch loss 1.24597657 epoch total loss 1.18876672\n",
      "Trained batch 961 batch loss 1.23486578 epoch total loss 1.18881476\n",
      "Trained batch 962 batch loss 1.18767488 epoch total loss 1.18881345\n",
      "Trained batch 963 batch loss 1.26833594 epoch total loss 1.18889606\n",
      "Trained batch 964 batch loss 1.23112059 epoch total loss 1.18893981\n",
      "Trained batch 965 batch loss 1.34786165 epoch total loss 1.18910444\n",
      "Trained batch 966 batch loss 1.37836397 epoch total loss 1.18930042\n",
      "Trained batch 967 batch loss 1.32914448 epoch total loss 1.18944502\n",
      "Trained batch 968 batch loss 1.22808576 epoch total loss 1.18948495\n",
      "Trained batch 969 batch loss 1.14610624 epoch total loss 1.18944013\n",
      "Trained batch 970 batch loss 1.19308877 epoch total loss 1.18944395\n",
      "Trained batch 971 batch loss 1.22207165 epoch total loss 1.18947756\n",
      "Trained batch 972 batch loss 1.19506288 epoch total loss 1.18948328\n",
      "Trained batch 973 batch loss 1.14829671 epoch total loss 1.18944097\n",
      "Trained batch 974 batch loss 1.15881824 epoch total loss 1.18940949\n",
      "Trained batch 975 batch loss 1.2078371 epoch total loss 1.18942845\n",
      "Trained batch 976 batch loss 1.03232479 epoch total loss 1.18926752\n",
      "Trained batch 977 batch loss 0.985372961 epoch total loss 1.18905878\n",
      "Trained batch 978 batch loss 1.0265609 epoch total loss 1.18889272\n",
      "Trained batch 979 batch loss 1.1629926 epoch total loss 1.18886626\n",
      "Trained batch 980 batch loss 1.02734017 epoch total loss 1.18870139\n",
      "Trained batch 981 batch loss 1.15885651 epoch total loss 1.18867087\n",
      "Trained batch 982 batch loss 1.11321163 epoch total loss 1.18859398\n",
      "Trained batch 983 batch loss 1.05729282 epoch total loss 1.18846047\n",
      "Trained batch 984 batch loss 1.15177596 epoch total loss 1.18842304\n",
      "Trained batch 985 batch loss 1.2329917 epoch total loss 1.18846834\n",
      "Trained batch 986 batch loss 1.22133565 epoch total loss 1.18850172\n",
      "Trained batch 987 batch loss 1.07348061 epoch total loss 1.18838513\n",
      "Trained batch 988 batch loss 1.00704288 epoch total loss 1.18820167\n",
      "Trained batch 989 batch loss 1.14852691 epoch total loss 1.18816161\n",
      "Trained batch 990 batch loss 1.07437062 epoch total loss 1.18804657\n",
      "Trained batch 991 batch loss 1.20250428 epoch total loss 1.18806124\n",
      "Trained batch 992 batch loss 1.27755499 epoch total loss 1.18815148\n",
      "Trained batch 993 batch loss 1.33175373 epoch total loss 1.18829608\n",
      "Trained batch 994 batch loss 1.38389683 epoch total loss 1.18849289\n",
      "Trained batch 995 batch loss 1.17675579 epoch total loss 1.18848109\n",
      "Trained batch 996 batch loss 1.31066227 epoch total loss 1.18860376\n",
      "Trained batch 997 batch loss 1.05635571 epoch total loss 1.1884712\n",
      "Trained batch 998 batch loss 1.13399863 epoch total loss 1.1884166\n",
      "Trained batch 999 batch loss 1.18789315 epoch total loss 1.18841612\n",
      "Trained batch 1000 batch loss 1.23418438 epoch total loss 1.18846178\n",
      "Trained batch 1001 batch loss 1.19158864 epoch total loss 1.18846488\n",
      "Trained batch 1002 batch loss 1.15756035 epoch total loss 1.188434\n",
      "Trained batch 1003 batch loss 1.28723538 epoch total loss 1.18853259\n",
      "Trained batch 1004 batch loss 1.22851455 epoch total loss 1.18857241\n",
      "Trained batch 1005 batch loss 1.32252407 epoch total loss 1.18870568\n",
      "Trained batch 1006 batch loss 1.08127773 epoch total loss 1.18859887\n",
      "Trained batch 1007 batch loss 1.17523098 epoch total loss 1.18858552\n",
      "Trained batch 1008 batch loss 1.31493711 epoch total loss 1.18871093\n",
      "Trained batch 1009 batch loss 1.218135 epoch total loss 1.18874\n",
      "Trained batch 1010 batch loss 1.20757496 epoch total loss 1.18875861\n",
      "Trained batch 1011 batch loss 1.12748492 epoch total loss 1.18869805\n",
      "Trained batch 1012 batch loss 1.12946892 epoch total loss 1.18863952\n",
      "Trained batch 1013 batch loss 1.27533185 epoch total loss 1.18872511\n",
      "Trained batch 1014 batch loss 1.29929674 epoch total loss 1.18883419\n",
      "Trained batch 1015 batch loss 1.24607182 epoch total loss 1.1888907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1016 batch loss 1.28918 epoch total loss 1.1889894\n",
      "Trained batch 1017 batch loss 1.30774379 epoch total loss 1.18910611\n",
      "Trained batch 1018 batch loss 1.21430635 epoch total loss 1.1891309\n",
      "Trained batch 1019 batch loss 1.17272139 epoch total loss 1.18911481\n",
      "Trained batch 1020 batch loss 1.17549467 epoch total loss 1.18910146\n",
      "Trained batch 1021 batch loss 1.18382084 epoch total loss 1.18909633\n",
      "Trained batch 1022 batch loss 1.14148068 epoch total loss 1.18904972\n",
      "Trained batch 1023 batch loss 1.25386548 epoch total loss 1.18911314\n",
      "Trained batch 1024 batch loss 1.11846662 epoch total loss 1.18904412\n",
      "Trained batch 1025 batch loss 1.09117198 epoch total loss 1.18894863\n",
      "Trained batch 1026 batch loss 1.19981027 epoch total loss 1.18895924\n",
      "Trained batch 1027 batch loss 1.09429479 epoch total loss 1.18886697\n",
      "Trained batch 1028 batch loss 1.04624271 epoch total loss 1.18872833\n",
      "Trained batch 1029 batch loss 0.962041616 epoch total loss 1.18850803\n",
      "Trained batch 1030 batch loss 0.996098816 epoch total loss 1.18832123\n",
      "Trained batch 1031 batch loss 1.0966624 epoch total loss 1.1882323\n",
      "Trained batch 1032 batch loss 1.05097854 epoch total loss 1.18809938\n",
      "Trained batch 1033 batch loss 1.0847249 epoch total loss 1.18799925\n",
      "Trained batch 1034 batch loss 1.04565108 epoch total loss 1.18786156\n",
      "Trained batch 1035 batch loss 1.06592464 epoch total loss 1.18774378\n",
      "Trained batch 1036 batch loss 1.14971805 epoch total loss 1.18770707\n",
      "Trained batch 1037 batch loss 1.18450773 epoch total loss 1.18770385\n",
      "Trained batch 1038 batch loss 1.26200616 epoch total loss 1.18777537\n",
      "Trained batch 1039 batch loss 1.26725769 epoch total loss 1.18785191\n",
      "Trained batch 1040 batch loss 1.20654714 epoch total loss 1.18786991\n",
      "Trained batch 1041 batch loss 1.1824621 epoch total loss 1.18786466\n",
      "Trained batch 1042 batch loss 1.24013352 epoch total loss 1.18791485\n",
      "Trained batch 1043 batch loss 1.46314716 epoch total loss 1.18817866\n",
      "Trained batch 1044 batch loss 1.33688033 epoch total loss 1.18832111\n",
      "Trained batch 1045 batch loss 1.32748485 epoch total loss 1.18845439\n",
      "Trained batch 1046 batch loss 1.34349608 epoch total loss 1.18860257\n",
      "Trained batch 1047 batch loss 1.32026529 epoch total loss 1.18872845\n",
      "Trained batch 1048 batch loss 1.24346638 epoch total loss 1.18878055\n",
      "Trained batch 1049 batch loss 1.18390942 epoch total loss 1.18877602\n",
      "Trained batch 1050 batch loss 1.1884234 epoch total loss 1.18877566\n",
      "Trained batch 1051 batch loss 1.27257633 epoch total loss 1.18885541\n",
      "Trained batch 1052 batch loss 1.21826303 epoch total loss 1.18888342\n",
      "Trained batch 1053 batch loss 1.20188415 epoch total loss 1.1888957\n",
      "Trained batch 1054 batch loss 1.17870319 epoch total loss 1.18888605\n",
      "Trained batch 1055 batch loss 1.12684166 epoch total loss 1.18882728\n",
      "Trained batch 1056 batch loss 1.08218694 epoch total loss 1.18872631\n",
      "Trained batch 1057 batch loss 0.984105945 epoch total loss 1.18853271\n",
      "Trained batch 1058 batch loss 1.07671273 epoch total loss 1.18842697\n",
      "Trained batch 1059 batch loss 1.24087858 epoch total loss 1.18847644\n",
      "Trained batch 1060 batch loss 1.19999528 epoch total loss 1.18848729\n",
      "Trained batch 1061 batch loss 1.29529798 epoch total loss 1.1885879\n",
      "Trained batch 1062 batch loss 1.31328297 epoch total loss 1.18870533\n",
      "Trained batch 1063 batch loss 1.30733848 epoch total loss 1.18881691\n",
      "Trained batch 1064 batch loss 1.18339849 epoch total loss 1.18881178\n",
      "Trained batch 1065 batch loss 1.21913612 epoch total loss 1.18884027\n",
      "Trained batch 1066 batch loss 1.15984869 epoch total loss 1.18881297\n",
      "Trained batch 1067 batch loss 1.16032279 epoch total loss 1.18878627\n",
      "Trained batch 1068 batch loss 1.00834394 epoch total loss 1.18861723\n",
      "Trained batch 1069 batch loss 1.16559315 epoch total loss 1.18859577\n",
      "Trained batch 1070 batch loss 1.25070953 epoch total loss 1.18865383\n",
      "Trained batch 1071 batch loss 1.10786533 epoch total loss 1.18857849\n",
      "Trained batch 1072 batch loss 1.13645494 epoch total loss 1.18852985\n",
      "Trained batch 1073 batch loss 1.16040742 epoch total loss 1.18850362\n",
      "Trained batch 1074 batch loss 1.17748 epoch total loss 1.18849337\n",
      "Trained batch 1075 batch loss 1.25780463 epoch total loss 1.18855786\n",
      "Trained batch 1076 batch loss 1.08915615 epoch total loss 1.18846548\n",
      "Trained batch 1077 batch loss 1.14995718 epoch total loss 1.18842959\n",
      "Trained batch 1078 batch loss 1.13860822 epoch total loss 1.18838334\n",
      "Trained batch 1079 batch loss 1.17599964 epoch total loss 1.1883719\n",
      "Trained batch 1080 batch loss 1.20098722 epoch total loss 1.18838358\n",
      "Trained batch 1081 batch loss 1.1658051 epoch total loss 1.1883626\n",
      "Trained batch 1082 batch loss 1.11069906 epoch total loss 1.18829083\n",
      "Trained batch 1083 batch loss 1.19090462 epoch total loss 1.18829334\n",
      "Trained batch 1084 batch loss 1.09962261 epoch total loss 1.18821144\n",
      "Trained batch 1085 batch loss 1.20873427 epoch total loss 1.1882304\n",
      "Trained batch 1086 batch loss 1.14794302 epoch total loss 1.18819332\n",
      "Trained batch 1087 batch loss 1.21121573 epoch total loss 1.18821442\n",
      "Trained batch 1088 batch loss 1.06105435 epoch total loss 1.1880976\n",
      "Trained batch 1089 batch loss 1.11861122 epoch total loss 1.18803382\n",
      "Trained batch 1090 batch loss 1.14291847 epoch total loss 1.18799245\n",
      "Trained batch 1091 batch loss 1.16513181 epoch total loss 1.18797147\n",
      "Trained batch 1092 batch loss 1.24876261 epoch total loss 1.18802714\n",
      "Trained batch 1093 batch loss 1.17606878 epoch total loss 1.18801618\n",
      "Trained batch 1094 batch loss 1.16502762 epoch total loss 1.1879952\n",
      "Trained batch 1095 batch loss 1.09101224 epoch total loss 1.18790662\n",
      "Trained batch 1096 batch loss 1.24529386 epoch total loss 1.18795896\n",
      "Trained batch 1097 batch loss 1.24342906 epoch total loss 1.1880095\n",
      "Trained batch 1098 batch loss 1.2420336 epoch total loss 1.18805873\n",
      "Trained batch 1099 batch loss 1.18283701 epoch total loss 1.18805408\n",
      "Trained batch 1100 batch loss 1.16967368 epoch total loss 1.18803728\n",
      "Trained batch 1101 batch loss 1.12194562 epoch total loss 1.18797731\n",
      "Trained batch 1102 batch loss 1.13810551 epoch total loss 1.18793201\n",
      "Trained batch 1103 batch loss 1.10145807 epoch total loss 1.18785357\n",
      "Trained batch 1104 batch loss 1.26970696 epoch total loss 1.18792772\n",
      "Trained batch 1105 batch loss 1.12549782 epoch total loss 1.18787122\n",
      "Trained batch 1106 batch loss 1.21725631 epoch total loss 1.1878978\n",
      "Trained batch 1107 batch loss 1.1988306 epoch total loss 1.1879077\n",
      "Trained batch 1108 batch loss 1.2122184 epoch total loss 1.18792951\n",
      "Trained batch 1109 batch loss 1.24033105 epoch total loss 1.18797684\n",
      "Trained batch 1110 batch loss 1.11704195 epoch total loss 1.18791294\n",
      "Trained batch 1111 batch loss 1.17867327 epoch total loss 1.18790472\n",
      "Trained batch 1112 batch loss 1.24688315 epoch total loss 1.18795764\n",
      "Trained batch 1113 batch loss 1.23252976 epoch total loss 1.1879977\n",
      "Trained batch 1114 batch loss 1.18416107 epoch total loss 1.18799436\n",
      "Trained batch 1115 batch loss 1.25307465 epoch total loss 1.18805265\n",
      "Trained batch 1116 batch loss 1.3729074 epoch total loss 1.18821836\n",
      "Trained batch 1117 batch loss 1.26269388 epoch total loss 1.18828499\n",
      "Trained batch 1118 batch loss 1.22658348 epoch total loss 1.18831921\n",
      "Trained batch 1119 batch loss 1.25777686 epoch total loss 1.18838131\n",
      "Trained batch 1120 batch loss 1.16020465 epoch total loss 1.18835616\n",
      "Trained batch 1121 batch loss 1.16235673 epoch total loss 1.18833292\n",
      "Trained batch 1122 batch loss 1.19306707 epoch total loss 1.18833721\n",
      "Trained batch 1123 batch loss 1.17960262 epoch total loss 1.18832934\n",
      "Trained batch 1124 batch loss 1.25051486 epoch total loss 1.18838465\n",
      "Trained batch 1125 batch loss 1.23912978 epoch total loss 1.18842983\n",
      "Trained batch 1126 batch loss 1.24328649 epoch total loss 1.18847847\n",
      "Trained batch 1127 batch loss 1.15158844 epoch total loss 1.18844581\n",
      "Trained batch 1128 batch loss 1.18717456 epoch total loss 1.18844461\n",
      "Trained batch 1129 batch loss 1.20553303 epoch total loss 1.18845975\n",
      "Trained batch 1130 batch loss 1.06907094 epoch total loss 1.18835413\n",
      "Trained batch 1131 batch loss 1.15559506 epoch total loss 1.18832529\n",
      "Trained batch 1132 batch loss 1.25866723 epoch total loss 1.18838739\n",
      "Trained batch 1133 batch loss 1.18899179 epoch total loss 1.18838787\n",
      "Trained batch 1134 batch loss 1.25884414 epoch total loss 1.18845\n",
      "Trained batch 1135 batch loss 1.22344482 epoch total loss 1.18848073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1136 batch loss 1.1863687 epoch total loss 1.18847895\n",
      "Trained batch 1137 batch loss 1.27814758 epoch total loss 1.18855786\n",
      "Trained batch 1138 batch loss 1.29330194 epoch total loss 1.18864989\n",
      "Trained batch 1139 batch loss 1.1690042 epoch total loss 1.18863261\n",
      "Trained batch 1140 batch loss 1.10868871 epoch total loss 1.18856239\n",
      "Trained batch 1141 batch loss 1.13921905 epoch total loss 1.18851912\n",
      "Trained batch 1142 batch loss 1.14272141 epoch total loss 1.18847907\n",
      "Trained batch 1143 batch loss 1.08080244 epoch total loss 1.18838477\n",
      "Trained batch 1144 batch loss 1.04956591 epoch total loss 1.18826342\n",
      "Trained batch 1145 batch loss 1.0970304 epoch total loss 1.18818378\n",
      "Trained batch 1146 batch loss 1.19480467 epoch total loss 1.18818963\n",
      "Trained batch 1147 batch loss 1.05920219 epoch total loss 1.18807709\n",
      "Trained batch 1148 batch loss 1.18221951 epoch total loss 1.18807209\n",
      "Trained batch 1149 batch loss 1.20570624 epoch total loss 1.18808734\n",
      "Trained batch 1150 batch loss 1.06452084 epoch total loss 1.18798\n",
      "Trained batch 1151 batch loss 1.15725899 epoch total loss 1.18795323\n",
      "Trained batch 1152 batch loss 1.02621317 epoch total loss 1.18781292\n",
      "Trained batch 1153 batch loss 0.964333892 epoch total loss 1.18761909\n",
      "Trained batch 1154 batch loss 1.05721 epoch total loss 1.18750608\n",
      "Trained batch 1155 batch loss 1.09911346 epoch total loss 1.18742967\n",
      "Trained batch 1156 batch loss 1.22038758 epoch total loss 1.18745804\n",
      "Trained batch 1157 batch loss 1.23792458 epoch total loss 1.18750167\n",
      "Trained batch 1158 batch loss 1.37738919 epoch total loss 1.1876657\n",
      "Trained batch 1159 batch loss 1.28983438 epoch total loss 1.1877538\n",
      "Trained batch 1160 batch loss 1.21371794 epoch total loss 1.18777621\n",
      "Trained batch 1161 batch loss 1.13735795 epoch total loss 1.18773282\n",
      "Trained batch 1162 batch loss 1.17269766 epoch total loss 1.18771982\n",
      "Trained batch 1163 batch loss 1.19241798 epoch total loss 1.18772388\n",
      "Trained batch 1164 batch loss 1.12588584 epoch total loss 1.18767071\n",
      "Trained batch 1165 batch loss 0.955656171 epoch total loss 1.18747163\n",
      "Trained batch 1166 batch loss 0.84621197 epoch total loss 1.18717897\n",
      "Trained batch 1167 batch loss 1.2212137 epoch total loss 1.18720806\n",
      "Trained batch 1168 batch loss 1.27590096 epoch total loss 1.18728399\n",
      "Trained batch 1169 batch loss 1.38459158 epoch total loss 1.18745279\n",
      "Trained batch 1170 batch loss 1.30407429 epoch total loss 1.18755245\n",
      "Trained batch 1171 batch loss 1.31574225 epoch total loss 1.18766201\n",
      "Trained batch 1172 batch loss 0.965858 epoch total loss 1.1874727\n",
      "Trained batch 1173 batch loss 0.9126544 epoch total loss 1.18723834\n",
      "Trained batch 1174 batch loss 1.10061598 epoch total loss 1.18716455\n",
      "Trained batch 1175 batch loss 1.22009397 epoch total loss 1.18719256\n",
      "Trained batch 1176 batch loss 1.44756222 epoch total loss 1.18741393\n",
      "Trained batch 1177 batch loss 1.213153 epoch total loss 1.18743575\n",
      "Trained batch 1178 batch loss 1.30798197 epoch total loss 1.18753815\n",
      "Trained batch 1179 batch loss 1.13264942 epoch total loss 1.18749166\n",
      "Trained batch 1180 batch loss 1.24174058 epoch total loss 1.18753755\n",
      "Trained batch 1181 batch loss 1.23811388 epoch total loss 1.18758047\n",
      "Trained batch 1182 batch loss 1.21598577 epoch total loss 1.18760443\n",
      "Trained batch 1183 batch loss 1.23158956 epoch total loss 1.18764162\n",
      "Trained batch 1184 batch loss 1.16898334 epoch total loss 1.18762577\n",
      "Trained batch 1185 batch loss 1.31565964 epoch total loss 1.18773389\n",
      "Trained batch 1186 batch loss 1.22105169 epoch total loss 1.1877619\n",
      "Trained batch 1187 batch loss 1.15215087 epoch total loss 1.18773186\n",
      "Trained batch 1188 batch loss 1.12570536 epoch total loss 1.18767977\n",
      "Trained batch 1189 batch loss 1.02128434 epoch total loss 1.1875397\n",
      "Trained batch 1190 batch loss 1.2247231 epoch total loss 1.18757093\n",
      "Trained batch 1191 batch loss 1.16950631 epoch total loss 1.18755591\n",
      "Trained batch 1192 batch loss 1.08167815 epoch total loss 1.18746698\n",
      "Trained batch 1193 batch loss 1.03137827 epoch total loss 1.18733621\n",
      "Trained batch 1194 batch loss 1.25052035 epoch total loss 1.18738902\n",
      "Trained batch 1195 batch loss 1.27270329 epoch total loss 1.18746042\n",
      "Trained batch 1196 batch loss 1.17356586 epoch total loss 1.18744886\n",
      "Trained batch 1197 batch loss 1.09877336 epoch total loss 1.18737471\n",
      "Trained batch 1198 batch loss 1.14330864 epoch total loss 1.18733799\n",
      "Trained batch 1199 batch loss 1.11973226 epoch total loss 1.18728161\n",
      "Trained batch 1200 batch loss 1.12393379 epoch total loss 1.1872288\n",
      "Trained batch 1201 batch loss 1.2245276 epoch total loss 1.18725979\n",
      "Trained batch 1202 batch loss 1.18703175 epoch total loss 1.18725967\n",
      "Trained batch 1203 batch loss 1.16833472 epoch total loss 1.18724394\n",
      "Trained batch 1204 batch loss 1.15344334 epoch total loss 1.18721581\n",
      "Trained batch 1205 batch loss 1.09135675 epoch total loss 1.18713617\n",
      "Trained batch 1206 batch loss 1.12970781 epoch total loss 1.18708861\n",
      "Trained batch 1207 batch loss 1.23131382 epoch total loss 1.18712533\n",
      "Trained batch 1208 batch loss 1.0482769 epoch total loss 1.18701029\n",
      "Trained batch 1209 batch loss 1.24184644 epoch total loss 1.18705559\n",
      "Trained batch 1210 batch loss 1.14724433 epoch total loss 1.18702269\n",
      "Trained batch 1211 batch loss 1.18199527 epoch total loss 1.18701851\n",
      "Trained batch 1212 batch loss 1.1885829 epoch total loss 1.18701982\n",
      "Trained batch 1213 batch loss 1.16019404 epoch total loss 1.18699777\n",
      "Trained batch 1214 batch loss 0.999528289 epoch total loss 1.18684328\n",
      "Trained batch 1215 batch loss 1.07425547 epoch total loss 1.18675065\n",
      "Trained batch 1216 batch loss 1.03529131 epoch total loss 1.18662608\n",
      "Trained batch 1217 batch loss 1.20409513 epoch total loss 1.18664038\n",
      "Trained batch 1218 batch loss 1.28772092 epoch total loss 1.18672335\n",
      "Trained batch 1219 batch loss 1.19646108 epoch total loss 1.18673134\n",
      "Trained batch 1220 batch loss 1.1927315 epoch total loss 1.18673623\n",
      "Trained batch 1221 batch loss 1.15733147 epoch total loss 1.18671215\n",
      "Trained batch 1222 batch loss 1.11527252 epoch total loss 1.18665373\n",
      "Trained batch 1223 batch loss 1.22602522 epoch total loss 1.18668592\n",
      "Trained batch 1224 batch loss 1.24398291 epoch total loss 1.18673277\n",
      "Trained batch 1225 batch loss 1.08461022 epoch total loss 1.18664944\n",
      "Trained batch 1226 batch loss 1.06571341 epoch total loss 1.18655074\n",
      "Trained batch 1227 batch loss 1.099594 epoch total loss 1.18647981\n",
      "Trained batch 1228 batch loss 1.09130144 epoch total loss 1.18640232\n",
      "Trained batch 1229 batch loss 1.23593831 epoch total loss 1.18644273\n",
      "Trained batch 1230 batch loss 1.27960408 epoch total loss 1.18651843\n",
      "Trained batch 1231 batch loss 1.16766322 epoch total loss 1.18650305\n",
      "Trained batch 1232 batch loss 1.00577056 epoch total loss 1.18635643\n",
      "Trained batch 1233 batch loss 0.947386622 epoch total loss 1.18616259\n",
      "Trained batch 1234 batch loss 0.964002728 epoch total loss 1.18598258\n",
      "Trained batch 1235 batch loss 0.948265791 epoch total loss 1.18579006\n",
      "Trained batch 1236 batch loss 0.9762398 epoch total loss 1.18562043\n",
      "Trained batch 1237 batch loss 1.12463379 epoch total loss 1.18557119\n",
      "Trained batch 1238 batch loss 1.23216891 epoch total loss 1.18560874\n",
      "Trained batch 1239 batch loss 1.11837101 epoch total loss 1.1855545\n",
      "Trained batch 1240 batch loss 1.3283993 epoch total loss 1.18566978\n",
      "Trained batch 1241 batch loss 1.33063638 epoch total loss 1.1857866\n",
      "Trained batch 1242 batch loss 1.53592372 epoch total loss 1.18606842\n",
      "Trained batch 1243 batch loss 1.4521724 epoch total loss 1.18628252\n",
      "Trained batch 1244 batch loss 1.1393038 epoch total loss 1.18624473\n",
      "Trained batch 1245 batch loss 1.14255452 epoch total loss 1.18620968\n",
      "Trained batch 1246 batch loss 1.09436762 epoch total loss 1.18613601\n",
      "Trained batch 1247 batch loss 1.21568286 epoch total loss 1.18615973\n",
      "Trained batch 1248 batch loss 1.10021663 epoch total loss 1.18609083\n",
      "Trained batch 1249 batch loss 0.972001553 epoch total loss 1.1859194\n",
      "Trained batch 1250 batch loss 1.03814185 epoch total loss 1.18580115\n",
      "Trained batch 1251 batch loss 1.10127485 epoch total loss 1.18573368\n",
      "Trained batch 1252 batch loss 0.916316152 epoch total loss 1.18551838\n",
      "Trained batch 1253 batch loss 0.981114149 epoch total loss 1.18535519\n",
      "Trained batch 1254 batch loss 1.10832369 epoch total loss 1.18529379\n",
      "Trained batch 1255 batch loss 1.24380732 epoch total loss 1.1853404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1256 batch loss 0.938880444 epoch total loss 1.18514407\n",
      "Trained batch 1257 batch loss 0.95913589 epoch total loss 1.1849643\n",
      "Trained batch 1258 batch loss 1.15642452 epoch total loss 1.18494153\n",
      "Trained batch 1259 batch loss 1.1380434 epoch total loss 1.18490434\n",
      "Trained batch 1260 batch loss 1.21489775 epoch total loss 1.18492806\n",
      "Trained batch 1261 batch loss 1.14551091 epoch total loss 1.18489683\n",
      "Trained batch 1262 batch loss 1.2693 epoch total loss 1.1849637\n",
      "Trained batch 1263 batch loss 1.28502011 epoch total loss 1.18504298\n",
      "Trained batch 1264 batch loss 1.18171799 epoch total loss 1.18504035\n",
      "Trained batch 1265 batch loss 1.12070608 epoch total loss 1.18498945\n",
      "Trained batch 1266 batch loss 1.15416825 epoch total loss 1.18496513\n",
      "Trained batch 1267 batch loss 1.17354226 epoch total loss 1.18495619\n",
      "Trained batch 1268 batch loss 1.18390238 epoch total loss 1.18495536\n",
      "Trained batch 1269 batch loss 1.07510567 epoch total loss 1.18486881\n",
      "Trained batch 1270 batch loss 1.11547482 epoch total loss 1.18481421\n",
      "Trained batch 1271 batch loss 1.29979587 epoch total loss 1.18490469\n",
      "Trained batch 1272 batch loss 1.38751554 epoch total loss 1.18506396\n",
      "Trained batch 1273 batch loss 1.35749245 epoch total loss 1.1851995\n",
      "Trained batch 1274 batch loss 1.23851526 epoch total loss 1.18524134\n",
      "Trained batch 1275 batch loss 1.2110585 epoch total loss 1.18526161\n",
      "Trained batch 1276 batch loss 1.16182089 epoch total loss 1.18524325\n",
      "Trained batch 1277 batch loss 0.95559454 epoch total loss 1.18506336\n",
      "Trained batch 1278 batch loss 0.968107104 epoch total loss 1.18489361\n",
      "Trained batch 1279 batch loss 0.919643879 epoch total loss 1.1846863\n",
      "Trained batch 1280 batch loss 1.09302592 epoch total loss 1.18461466\n",
      "Trained batch 1281 batch loss 1.04244423 epoch total loss 1.18450367\n",
      "Trained batch 1282 batch loss 1.05366075 epoch total loss 1.18440163\n",
      "Trained batch 1283 batch loss 1.15714991 epoch total loss 1.18438041\n",
      "Trained batch 1284 batch loss 1.15432703 epoch total loss 1.18435693\n",
      "Trained batch 1285 batch loss 1.17350328 epoch total loss 1.18434846\n",
      "Trained batch 1286 batch loss 1.2634877 epoch total loss 1.18441\n",
      "Trained batch 1287 batch loss 1.21322715 epoch total loss 1.18443239\n",
      "Trained batch 1288 batch loss 1.26091838 epoch total loss 1.18449175\n",
      "Trained batch 1289 batch loss 1.21002507 epoch total loss 1.18451154\n",
      "Trained batch 1290 batch loss 1.25774848 epoch total loss 1.18456829\n",
      "Trained batch 1291 batch loss 1.16210568 epoch total loss 1.18455088\n",
      "Trained batch 1292 batch loss 1.14702761 epoch total loss 1.18452179\n",
      "Trained batch 1293 batch loss 1.25202405 epoch total loss 1.18457413\n",
      "Trained batch 1294 batch loss 1.21561241 epoch total loss 1.18459809\n",
      "Trained batch 1295 batch loss 1.30212915 epoch total loss 1.18468881\n",
      "Trained batch 1296 batch loss 1.06718361 epoch total loss 1.18459809\n",
      "Trained batch 1297 batch loss 1.26739788 epoch total loss 1.18466198\n",
      "Trained batch 1298 batch loss 1.37135696 epoch total loss 1.18480575\n",
      "Trained batch 1299 batch loss 1.32906449 epoch total loss 1.18491685\n",
      "Trained batch 1300 batch loss 1.1030004 epoch total loss 1.18485391\n",
      "Trained batch 1301 batch loss 1.22769809 epoch total loss 1.18488681\n",
      "Trained batch 1302 batch loss 1.09438026 epoch total loss 1.18481731\n",
      "Trained batch 1303 batch loss 1.10815191 epoch total loss 1.18475842\n",
      "Trained batch 1304 batch loss 1.07114708 epoch total loss 1.18467128\n",
      "Trained batch 1305 batch loss 1.08745801 epoch total loss 1.18459678\n",
      "Trained batch 1306 batch loss 1.09110308 epoch total loss 1.18452513\n",
      "Trained batch 1307 batch loss 0.927368939 epoch total loss 1.18432844\n",
      "Trained batch 1308 batch loss 1.11619484 epoch total loss 1.18427634\n",
      "Trained batch 1309 batch loss 1.08053851 epoch total loss 1.18419707\n",
      "Trained batch 1310 batch loss 1.04157329 epoch total loss 1.18408823\n",
      "Trained batch 1311 batch loss 1.10329127 epoch total loss 1.1840266\n",
      "Trained batch 1312 batch loss 1.0332334 epoch total loss 1.18391168\n",
      "Trained batch 1313 batch loss 1.03481233 epoch total loss 1.18379807\n",
      "Trained batch 1314 batch loss 1.02547789 epoch total loss 1.18367767\n",
      "Trained batch 1315 batch loss 1.40800858 epoch total loss 1.18384814\n",
      "Trained batch 1316 batch loss 1.34125781 epoch total loss 1.18396783\n",
      "Trained batch 1317 batch loss 1.19566679 epoch total loss 1.18397677\n",
      "Trained batch 1318 batch loss 1.24612856 epoch total loss 1.18402386\n",
      "Trained batch 1319 batch loss 1.33790088 epoch total loss 1.18414056\n",
      "Trained batch 1320 batch loss 1.19241095 epoch total loss 1.18414676\n",
      "Trained batch 1321 batch loss 1.2234273 epoch total loss 1.18417645\n",
      "Trained batch 1322 batch loss 1.0794518 epoch total loss 1.18409729\n",
      "Trained batch 1323 batch loss 1.09443069 epoch total loss 1.18402958\n",
      "Trained batch 1324 batch loss 1.16659462 epoch total loss 1.18401635\n",
      "Trained batch 1325 batch loss 1.39072096 epoch total loss 1.18417239\n",
      "Trained batch 1326 batch loss 1.2980305 epoch total loss 1.18425822\n",
      "Trained batch 1327 batch loss 1.01358795 epoch total loss 1.1841296\n",
      "Trained batch 1328 batch loss 1.05386448 epoch total loss 1.18403149\n",
      "Trained batch 1329 batch loss 1.04446721 epoch total loss 1.18392646\n",
      "Trained batch 1330 batch loss 1.1049006 epoch total loss 1.18386698\n",
      "Trained batch 1331 batch loss 1.09223664 epoch total loss 1.18379819\n",
      "Trained batch 1332 batch loss 1.31548798 epoch total loss 1.18389702\n",
      "Trained batch 1333 batch loss 1.20491219 epoch total loss 1.18391275\n",
      "Trained batch 1334 batch loss 1.21869063 epoch total loss 1.18393886\n",
      "Trained batch 1335 batch loss 1.1494658 epoch total loss 1.18391311\n",
      "Trained batch 1336 batch loss 1.11079776 epoch total loss 1.18385839\n",
      "Trained batch 1337 batch loss 1.23676276 epoch total loss 1.18389797\n",
      "Trained batch 1338 batch loss 1.11957598 epoch total loss 1.18384993\n",
      "Trained batch 1339 batch loss 1.29358733 epoch total loss 1.18393183\n",
      "Trained batch 1340 batch loss 1.27981973 epoch total loss 1.18400347\n",
      "Trained batch 1341 batch loss 1.10803854 epoch total loss 1.18394673\n",
      "Trained batch 1342 batch loss 1.05833304 epoch total loss 1.18385315\n",
      "Trained batch 1343 batch loss 1.05241787 epoch total loss 1.18375528\n",
      "Trained batch 1344 batch loss 1.18511546 epoch total loss 1.18375623\n",
      "Trained batch 1345 batch loss 1.22122753 epoch total loss 1.18378413\n",
      "Trained batch 1346 batch loss 1.33607304 epoch total loss 1.18389726\n",
      "Trained batch 1347 batch loss 1.3455646 epoch total loss 1.18401718\n",
      "Trained batch 1348 batch loss 1.31854224 epoch total loss 1.18411696\n",
      "Trained batch 1349 batch loss 1.16074896 epoch total loss 1.18409967\n",
      "Trained batch 1350 batch loss 1.16126013 epoch total loss 1.18408275\n",
      "Trained batch 1351 batch loss 1.19470692 epoch total loss 1.18409061\n",
      "Trained batch 1352 batch loss 1.20328093 epoch total loss 1.1841048\n",
      "Trained batch 1353 batch loss 1.08437252 epoch total loss 1.18403101\n",
      "Trained batch 1354 batch loss 1.29795241 epoch total loss 1.18411517\n",
      "Trained batch 1355 batch loss 1.19922256 epoch total loss 1.18412638\n",
      "Trained batch 1356 batch loss 1.16858757 epoch total loss 1.18411493\n",
      "Trained batch 1357 batch loss 1.26122522 epoch total loss 1.18417168\n",
      "Trained batch 1358 batch loss 1.16965818 epoch total loss 1.18416107\n",
      "Trained batch 1359 batch loss 1.07412887 epoch total loss 1.18408\n",
      "Trained batch 1360 batch loss 1.06181586 epoch total loss 1.18399012\n",
      "Trained batch 1361 batch loss 1.09851456 epoch total loss 1.1839273\n",
      "Trained batch 1362 batch loss 1.08429456 epoch total loss 1.18385422\n",
      "Trained batch 1363 batch loss 1.07516241 epoch total loss 1.18377447\n",
      "Trained batch 1364 batch loss 1.09824133 epoch total loss 1.18371177\n",
      "Trained batch 1365 batch loss 1.15763545 epoch total loss 1.18369269\n",
      "Trained batch 1366 batch loss 1.28919959 epoch total loss 1.18377\n",
      "Trained batch 1367 batch loss 1.40948594 epoch total loss 1.18393505\n",
      "Trained batch 1368 batch loss 1.38277626 epoch total loss 1.18408048\n",
      "Trained batch 1369 batch loss 1.22485447 epoch total loss 1.18411016\n",
      "Trained batch 1370 batch loss 1.25646448 epoch total loss 1.18416297\n",
      "Trained batch 1371 batch loss 1.19833279 epoch total loss 1.18417335\n",
      "Trained batch 1372 batch loss 1.31131721 epoch total loss 1.18426597\n",
      "Trained batch 1373 batch loss 1.15087152 epoch total loss 1.18424165\n",
      "Trained batch 1374 batch loss 1.14715075 epoch total loss 1.18421471\n",
      "Trained batch 1375 batch loss 1.18596292 epoch total loss 1.1842159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1376 batch loss 1.19307673 epoch total loss 1.18422234\n",
      "Trained batch 1377 batch loss 1.28006458 epoch total loss 1.18429196\n",
      "Trained batch 1378 batch loss 1.18565059 epoch total loss 1.18429291\n",
      "Trained batch 1379 batch loss 1.24079597 epoch total loss 1.18433392\n",
      "Trained batch 1380 batch loss 1.22752678 epoch total loss 1.18436527\n",
      "Trained batch 1381 batch loss 1.3349427 epoch total loss 1.18447435\n",
      "Trained batch 1382 batch loss 1.2730391 epoch total loss 1.18453848\n",
      "Trained batch 1383 batch loss 0.992888 epoch total loss 1.18439984\n",
      "Trained batch 1384 batch loss 1.03968751 epoch total loss 1.1842953\n",
      "Trained batch 1385 batch loss 1.1485033 epoch total loss 1.18426955\n",
      "Trained batch 1386 batch loss 1.26600325 epoch total loss 1.18432844\n",
      "Trained batch 1387 batch loss 1.16299272 epoch total loss 1.18431306\n",
      "Trained batch 1388 batch loss 1.14209175 epoch total loss 1.18428266\n",
      "Epoch 5 train loss 1.184282660484314\n",
      "Validated batch 1 batch loss 1.13749957\n",
      "Validated batch 2 batch loss 1.05294466\n",
      "Validated batch 3 batch loss 1.14615178\n",
      "Validated batch 4 batch loss 1.12290597\n",
      "Validated batch 5 batch loss 1.16596329\n",
      "Validated batch 6 batch loss 1.2240237\n",
      "Validated batch 7 batch loss 1.22648907\n",
      "Validated batch 8 batch loss 1.20116496\n",
      "Validated batch 9 batch loss 1.18336749\n",
      "Validated batch 10 batch loss 1.20001316\n",
      "Validated batch 11 batch loss 1.13630521\n",
      "Validated batch 12 batch loss 1.16437721\n",
      "Validated batch 13 batch loss 1.1448642\n",
      "Validated batch 14 batch loss 1.15168428\n",
      "Validated batch 15 batch loss 1.15607619\n",
      "Validated batch 16 batch loss 1.15647495\n",
      "Validated batch 17 batch loss 1.35116863\n",
      "Validated batch 18 batch loss 1.2191062\n",
      "Validated batch 19 batch loss 1.05719757\n",
      "Validated batch 20 batch loss 1.31454968\n",
      "Validated batch 21 batch loss 1.13394022\n",
      "Validated batch 22 batch loss 1.07395196\n",
      "Validated batch 23 batch loss 1.1841805\n",
      "Validated batch 24 batch loss 1.21076775\n",
      "Validated batch 25 batch loss 1.18791366\n",
      "Validated batch 26 batch loss 1.1573081\n",
      "Validated batch 27 batch loss 1.15578473\n",
      "Validated batch 28 batch loss 1.28233945\n",
      "Validated batch 29 batch loss 1.34579062\n",
      "Validated batch 30 batch loss 1.12913167\n",
      "Validated batch 31 batch loss 1.20776129\n",
      "Validated batch 32 batch loss 1.17124152\n",
      "Validated batch 33 batch loss 1.21815753\n",
      "Validated batch 34 batch loss 1.23855066\n",
      "Validated batch 35 batch loss 1.08723891\n",
      "Validated batch 36 batch loss 1.35305095\n",
      "Validated batch 37 batch loss 1.12490833\n",
      "Validated batch 38 batch loss 1.27875161\n",
      "Validated batch 39 batch loss 1.25768685\n",
      "Validated batch 40 batch loss 1.30916405\n",
      "Validated batch 41 batch loss 1.0825088\n",
      "Validated batch 42 batch loss 1.2192322\n",
      "Validated batch 43 batch loss 1.14790952\n",
      "Validated batch 44 batch loss 1.2300514\n",
      "Validated batch 45 batch loss 1.18426847\n",
      "Validated batch 46 batch loss 1.21106851\n",
      "Validated batch 47 batch loss 1.31912243\n",
      "Validated batch 48 batch loss 1.20580626\n",
      "Validated batch 49 batch loss 1.10864949\n",
      "Validated batch 50 batch loss 1.15058255\n",
      "Validated batch 51 batch loss 1.10885942\n",
      "Validated batch 52 batch loss 1.17980886\n",
      "Validated batch 53 batch loss 1.24237597\n",
      "Validated batch 54 batch loss 1.09823966\n",
      "Validated batch 55 batch loss 1.23907435\n",
      "Validated batch 56 batch loss 1.19786811\n",
      "Validated batch 57 batch loss 1.24595261\n",
      "Validated batch 58 batch loss 1.24427438\n",
      "Validated batch 59 batch loss 1.21238315\n",
      "Validated batch 60 batch loss 1.11599219\n",
      "Validated batch 61 batch loss 1.13181567\n",
      "Validated batch 62 batch loss 1.18907619\n",
      "Validated batch 63 batch loss 1.18202257\n",
      "Validated batch 64 batch loss 1.22647822\n",
      "Validated batch 65 batch loss 1.22148\n",
      "Validated batch 66 batch loss 1.42337584\n",
      "Validated batch 67 batch loss 1.23352766\n",
      "Validated batch 68 batch loss 1.22783709\n",
      "Validated batch 69 batch loss 1.10717416\n",
      "Validated batch 70 batch loss 1.11733699\n",
      "Validated batch 71 batch loss 1.21905482\n",
      "Validated batch 72 batch loss 1.15206623\n",
      "Validated batch 73 batch loss 1.1299082\n",
      "Validated batch 74 batch loss 1.21175909\n",
      "Validated batch 75 batch loss 1.2611717\n",
      "Validated batch 76 batch loss 1.25094438\n",
      "Validated batch 77 batch loss 1.27469504\n",
      "Validated batch 78 batch loss 1.19583511\n",
      "Validated batch 79 batch loss 1.13924885\n",
      "Validated batch 80 batch loss 1.21803224\n",
      "Validated batch 81 batch loss 1.14986837\n",
      "Validated batch 82 batch loss 1.22612023\n",
      "Validated batch 83 batch loss 1.3328917\n",
      "Validated batch 84 batch loss 1.25087547\n",
      "Validated batch 85 batch loss 1.22119164\n",
      "Validated batch 86 batch loss 1.32685244\n",
      "Validated batch 87 batch loss 1.07462227\n",
      "Validated batch 88 batch loss 1.22680521\n",
      "Validated batch 89 batch loss 1.00746417\n",
      "Validated batch 90 batch loss 1.15194273\n",
      "Validated batch 91 batch loss 1.26108515\n",
      "Validated batch 92 batch loss 1.10262239\n",
      "Validated batch 93 batch loss 1.05623937\n",
      "Validated batch 94 batch loss 1.14235723\n",
      "Validated batch 95 batch loss 1.26317918\n",
      "Validated batch 96 batch loss 1.10042834\n",
      "Validated batch 97 batch loss 1.22733092\n",
      "Validated batch 98 batch loss 1.33530259\n",
      "Validated batch 99 batch loss 1.05219889\n",
      "Validated batch 100 batch loss 1.17693174\n",
      "Validated batch 101 batch loss 1.17785501\n",
      "Validated batch 102 batch loss 1.25480902\n",
      "Validated batch 103 batch loss 1.22409439\n",
      "Validated batch 104 batch loss 1.10195017\n",
      "Validated batch 105 batch loss 1.03644347\n",
      "Validated batch 106 batch loss 1.16730928\n",
      "Validated batch 107 batch loss 1.08429408\n",
      "Validated batch 108 batch loss 1.15013409\n",
      "Validated batch 109 batch loss 1.21477389\n",
      "Validated batch 110 batch loss 1.04314089\n",
      "Validated batch 111 batch loss 1.17319715\n",
      "Validated batch 112 batch loss 1.24941838\n",
      "Validated batch 113 batch loss 1.22715533\n",
      "Validated batch 114 batch loss 1.17470384\n",
      "Validated batch 115 batch loss 1.05993676\n",
      "Validated batch 116 batch loss 1.20280492\n",
      "Validated batch 117 batch loss 1.19242477\n",
      "Validated batch 118 batch loss 1.14381826\n",
      "Validated batch 119 batch loss 1.15539062\n",
      "Validated batch 120 batch loss 1.15231156\n",
      "Validated batch 121 batch loss 1.17681885\n",
      "Validated batch 122 batch loss 1.24454522\n",
      "Validated batch 123 batch loss 1.13367915\n",
      "Validated batch 124 batch loss 1.14339137\n",
      "Validated batch 125 batch loss 1.29902768\n",
      "Validated batch 126 batch loss 1.078179\n",
      "Validated batch 127 batch loss 1.08406413\n",
      "Validated batch 128 batch loss 1.1644336\n",
      "Validated batch 129 batch loss 1.31053138\n",
      "Validated batch 130 batch loss 1.29525983\n",
      "Validated batch 131 batch loss 1.31967413\n",
      "Validated batch 132 batch loss 1.16053867\n",
      "Validated batch 133 batch loss 1.34244633\n",
      "Validated batch 134 batch loss 1.16140199\n",
      "Validated batch 135 batch loss 1.25339\n",
      "Validated batch 136 batch loss 1.23772573\n",
      "Validated batch 137 batch loss 0.940438449\n",
      "Validated batch 138 batch loss 1.1009028\n",
      "Validated batch 139 batch loss 1.15230608\n",
      "Validated batch 140 batch loss 1.15799713\n",
      "Validated batch 141 batch loss 1.15605092\n",
      "Validated batch 142 batch loss 1.07113063\n",
      "Validated batch 143 batch loss 1.12535346\n",
      "Validated batch 144 batch loss 1.28415608\n",
      "Validated batch 145 batch loss 1.04186368\n",
      "Validated batch 146 batch loss 1.05446672\n",
      "Validated batch 147 batch loss 1.09269047\n",
      "Validated batch 148 batch loss 1.15673792\n",
      "Validated batch 149 batch loss 1.03916907\n",
      "Validated batch 150 batch loss 1.18281031\n",
      "Validated batch 151 batch loss 1.05429757\n",
      "Validated batch 152 batch loss 1.18000388\n",
      "Validated batch 153 batch loss 1.25648236\n",
      "Validated batch 154 batch loss 1.26452839\n",
      "Validated batch 155 batch loss 1.1255337\n",
      "Validated batch 156 batch loss 1.28645134\n",
      "Validated batch 157 batch loss 0.996897519\n",
      "Validated batch 158 batch loss 1.02057421\n",
      "Validated batch 159 batch loss 1.11240637\n",
      "Validated batch 160 batch loss 1.14044142\n",
      "Validated batch 161 batch loss 1.29759657\n",
      "Validated batch 162 batch loss 1.22864652\n",
      "Validated batch 163 batch loss 1.1427002\n",
      "Validated batch 164 batch loss 1.13048649\n",
      "Validated batch 165 batch loss 1.11937404\n",
      "Validated batch 166 batch loss 1.13979852\n",
      "Validated batch 167 batch loss 1.20561194\n",
      "Validated batch 168 batch loss 1.19859457\n",
      "Validated batch 169 batch loss 1.25892293\n",
      "Validated batch 170 batch loss 1.29650855\n",
      "Validated batch 171 batch loss 1.26700449\n",
      "Validated batch 172 batch loss 1.16558719\n",
      "Validated batch 173 batch loss 1.34629059\n",
      "Validated batch 174 batch loss 1.243119\n",
      "Validated batch 175 batch loss 1.28346515\n",
      "Validated batch 176 batch loss 1.24103308\n",
      "Validated batch 177 batch loss 1.34991479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 178 batch loss 1.29715157\n",
      "Validated batch 179 batch loss 1.15187299\n",
      "Validated batch 180 batch loss 1.16927648\n",
      "Validated batch 181 batch loss 1.29341877\n",
      "Validated batch 182 batch loss 1.34441817\n",
      "Validated batch 183 batch loss 1.10993636\n",
      "Validated batch 184 batch loss 1.22783387\n",
      "Validated batch 185 batch loss 1.11870837\n",
      "Epoch 5 val loss 1.1858121156692505\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-5-loss-1.1858.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "best_model_SHN_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SHN', is_stackedhourglassnetwork=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe90712",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb59c03",
   "metadata": {},
   "source": [
    "## 예측 엔진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3891e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57806d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4299cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ae9c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6462fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "288b24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6852024c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6491f55",
   "metadata": {},
   "source": [
    "## StackedHourglass 모델 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acce0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model_SHN-epoch-5-loss-1.1858.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d14064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SHN = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model_SHN.load_weights(WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9627caea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_SHN, keypoints_SHN = predict(model_SHN, test_image)\n",
    "draw_keypoints_on_image(image_SHN, keypoints_SHN)\n",
    "draw_skeleton_on_image(image_SHN, keypoints_SHN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fbcda",
   "metadata": {},
   "source": [
    "## Simplebaseline 학습(실패)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e3848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "best_model_SBL_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SBL', is_stackedhourglassnetwork=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d7b67",
   "metadata": {},
   "source": [
    "# 마무리\n",
    "\n",
    "## 어려웠던 점\n",
    "\n",
    "- simplebaseline이 무엇인지 아직도 모르겠다(학습이 안됨)\n",
    "\n",
    "- 병렬 프로그래밍은 로컬 환경의 주피터 노트북으로 실행이 제대로 안됨\n",
    "\n",
    "- 시간이 너무 많이 걸렸음\n",
    "\n",
    "## 결론\n",
    "\n",
    "마지막 프로젝트인 만큼 잘해보자라는 생각을 가지고 있었지만,\n",
    "\n",
    "역시 마지막이라서 그런지 난이도가 매우 높았다.\n",
    "\n",
    "simplebaseline을 구현은 성공했는데 학습을 돌릴때 저 상태에서 진행이 안되었다.\n",
    "\n",
    "참고할 깃허브가 아예 없어 구현을 성공하더라고 맞게 성공한것인지 확인도 못할것이다.\n",
    "\n",
    "StackedHourglass 모델은 5에포크 인데도 불구하고 많은 시간을 잡아먹었다.\n",
    "\n",
    "학습결과도 그다지 좋지 않은것으로 보인다.\n",
    "\n",
    "여러모로 기쁘지 않은 프로젝트였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac126ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
